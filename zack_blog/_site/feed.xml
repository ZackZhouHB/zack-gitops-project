<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-06-12T21:55:19+10:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Zack’s Blog</title><subtitle>## AWS   ## Jenkins  ## Microservices ## Automation ## K8S   ## CICD     ## Gitops  [京ICP备2024056683号-1]</subtitle><entry><title type="html">Istio: Distributed Tracing with Jaeger</title><link href="http://localhost:4000/jekyll/cat2/2024/05/24/istio2.html" rel="alternate" type="text/html" title="Istio: Distributed Tracing with Jaeger" /><published>2024-05-24T10:15:29+10:00</published><updated>2024-05-24T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/24/istio2</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/24/istio2.html"><![CDATA[<p><b>About Jaeger</b></p>

<p>Last post we are able to depoly <a href="https://zackz.online/jekyll/cat2/2024/05/22/istio.html">Istio</a> and manage traffic for a book review microservice application. This session we will dive deeper into Istia for it add-on Jaeger for Microservice tracing.</p>

<p>Jaeger is an open-source end-to-end distributed tracing tool to monitor and troubleshoot the performance of microservices-based distributed systems by providing insights into the latency and other performance metrics.</p>

<ul>
  <li>Trace</li>
</ul>

<p>A trace represents the entire journey of a request or transaction as it propagates through various services and components of a distributed system. It captures the path the request takes, including all the microservices it interacts with, from start to finish. A trace is composed of multiple spans.</p>

<ul>
  <li>Span</li>
</ul>

<p>A span is a single unit of work within a trace. It represents an individual operation within a microservice, such as a function call, database query, or external API request. Each span contains metadata such as:</p>

<p><b>Prepration for Hands on</b></p>

<p>Here we will use <a href="https://github.com/DickChesterwood/k8s-fleetman">Fleetman GPS sumilater microservice application</a> as example to explore Jaeger and it capabilities.</p>

<ul>
  <li>Enable Istio sidecar injection for existing deployment</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># lable the namaspace to allow istio sidecar container injection</span>
<span class="o">[</span>root@freeipa-server ~]# kubectl label namespace default istio-injection<span class="o">=</span>enabled <span class="nt">--overwrite</span>

<span class="c"># Redeploy fleetman application</span>
<span class="o">[</span>root@freeipa-server ~]# kubectl rollout restart deployment <span class="nt">-n</span> default</code></pre></figure>

<ul>
  <li>validate pod for istio sidecar injection, also check service status in Kiali</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@freeipa-server ~]# kubectl get po
NAME                                                        READY   STATUS    RESTARTS       AGE
api-gateway-58f978dfc6-phdgp                                2/2     Running   4 <span class="o">(</span>30m ago<span class="o">)</span>    17h
position-simulator-6f5df9b447-57d75                         2/2     Running   4 <span class="o">(</span>30m ago<span class="o">)</span>    17h
position-tracker-6698577777-fz52v                           2/2     Running   4 <span class="o">(</span>30m ago<span class="o">)</span>    17h
staff-service-59987757dc-mfm2t                              2/2     Running   4 <span class="o">(</span>30m ago<span class="o">)</span>    17h
vehicle-telemetry-56c7f8d859-jvtpj                          2/2     Running   4 <span class="o">(</span>30m ago<span class="o">)</span>    17h
webapp-59bc7757fb-trnnv                                     2/2     Running   6 <span class="o">(</span>30m ago<span class="o">)</span>    17h</code></pre></figure>

<p><img src="/assets/istio2-2.png" alt="image tooltip here" /></p>

<p><b>How Jaeger Works</b></p>

<p>When a request enters a microservice (e.g., a user making a request to a frontend service), the tracing library creates a span and assigns it a trace ID. As the request propagates through other services, additional spans are created and linked to the same trace ID. Each span is recorded with its respective start and end timestamps, operation name, and other metadata.</p>

<p>The Jaeger UI provides a way to visualize traces. Users can search for traces based on various criteria (e.g., service name, operation name, duration) and view the detailed structure of individual traces, like durations of time spent between microservices.</p>

<p>As the request flows through different services, each service creates additional or child spans. (e.g., The frontend service might call an authentication service. then authentication service call a user service, thus Jaeger will create 2 child spans)</p>

<p><img src="/assets/istio2-1.png" alt="image tooltip here" /></p>

<p><b>Latency and Performance Analysis</b></p>

<p>By examining the durations of each span, If a particular span has a long duration, that service might be a bottleneck.
If spans have significant gaps between them, network latency or queuing delays might be an issue. so we can identify which part of the request is taking the most time and investigate further to optimize performance.</p>

<p><img src="/assets/istio2-4.png" alt="image tooltip here" /></p>

<p><b>Manage routing in each service from Kiali </b></p>

<p>Managing routing in Istio can be done either through the Kiali console or by defining VirtualServices and DestinationRules using Kubernetes YAML manifests. here from Kiali console, we have the visualization of each service traffic flow, metrics, and dependencies between services in real-time.</p>

<p>By create weighted routing or suspend traffic, Kiali will create it own VirtualServices and DestinationRules to manage the traffic</p>

<p><img src="/assets/istio2-3.png" alt="image tooltip here" /></p>

<p><b>Add timeout in istio virtual service YAML</b></p>

<p>To add a timeout into istio virtual service YAML and ensure it works with Jaeger for better visibility and efficiency in the microservice architecture.</p>

<p>By adding this timeout to 3s for bellow “api-gateway” virtual service, Jaeger trace will aviod long response time when a request call api-gateway, any response longer than 3s will reture http timeout, which add visibility to Jaeger UI to determine if the request successful or not.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: api-gateway
spec:
  hosts:
  - api-gateway
  http:
  - route:
    - destination:
        host: api-gateway
        port:
          number: 80
    <span class="nb">timeout</span>: 3s <span class="c"># 3 seconds timeout add</span></code></pre></figure>

<p><b> Conclusion</b></p>

<p>In this session we deep dive into Istio add-on Jaeger for distributed tracing, which Jaeger facilitates, involves tracking requests as they flow through various services and components of an application. This helps identify bottlenecks, understand service dependencies, and improve overall performance.</p>

<p>In the next post I will see how to use Istio and Kiali to run some Canary Releases, Blue-Green deployment Rolling Updates and A/B Testing.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Jaeger]]></summary></entry><entry><title type="html">Istio: Traffic Routing</title><link href="http://localhost:4000/jekyll/cat2/2024/05/22/istio.html" rel="alternate" type="text/html" title="Istio: Traffic Routing" /><published>2024-05-22T10:15:29+10:00</published><updated>2024-05-22T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/22/istio</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/22/istio.html"><![CDATA[<p><b>Helm install istio </b></p>

<p>Here we use helm to install istio (istio-base, istiod, istia gateway), then deploy a sample online book store microservice “bookinfo”, practise istio tasks include Traffic Management, Observability, Security.</p>

<p>Bookinfo Topology:</p>

<p><img src="/assets/bookinfo.png" alt="image tooltip here" /></p>

<ul>
  <li>Helm install istio (istiod, istio-ingress)</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl create namespace istio-system
helm pull istio/base
helm <span class="nb">install </span>istio-base <span class="nb">.</span> <span class="nt">-n</span> istio-system <span class="nt">--set</span> <span class="nv">defaultRevision</span><span class="o">=</span>default

helm pull istio/istiod
helm <span class="nb">install </span>istiod <span class="nb">.</span> <span class="nt">-n</span> istio-system 

kubectl create namespace istio-ingress
helm pull istio/gateway
helm <span class="nb">install </span>istio-ingress <span class="nb">.</span> <span class="nt">-n</span> istio-ingress

helm <span class="nb">ls</span> <span class="nt">-n</span> istio-system
NAME      	NAMESPACE   	REVISION	UPDATED                                	STATUS  	CHART        	APP VERSION
istio-base	istio-system	1       	2023-12-17 08:14:06.943276388 +0800 CST	deployed	base-1.20.1  	1.20.1     
istiod    	istio-system	1       	2023-12-17 08:15:40.370551503 +0800 CST	deployed	istiod-1.20.1	1.20.1 

helm <span class="nb">ls</span> <span class="nt">-n</span> istio-ingress
NAME         	NAMESPACE    	REVISION	UPDATED                                	STATUS  	CHART         	APP VERSION
istio-ingress	istio-ingress	1       	2023-12-17 08:25:07.111999373 +0800 CST	deployed	gateway-1.20.1	1.20.1</code></pre></figure>

<ul>
  <li>Deploy bookinfo microservice and istio ingressgateway and virtualservice</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl label namespace istio-system istio-injection<span class="o">=</span>enabled

kubectl apply <span class="nt">-f</span> https://github.com/istio/istio/blob/master/samples/bookinfo/platform/kube/bookinfo.yaml <span class="nt">-oyaml</span> <span class="o">&gt;</span> bookinfo.yaml
kubectl apply <span class="nt">-f</span> bookinfo.yaml

kubectl get po
NAME                                                     READY   STATUS    RESTARTS       AGE
details-v1-698d88b-wmfcb                                 2/2     Running   0              21m
ratings-v1-6484c4d9bb-cb6gx                              2/2     Running   0              21m
reviews-v1-5b5d6494f4-jrsvc                              2/2     Running   0              21m
reviews-v2-5b667bcbf8-jgfzj                              2/2     Running   0              21m
reviews-v3-5b9bd44f4-tmmfz                               2/2     Running   0              21m

kubectl apply <span class="nt">-f</span> https://github.com/istio/istio/blob/master/samples/bookinfo/networking/bookinfo-gateway.yaml <span class="nt">-oyaml</span> <span class="o">&gt;</span> bookinfo-gateway.yaml
kubectl apply <span class="nt">-f</span> bookinfo-gateway.yaml</code></pre></figure>

<ul>
  <li>Deploy Kiali, jaeger, grafana, prometheus</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">wget https://raw.githubusercontent.com/istio/istio/release-1.20/samples/addons/prometheus.yaml
wget https://raw.githubusercontent.com/istio/istio/release-1.20/samples/addons/jaeger.yaml
wget https://raw.githubusercontent.com/istio/istio/release-1.20/samples/addons/grafana.yaml
kubectl create <span class="nt">-f</span> prometheus.yaml <span class="nt">-f</span> jaeger.yaml <span class="nt">-f</span> grafana.yaml</code></pre></figure>

<ul>
  <li>
    <p>visit http://book.istio:31000/productpage, with review (v1, v2, v3)
<img src="/assets/kiali.png" alt="image tooltip here" /></p>
  </li>
  <li>
    <p>define destination rules and virtual service for reviews</p>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">wget https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/networking/destination-rule-all.yaml
wget https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/networking/virtual-service-reviews-90-10.yaml

kubectl create <span class="nt">-f</span> destination-rule-all.yaml <span class="nt">-f</span> virtual-service-reviews-90-10.yaml   <span class="c"># route v1 10% and v3 90%</span>
kubectl scale deployment reviews-v2 <span class="nt">-n</span> istio-system <span class="nt">--replicas</span><span class="o">=</span>0 <span class="c"># scale down v2 to 0</span>

kubectl get dr <span class="nt">-A</span>
NAMESPACE      NAME          HOST          AGE
istio-system   details       details       6m56s
istio-system   productpage   productpage   6m56s
istio-system   ratings       ratings       6m56s
istio-system   reviews       reviews       6m56s
kubectl get vs <span class="nt">-A</span>
NAMESPACE      NAME       GATEWAYS               HOSTS            AGE
istio-system   bookinfo   <span class="o">[</span><span class="s2">"bookinfo-gateway"</span><span class="o">]</span>   <span class="o">[</span><span class="s2">"book.istio"</span><span class="o">]</span>   19h
istio-system   reviews                           <span class="o">[</span><span class="s2">"reviews"</span><span class="o">]</span>      5m26s

Spec:
  Hosts:
    reviews
  Http:
    Route:
      Destination:
        Host:    reviews
        Subset:  v1
      Weight:    10
      Destination:
        Host:    reviews
        Subset:  v3
      Weight:    90</code></pre></figure>

<p>refresh bookinfo webpage, test Traffic route weight as bellow</p>

<p>90% traffic for reviews v3  vs  10% traffic for review v1</p>

<p><img src="/assets/1090.png" alt="image tooltip here" /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Helm install istio]]></summary></entry><entry><title type="html">Terraform Module for AWS SSO User Assignment</title><link href="http://localhost:4000/jekyll/cat2/2024/05/20/terraform1.html" rel="alternate" type="text/html" title="Terraform Module for AWS SSO User Assignment" /><published>2024-05-20T10:15:29+10:00</published><updated>2024-05-20T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/20/terraform1</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/20/terraform1.html"><![CDATA[<p><b> Challange About AWS SSO User Assignment</b></p>

<p>In the company, we use AWS SSO for user authentication; when a new user is created from Azure AD, it will automatically synced by AWS IAM Identity Center and Azure AD integration, and then our team will need to handle the SSO user assignment to put them in the required AWS accounts with requested permission sets, it became a pain when such requests coming more frequently and every time an individual user or a whole team with different accounts and permission requirements need to be fulfilled, so how to handle this efficiently become my recent topic.</p>

<p>So far, I have tried shell script and Python script to read the user name, AWS account ID, and permission sets ARN from a CSV file, then complete the task with AWScli. However it is not smart enough when the request or scenario changes. I have to adjust the script everytime.</p>

<p><b> Managing individual request via Terraform </b></p>

<p>Let’s start with handling user assignment individually with terraform first. here I have 2 requests:</p>

<ol>
  <li>
    <p>A user “user1@company.com”, under a group called “AD-RDS-READ-ONLY” in AWS IAM Identity Center, I need to create a permission set in aws account “123456789”, and assign to this user.</p>
  </li>
  <li>
    <p>The second request is from security team, we have 3 security team members (security1@company.com, security2@company.com, security3@company.com), under a group called “AD-ACM-FULL-ACCESS” in AWS IAM Identity Center, they all need full access for AWS certificate manager access, for all of our 3 AWS accounts (12345678901, 12345678902, and 12345678903).</p>
  </li>
</ol>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">vim main.tf

<span class="c"># For Request 1 for RDS read only access for 1 user in 1 AWS account </span>

<span class="c"># for AWS in ap-southeast-2</span>
provider <span class="s2">"aws"</span> <span class="o">{</span>
 region <span class="o">=</span> <span class="s2">"ap-southeast-2"</span>
<span class="o">}</span>

<span class="c"># Define the AWS SSO Instance ARN</span>
data <span class="s2">"aws_ssoadmin_instances"</span> <span class="s2">"main"</span> <span class="o">{}</span>

resource <span class="s2">"aws_ssoadmin_permission_set"</span> <span class="s2">"request1"</span> <span class="o">{</span>
 instance_arn <span class="o">=</span> data.aws_ssoadmin_instances.main.arns[0]
 name         <span class="o">=</span> <span class="s2">"RDS-ReadOnly"</span>
 description  <span class="o">=</span> <span class="s2">"Read-only access to RDS resources"</span>
 session_duration <span class="o">=</span> <span class="s2">"PT1H"</span>

 <span class="c"># Add the policies you need for this permission set</span>
 managed_policies <span class="o">=</span> <span class="o">[</span>
 <span class="s2">"arn:aws:iam::aws:policy/AmazonRDSReadOnlyAccess"</span>,
 <span class="o">]</span>
<span class="o">}</span>

resource <span class="s2">"aws_ssoadmin_account_assignment"</span> <span class="s2">"request1"</span> <span class="o">{</span>
 instance_arn       <span class="o">=</span> data.aws_ssoadmin_instances.main.arns[0]
 permission_set_arn <span class="o">=</span> aws_ssoadmin_permission_set.request1.arn
 principal_id       <span class="o">=</span> <span class="s2">"user1@company.com"</span>
 principal_type     <span class="o">=</span> <span class="s2">"USER"</span>
 target_id          <span class="o">=</span> <span class="s2">"123456789"</span>  # Replace with your AWS Account ID
 target_type        <span class="o">=</span> <span class="s2">"AWS_ACCOUNT"</span>
<span class="o">}</span>

<span class="c"># Ensure the user is part of the required group</span>
data <span class="s2">"aws_identitystore_group"</span> <span class="s2">"request1"</span> <span class="o">{</span>
 identity_store_id <span class="o">=</span> data.aws_ssoadmin_instances.main.identity_store_id
 display_name      <span class="o">=</span> <span class="s2">"AD-RDS-READ-ONLY"</span>
<span class="o">}</span>

resource <span class="s2">"aws_ssoadmin_group_membership"</span> <span class="s2">"request1"</span> <span class="o">{</span>
 identity_store_id <span class="o">=</span> data.aws_ssoadmin_instances.main.identity_store_id
 group_id          <span class="o">=</span> data.aws_identitystore_group.request1.group_id
 user_ids          <span class="o">=</span> <span class="o">[</span><span class="s2">"user1@company.com"</span><span class="o">]</span>
<span class="o">}</span>

<span class="c"># handle request 2 for ACM full access for whole security team in all 3 AWS accounts </span>

vim main.tf

<span class="c"># use existing main.tf file  </span>
<span class="c"># provider "aws" {</span>
<span class="c">#  region = "ap-southeast-2"</span>
<span class="c"># }</span>
<span class="c"># data "aws_ssoadmin_instances" "main" {}</span>

<span class="c"># Create the Permission Set for ACM Full Access</span>
resource <span class="s2">"aws_ssoadmin_permission_set"</span> <span class="s2">"acm_full_access"</span> <span class="o">{</span>
 instance_arn <span class="o">=</span> data.aws_ssoadmin_instances.main.arns[0]
 name         <span class="o">=</span> <span class="s2">"ACM-FullAccess"</span>
 description  <span class="o">=</span> <span class="s2">"Full access to AWS Certificate Manager"</span>
 session_duration <span class="o">=</span> <span class="s2">"PT1H"</span>

 managed_policies <span class="o">=</span> <span class="o">[</span>
 <span class="s2">"arn:aws:iam::aws:policy/AWSCertificateManagerFullAccess"</span>,
 <span class="o">]</span>
<span class="o">}</span>

<span class="c"># List of AWS account IDs</span>
<span class="c"># here we use terrafom "locals", "dynamic" and "for_each" to loop SSO user assignment for security team within all AWS accounts </span>
locals <span class="o">{</span>
 aws_account_ids <span class="o">=</span> <span class="o">[</span><span class="s2">"12345678901"</span>, <span class="s2">"12345678902"</span>, <span class="s2">"12345678903"</span><span class="o">]</span>
<span class="o">}</span>

<span class="c"># Security team members</span>
locals <span class="o">{</span>
 security_team_members <span class="o">=</span> <span class="o">[</span><span class="s2">"security1@company.com"</span>, <span class="s2">"security2@company.com"</span>, <span class="s2">"security3@company.com"</span><span class="o">]</span>
<span class="o">}</span>

<span class="c"># Ensure the users are part of the required group</span>
data <span class="s2">"aws_identitystore_group"</span> <span class="s2">"acm_full_access_group"</span> <span class="o">{</span>
 identity_store_id <span class="o">=</span> data.aws_ssoadmin_instances.main.identity_store_id
 display_name      <span class="o">=</span> <span class="s2">"AD-ACM-FULL-ACCESS"</span>
<span class="o">}</span>

resource <span class="s2">"aws_ssoadmin_group_membership"</span> <span class="s2">"acm_full_access_membership"</span> <span class="o">{</span>
 identity_store_id <span class="o">=</span> data.aws_ssoadmin_instances.main.identity_store_id
 group_id          <span class="o">=</span> data.aws_identitystore_group.acm_full_access_group.group_id
 user_ids          <span class="o">=</span> local.security_team_members
<span class="o">}</span>

<span class="c"># Assign the Permission Set to Each User for Each Account</span>
resource <span class="s2">"aws_ssoadmin_account_assignment"</span> <span class="s2">"acm_full_access_assignments"</span> <span class="o">{</span>
 for_each <span class="o">=</span> <span class="o">{</span> <span class="k">for </span>acc_id <span class="k">in </span>local.aws_account_ids : acc_id <span class="o">=&gt;</span> acc_id <span class="o">}</span>

 instance_arn       <span class="o">=</span> data.aws_ssoadmin_instances.main.arns[0]
 permission_set_arn <span class="o">=</span> aws_ssoadmin_permission_set.acm_full_access.arn
 principal_type     <span class="o">=</span> <span class="s2">"USER"</span>
 target_type        <span class="o">=</span> <span class="s2">"AWS_ACCOUNT"</span>

 dynamic <span class="s2">"assignment"</span> <span class="o">{</span>
 for_each <span class="o">=</span> local.security_team_members
 content <span class="o">{</span>
 principal_id <span class="o">=</span> assignment.value
 target_id    <span class="o">=</span> each.key
 <span class="o">}</span>
 <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

<p><b> Terraform Modularity </b></p>

<p>How about the Terraform module, as I will get different user assignment requests with different permission sets and AWS accounts? I guess a Terraform module for SSO user assignment is the best way to make the Terraform code more clean and reuseable. There are many benefits to infrastructure as code with modularity. It can reduce code duplication, is easy to update, and has a clear code structure, which fits my AWS SSO user assignment task and challenge perfectly.</p>

<p>To achieve this, I will need to create a folder called “sso_user_assignment_module”, inside the folder it will contain:</p>

<ul>
  <li>A “main.tf” file to define the resources for creating permission sets and assigning them to users.</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># modules/sso_account_assignment/main.tf</span>

provider <span class="s2">"aws"</span> <span class="o">{</span>
 region <span class="o">=</span> var.aws_region
<span class="o">}</span>

data <span class="s2">"aws_ssoadmin_instances"</span> <span class="s2">"main"</span> <span class="o">{}</span>

resource <span class="s2">"aws_ssoadmin_permission_set"</span> <span class="s2">"this"</span> <span class="o">{</span>
 for_each <span class="o">=</span> var.permission_sets

 instance_arn <span class="o">=</span> data.aws_ssoadmin_instances.main.arns[0]
 name         <span class="o">=</span> each.key
 description  <span class="o">=</span> each.value.description
 session_duration <span class="o">=</span> each.value.session_duration

 managed_policies <span class="o">=</span> each.value.managed_policies
<span class="o">}</span>

resource <span class="s2">"aws_ssoadmin_account_assignment"</span> <span class="s2">"this"</span> <span class="o">{</span>
 for_each <span class="o">=</span> <span class="o">{</span> <span class="k">for </span>ps_key, ps_value <span class="k">in </span>var.permission_sets : ps_key <span class="o">=&gt;</span> ps_value.accounts <span class="o">}</span>

 instance_arn       <span class="o">=</span> data.aws_ssoadmin_instances.main.arns[0]
 permission_set_arn <span class="o">=</span> aws_ssoadmin_permission_set.this[each.key].arn
 principal_type     <span class="o">=</span> <span class="s2">"USER"</span>
 target_type        <span class="o">=</span> <span class="s2">"AWS_ACCOUNT"</span>

 dynamic <span class="s2">"assignment"</span> <span class="o">{</span>
 for_each <span class="o">=</span> each.value.users
 content <span class="o">{</span>
 principal_id <span class="o">=</span> assignment.value
 target_id    <span class="o">=</span> each.value.account_id
 <span class="o">}</span>
 <span class="o">}</span>
<span class="o">}</span>

data <span class="s2">"aws_identitystore_group"</span> <span class="s2">"this"</span> <span class="o">{</span>
 for_each <span class="o">=</span> var.groups

 identity_store_id <span class="o">=</span> data.aws_ssoadmin_instances.main.identity_store_id
 display_name      <span class="o">=</span> each.key
<span class="o">}</span>

resource <span class="s2">"aws_ssoadmin_group_membership"</span> <span class="s2">"this"</span> <span class="o">{</span>
 for_each <span class="o">=</span> var.groups

 identity_store_id <span class="o">=</span> data.aws_ssoadmin_instances.main.identity_store_id
 group_id          <span class="o">=</span> data.aws_identitystore_group.this[each.key].group_id
 user_ids          <span class="o">=</span> each.value
<span class="o">}</span></code></pre></figure>

<ul>
  <li>A “variables.tf” to define the input variables for the module</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># variables.tf</span>
vim variables.tf

provider <span class="s2">"aws"</span> <span class="o">{</span>
 region <span class="o">=</span> var.region
<span class="o">}</span>

variable <span class="s2">"sso_instance_arn"</span> <span class="o">{</span>
 description <span class="o">=</span> <span class="s2">"The ARN of the AWS SSO instance"</span>
 <span class="nb">type</span>    <span class="o">=</span> string
<span class="o">}</span>

variable <span class="s2">"assignments"</span> <span class="o">{</span>
 description <span class="o">=</span> <span class="s2">"Map of account IDs to users and their permission sets"</span>
 <span class="nb">type</span>    <span class="o">=</span> map<span class="o">(</span>list<span class="o">(</span>object<span class="o">({</span>
 principal_id   <span class="o">=</span> string
 permission_set_arn <span class="o">=</span> string
 <span class="o">})))</span>
<span class="o">}</span>

variable <span class="s2">"region"</span> <span class="o">{</span>
 description <span class="o">=</span> <span class="s2">"AWS region"</span>
 <span class="nb">type</span>    <span class="o">=</span> string
 default  <span class="o">=</span> <span class="s2">"ap-southeast-2"</span>
<span class="o">}</span>

module <span class="s2">"sso_account_assignments"</span> <span class="o">{</span>
 <span class="nb">source</span> <span class="o">=</span> <span class="s2">"./modules/sso_account_assignment"</span>

 for_each    <span class="o">=</span> var.assignments
 sso_instance_arn <span class="o">=</span> var.sso_instance_arn
 account_id   <span class="o">=</span> each.key
 <span class="nb">users</span>     <span class="o">=</span> each.value
<span class="o">}</span></code></pre></figure>

<ul>
  <li>A “outputs.tf” file to define the outputs of the module.</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">vim outputs.tf
<span class="c"># define outputs of permission_set_arns and group_ids</span>
output <span class="s2">"permission_set_arns"</span> <span class="o">{</span>
 value <span class="o">=</span> <span class="o">{</span> <span class="k">for </span>k, v <span class="k">in </span>aws_ssoadmin_permission_set.this : k <span class="o">=&gt;</span> v.arn <span class="o">}</span>
<span class="o">}</span>

output <span class="s2">"group_ids"</span> <span class="o">{</span>
 value <span class="o">=</span> <span class="o">{</span> <span class="k">for </span>k, v <span class="k">in </span>data.aws_identitystore_group.this : k <span class="o">=&gt;</span> v.group_id <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

<ul>
  <li>now we need to Create a Terraform configuration that uses this module and set the environment variables accordingly. go back to the root folder, create a root “main.tf” file to call the module and pass the necessary variables.</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">cd</span> ..
vim main.tf
<span class="c"># the root main.tf file</span>
module <span class="s2">"sso_permission_sets"</span> <span class="o">{</span>
 <span class="nb">source</span> <span class="o">=</span> <span class="s2">"./modules/aws_sso_permission_sets"</span>

 aws_region       <span class="o">=</span> var.aws_region
 permission_sets  <span class="o">=</span> var.permission_sets
 <span class="nb">groups</span>           <span class="o">=</span> var.groups
<span class="o">}</span>

<span class="c"># Optionally output the values</span>
output <span class="s2">"permission_set_arns"</span> <span class="o">{</span>
 value <span class="o">=</span> module.sso_permission_sets.permission_set_arns
<span class="o">}</span>

output <span class="s2">"group_ids"</span> <span class="o">{</span>
 value <span class="o">=</span> module.sso_permission_sets.group_ids
<span class="o">}</span></code></pre></figure>

<ul>
  <li>root “variables.tf” file to define the input variables for the root configuration.</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># the root variables.tf</span>
vim variables.tf

variable <span class="s2">"aws_region"</span> <span class="o">{</span>
 description <span class="o">=</span> <span class="s2">"The AWS region to use."</span>
 <span class="nb">type</span>        <span class="o">=</span> string
 default     <span class="o">=</span> <span class="s2">"ap-southeast-2"</span>
<span class="o">}</span>

variable <span class="s2">"permission_sets"</span> <span class="o">{</span>
 description <span class="o">=</span> <span class="s2">"A map of permission sets with their configurations."</span>
 <span class="nb">type</span> <span class="o">=</span> map<span class="o">(</span>object<span class="o">({</span>
 description     <span class="o">=</span> string
 session_duration <span class="o">=</span> string
 managed_policies <span class="o">=</span> list<span class="o">(</span>string<span class="o">)</span>
 accounts        <span class="o">=</span> map<span class="o">(</span>object<span class="o">({</span>
 account_id <span class="o">=</span> string
 <span class="nb">users</span>      <span class="o">=</span> list<span class="o">(</span>string<span class="o">)</span>
 <span class="o">}))</span>
 <span class="o">}))</span>
<span class="o">}</span>

variable <span class="s2">"groups"</span> <span class="o">{</span>
 description <span class="o">=</span> <span class="s2">"A map of groups with their associated user emails."</span>
 <span class="nb">type</span>        <span class="o">=</span> map<span class="o">(</span>list<span class="o">(</span>string<span class="o">))</span>
<span class="o">}</span></code></pre></figure>

<ul>
  <li>now is the place we can reuse the module to create the root “terraform.tfvars” which provides the actual values for the variables to define each assignment request. in future we only set each request here as environment variables, and then apply the terraform module.</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">aws_region <span class="o">=</span> <span class="s2">"ap-southeast-2"</span>

permission_sets <span class="o">=</span> <span class="o">{</span>
 <span class="c"># The 1st request RDS read-only permission sets and user assignment redefine in the module using variables</span>
 <span class="s2">"RDS-ReadOnly"</span> <span class="o">=</span> <span class="o">{</span>
 description     <span class="o">=</span> <span class="s2">"Read-only access to RDS resources"</span>
 session_duration <span class="o">=</span> <span class="s2">"PT1H"</span>
 managed_policies <span class="o">=</span> <span class="o">[</span>
 <span class="s2">"arn:aws:iam::aws:policy/AmazonRDSReadOnlyAccess"</span>,
 <span class="o">]</span>
 accounts <span class="o">=</span> <span class="o">{</span>
 <span class="s2">"123456789"</span> <span class="o">=</span> <span class="o">{</span>
 account_id <span class="o">=</span> <span class="s2">"123456789"</span>
 <span class="nb">users</span>      <span class="o">=</span> <span class="o">[</span><span class="s2">"user1@company.com"</span><span class="o">]</span>
 <span class="o">}</span>
 <span class="o">}</span>
 <span class="o">}</span>
 <span class="c"># the 2nd request security full access for ACM redefine in the module using variables</span>
 <span class="s2">"ACM-FullAccess"</span> <span class="o">=</span> <span class="o">{</span>
 description     <span class="o">=</span> <span class="s2">"Full access to AWS Certificate Manager"</span>
 session_duration <span class="o">=</span> <span class="s2">"PT1H"</span>
 managed_policies <span class="o">=</span> <span class="o">[</span>
 <span class="s2">"arn:aws:iam::aws:policy/AWSCertificateManagerFullAccess"</span>,
 <span class="o">]</span>
 accounts <span class="o">=</span> <span class="o">{</span>
 <span class="s2">"12345678901"</span> <span class="o">=</span> <span class="o">{</span>
 account_id <span class="o">=</span> <span class="s2">"12345678901"</span>
 <span class="nb">users</span>      <span class="o">=</span> <span class="o">[</span><span class="s2">"security1@company.com"</span>, <span class="s2">"security2@company.com"</span>, <span class="s2">"security3@company.com"</span><span class="o">]</span>
 <span class="o">}</span>,
 <span class="s2">"12345678902"</span> <span class="o">=</span> <span class="o">{</span>
 account_id <span class="o">=</span> <span class="s2">"12345678902"</span>
 <span class="nb">users</span>      <span class="o">=</span> <span class="o">[</span><span class="s2">"security1@company.com"</span>, <span class="s2">"security2@company.com"</span>, <span class="s2">"security3@company.com"</span><span class="o">]</span>
 <span class="o">}</span>,
 <span class="s2">"12345678903"</span> <span class="o">=</span> <span class="o">{</span>
 account_id <span class="o">=</span> <span class="s2">"12345678903"</span>
 <span class="nb">users</span>      <span class="o">=</span> <span class="o">[</span><span class="s2">"security1@company.com"</span>, <span class="s2">"security2@company.com"</span>, <span class="s2">"security3@company.com"</span><span class="o">]</span>
 <span class="o">}</span>
 <span class="o">}</span>
 <span class="o">}</span>

 <span class="c"># Add 3rd request a developer needs S3 full access for 2 AWS accounts redefine in the module using variables</span>
 <span class="s2">"S3-ModifyAccess"</span> <span class="o">=</span> <span class="o">{</span>
 description     <span class="o">=</span> <span class="s2">"Modify access to S3 buckets"</span>
 session_duration <span class="o">=</span> <span class="s2">"PT1H"</span>
 managed_policies <span class="o">=</span> <span class="o">[</span>
 <span class="s2">"arn:aws:iam::aws:policy/AmazonS3FullAccess"</span>,
 <span class="o">]</span>
 accounts <span class="o">=</span> <span class="o">{</span>
 <span class="s2">"12345678902"</span> <span class="o">=</span> <span class="o">{</span>
 account_id <span class="o">=</span> <span class="s2">"12345678902"</span>
 <span class="nb">users</span>      <span class="o">=</span> <span class="o">[</span><span class="s2">"developer1@company.com"</span><span class="o">]</span>
 <span class="o">}</span>,
 <span class="s2">"12345678903"</span> <span class="o">=</span> <span class="o">{</span>
 account_id <span class="o">=</span> <span class="s2">"12345678903"</span>
 <span class="nb">users</span>      <span class="o">=</span> <span class="o">[</span><span class="s2">"developer1@company.com"</span><span class="o">]</span>
 <span class="o">}</span>
 <span class="o">}</span>
 <span class="o">}</span>
<span class="o">}</span>

<span class="nb">groups</span> <span class="o">=</span> <span class="o">{</span>
 <span class="s2">"AD-RDS-EAD-ONLY"</span> <span class="o">=</span> <span class="o">[</span><span class="s2">"user1@company.com"</span><span class="o">]</span>
 <span class="s2">"AD-ACM-FULL-ACCESS"</span> <span class="o">=</span> <span class="o">[</span><span class="s2">"security1@company.com"</span>, <span class="s2">"security2@company.com"</span>, <span class="s2">"security3@company.com"</span><span class="o">]</span>
 <span class="c"># Optionally add a group for the developer, if needed:</span>
 <span class="c"># "AD-S3-Modify-Access" = ["developer1@company.com"]</span>
<span class="o">}</span></code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now, we can achieve the task individually via terraform code and a Terraform module to handle the creation of AWS SSO users, This setup combines all three requests into a single Terraform configuration, leveraging the reusable module for creating permission sets and managing user assignments, it is more efficient, dynamically, and reusable. In future, we only define permission sets and maintain new users and assignments in the environment variables .tf file, then run Terraform apply to get the job done. The change also can be tracked when leveraging Git as version control.</p>

<p><a href="https://girishcodealchemy.medium.com/streamlining-aws-sso-in-complex-multi-account-environments-e82025792d11">Streamlining AWS SSO in Complex Multi-Account Environments</a></p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Challange About AWS SSO User Assignment]]></summary></entry><entry><title type="html">Python: File Handling for EC2 tagging</title><link href="http://localhost:4000/jekyll/cat2/2024/05/17/py2.html" rel="alternate" type="text/html" title="Python: File Handling for EC2 tagging" /><published>2024-05-17T10:15:29+10:00</published><updated>2024-05-17T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/17/py2</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/17/py2.html"><![CDATA[<p><b> About Python File Handling</b></p>

<p>In the previous post I developed shell script + awscli to apply <a href="https://zackz.online/jekyll/cat2/2024/05/01/AWS-tagging.html">aws EC2 tages</a>, since the last post we discovered <a href="https://zackz.online/jekyll/cat2/2024/05/16/py1.html">Python Boto3</a> scripts for AWS resource automation and management, I think it is time to improve the EC2 tagging task with Python and boto3, together with file handling to achieve:</p>

<ol>
  <li>
    <p>List and export ec2 information to a CSV file (instanceID, default instance name, Existing tags)</p>
  </li>
  <li>
    <p>define 4 mandatory tags in CSV header (Env, BizOwner, Technology, Project)</p>
  </li>
  <li>
    <p>validate exported tags against the 4 mandatory new tags, if any of the new mandatory tags exists, then keep the tage and value, if any of the new mandatory tags do not exist, add the key and leave the value blank</p>
  </li>
  <li>
    <p>Get csv file fill with mandatory tags input from Biz team (manual work)</p>
  </li>
  <li>
    <p>open the updated CSV file, apply the mandatory tags based on the input value</p>
  </li>
  <li>
    <p>create and trigger lambda function with aws config rules to enforce 4 mandatory tags whenever a new instance launch  </p>
  </li>
</ol>

<p><b> List and export ec2 information to a CSV</b></p>

<ul>
  <li>Here we need Python libraries for “boto3” and “csv”, to call boto3 sessions to retrieve EC2 information, then use Python “with open” and “for” loops to write each ec2 info to a csv file, also add mandatory tags write in the header fields “Env”, “BizOwner”, “Technology”, “Project”:</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~/pythonwork/new# vim export1.py
<span class="c"># Import libiaries</span>
import boto3
import csv
<span class="c"># Define the mandatory tags</span>
MANDATORY_TAGS <span class="o">=</span> <span class="o">[</span><span class="s2">"Env"</span>, <span class="s2">"BizOwner"</span>, <span class="s2">"Technology"</span>, <span class="s2">"Project"</span><span class="o">]</span>
<span class="c"># Initialize boto3 clients</span>
ec2 <span class="o">=</span> boto3.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>

def list_ec2_instances<span class="o">()</span>:
    instances <span class="o">=</span> <span class="o">[]</span>
    response <span class="o">=</span> ec2.describe_instances<span class="o">()</span>
    <span class="k">for </span>reservation <span class="k">in </span>response[<span class="s1">'Reservations'</span><span class="o">]</span>:
        <span class="k">for </span>instance <span class="k">in </span>reservation[<span class="s1">'Instances'</span><span class="o">]</span>:
            instance_id <span class="o">=</span> instance[<span class="s1">'InstanceId'</span><span class="o">]</span>
            default_name <span class="o">=</span> next<span class="o">((</span>tag[<span class="s1">'Value'</span><span class="o">]</span> <span class="k">for </span>tag <span class="k">in </span>instance.get<span class="o">(</span><span class="s1">'Tags'</span>, <span class="o">[])</span> <span class="k">if </span>tag[<span class="s1">'Key'</span><span class="o">]</span> <span class="o">==</span> <span class="s1">'Name'</span><span class="o">)</span>, <span class="s1">'No Name'</span><span class="o">)</span>
            tags <span class="o">=</span> <span class="o">{</span>tag[<span class="s1">'Key'</span><span class="o">]</span>: tag[<span class="s1">'Value'</span><span class="o">]</span> <span class="k">for </span>tag <span class="k">in </span>instance.get<span class="o">(</span><span class="s1">'Tags'</span>, <span class="o">[])}</span>
            instance_info <span class="o">=</span> <span class="o">{</span>
                <span class="s1">'InstanceId'</span>: instance_id,
                <span class="s1">'DefaultName'</span>: default_name,
                <span class="k">**</span>tags
            <span class="o">}</span>
            <span class="c"># Ensure mandatory tags are included with empty values if not present</span>
            <span class="k">for </span>mandatory_tag <span class="k">in </span>MANDATORY_TAGS:
                <span class="k">if </span>mandatory_tag not <span class="k">in </span>instance_info:
                    instance_info[mandatory_tag] <span class="o">=</span> <span class="s1">''</span>
            instances.append<span class="o">(</span>instance_info<span class="o">)</span>
    <span class="k">return </span>instances

<span class="c"># Define export to csv</span>
def export_to_csv<span class="o">(</span>instances, <span class="nv">filename</span><span class="o">=</span><span class="s1">'ec2_instances.csv'</span><span class="o">)</span>:
    <span class="c"># Collect all possible tag keys</span>
    all_tags <span class="o">=</span> <span class="nb">set</span><span class="o">()</span>
    <span class="k">for </span>instance <span class="k">in </span>instances:
        all_tags.update<span class="o">(</span>instance.keys<span class="o">())</span>
    
    <span class="c"># Ensure mandatory tags are included in the header</span>
    all_tags.update<span class="o">(</span>MANDATORY_TAGS<span class="o">)</span>
    fieldnames <span class="o">=</span> <span class="o">[</span><span class="s1">'InstanceId'</span>, <span class="s1">'DefaultName'</span><span class="o">]</span> + sorted<span class="o">(</span>all_tags - <span class="o">{</span><span class="s1">'InstanceId'</span>, <span class="s1">'DefaultName'</span><span class="o">})</span>
    
    with open<span class="o">(</span>filename, <span class="s1">'w'</span>, <span class="nv">newline</span><span class="o">=</span><span class="s1">''</span><span class="o">)</span> as csvfile:
        writer <span class="o">=</span> csv.DictWriter<span class="o">(</span>csvfile, <span class="nv">fieldnames</span><span class="o">=</span>fieldnames<span class="o">)</span>
        writer.writeheader<span class="o">()</span>
        <span class="k">for </span>instance <span class="k">in </span>instances:
            writer.writerow<span class="o">(</span>instance<span class="o">)</span>

def main<span class="o">()</span>:
    instances <span class="o">=</span> list_ec2_instances<span class="o">()</span>
    export_to_csv<span class="o">(</span>instances<span class="o">)</span>
    print<span class="o">(</span><span class="s2">"CSV export complete. Please update the mandatory tags in 'ec2_instances.csv'."</span><span class="o">)</span>

<span class="k">if </span>__name__ <span class="o">==</span> <span class="s1">'__main__'</span>:
    main<span class="o">()</span>

root@ubt-server:~/pythonwork/new# python3 export1.py 
CSV <span class="nb">export </span>complete. Please update the mandatory tags <span class="k">in</span> <span class="s1">'ec2_instances.csv'</span>.</code></pre></figure>

<p><img src="/assets/py2-1.png" alt="image tooltip here" /></p>

<ul>
  <li>next download and update ‘ec2_instances.csv’ with all required tags, then remane and upload as ‘ec2_instances_updated.csv’, create second script “update1.py” to apply new tags</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~/pythonwork/new# vim update1.py

import boto3
import csv

<span class="c"># Define the mandatory tags</span>
MANDATORY_TAGS <span class="o">=</span> <span class="o">[</span><span class="s2">"Env"</span>, <span class="s2">"BizOwner"</span>, <span class="s2">"Technology"</span>, <span class="s2">"Project"</span><span class="o">]</span>

def update_tags_from_csv<span class="o">(</span><span class="nv">filename</span><span class="o">=</span><span class="s1">'ec2_instances_updated.csv'</span><span class="o">)</span>:
    ec2 <span class="o">=</span> boto3.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>
    with open<span class="o">(</span>filename, <span class="nv">newline</span><span class="o">=</span><span class="s1">''</span><span class="o">)</span> as csvfile:
        reader <span class="o">=</span> csv.DictReader<span class="o">(</span>csvfile<span class="o">)</span>
        <span class="k">for </span>row <span class="k">in </span>reader:
            instance_id <span class="o">=</span> row[<span class="s1">'InstanceId'</span><span class="o">]</span>
            tags <span class="o">=</span> <span class="o">[{</span><span class="s1">'Key'</span>: tag, <span class="s1">'Value'</span>: row[tag]<span class="o">}</span> <span class="k">for </span>tag <span class="k">in </span>MANDATORY_TAGS <span class="k">if </span>row[tag]]
            <span class="k">if </span>tags:
                ec2.create_tags<span class="o">(</span><span class="nv">Resources</span><span class="o">=[</span>instance_id], <span class="nv">Tags</span><span class="o">=</span>tags<span class="o">)</span>

def main<span class="o">()</span>:
    update_tags_from_csv<span class="o">()</span>
    print<span class="o">(</span><span class="s2">"Tags updated successfully from 'ec2_instances_updated.csv'."</span><span class="o">)</span>

<span class="k">if </span>__name__ <span class="o">==</span> <span class="s1">'__main__'</span>:
    main<span class="o">()</span>

root@ubt-server:~/pythonwork/new# python3 update1.py 
Tags updated successfully from <span class="s1">'ec2_instances_updated.csv'</span>.</code></pre></figure>

<p>Now the new tags had been applied.</p>

<p><img src="/assets/py2-2.png" alt="image tooltip here" /></p>

<p><b> How about managing tags for multiple AWS accounts </b></p>

<p>Considering we have 20+ AWS accounts across the company and with more than 200 EC2 instances that need to apply tagging strategy, here I will</p>

<ul>
  <li>use AWScli profile to configure each AWS account creds, here I will use my own 2 AWS accounts (ZackBlog and JoeSite) to create AWScli profiles to validate the Python scripts</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Add account creds into ~/.aws/credentials</span>
vim ~/.aws/credentials

<span class="o">[</span>aws_account_zackblog]
aws_access_key_id <span class="o">=</span> xxxx
aws_secret_access_key <span class="o">=</span> yyyy

<span class="o">[</span>aws_account_joesite]
aws_access_key_id <span class="o">=</span> zzzz
aws_secret_access_key <span class="o">=</span> yyyy</code></pre></figure>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># add profiles into ~/.aws/config</span>
vim ~/.aws/config

<span class="o">[</span>profile aws_account_zackblog]
region <span class="o">=</span> ap-southeast-2

<span class="o">[</span>profile aws_account_joesite]
region <span class="o">=</span> ap-southeast-2</code></pre></figure>

<ul>
  <li>now update Python scripts to call each account profile to apply all 20+ AWS accounts in sequence.</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~/pythonwork# <span class="nb">mkdir </span>mutiple-aws
root@ubt-server:~/pythonwork# <span class="nb">cd </span>mutiple-aws/
root@ubt-server:~/pythonwork/mutiple-aws# vim export2.py

import boto3
import csv
from botocore.exceptions import ProfileNotFound

<span class="c"># Define the mandatory tags</span>
MANDATORY_TAGS <span class="o">=</span> <span class="o">[</span><span class="s2">"Env"</span>, <span class="s2">"BizOwner"</span>, <span class="s2">"Technology"</span>, <span class="s2">"Project"</span><span class="o">]</span>

<span class="c"># List of AWS account profiles</span>
AWS_PROFILES <span class="o">=</span> <span class="o">[</span><span class="s2">"aws_account_zackblog"</span>, <span class="s2">"aws_account_joesite"</span><span class="o">]</span>  <span class="c"># Add more profiles as needed</span>

def list_ec2_instances<span class="o">(</span>profile_name<span class="o">)</span>:
    session <span class="o">=</span> boto3.Session<span class="o">(</span><span class="nv">profile_name</span><span class="o">=</span>profile_name<span class="o">)</span>
    ec2 <span class="o">=</span> session.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>
    instances <span class="o">=</span> <span class="o">[]</span>
    response <span class="o">=</span> ec2.describe_instances<span class="o">()</span>
    <span class="k">for </span>reservation <span class="k">in </span>response[<span class="s1">'Reservations'</span><span class="o">]</span>:
        <span class="k">for </span>instance <span class="k">in </span>reservation[<span class="s1">'Instances'</span><span class="o">]</span>:
            instance_id <span class="o">=</span> instance[<span class="s1">'InstanceId'</span><span class="o">]</span>
            default_name <span class="o">=</span> next<span class="o">((</span>tag[<span class="s1">'Value'</span><span class="o">]</span> <span class="k">for </span>tag <span class="k">in </span>instance.get<span class="o">(</span><span class="s1">'Tags'</span>, <span class="o">[])</span> <span class="k">if </span>tag[<span class="s1">'Key'</span><span class="o">]</span> <span class="o">==</span> <span class="s1">'Name'</span><span class="o">)</span>, <span class="s1">'No Name'</span><span class="o">)</span>
            tags <span class="o">=</span> <span class="o">{</span>tag[<span class="s1">'Key'</span><span class="o">]</span>: tag[<span class="s1">'Value'</span><span class="o">]</span> <span class="k">for </span>tag <span class="k">in </span>instance.get<span class="o">(</span><span class="s1">'Tags'</span>, <span class="o">[])}</span>
            instance_info <span class="o">=</span> <span class="o">{</span>
                <span class="s1">'InstanceId'</span>: instance_id,
                <span class="s1">'DefaultName'</span>: default_name,
                <span class="k">**</span>tags
            <span class="o">}</span>
            <span class="c"># Ensure mandatory tags are included with empty values if not present</span>
            <span class="k">for </span>mandatory_tag <span class="k">in </span>MANDATORY_TAGS:
                <span class="k">if </span>mandatory_tag not <span class="k">in </span>instance_info:
                    instance_info[mandatory_tag] <span class="o">=</span> <span class="s1">''</span>
            instances.append<span class="o">(</span>instance_info<span class="o">)</span>
    <span class="k">return </span>instances

def export_to_csv<span class="o">(</span>instances, profile_name<span class="o">)</span>:
    filename <span class="o">=</span> f<span class="s2">"ec2_instances_{profile_name}.csv"</span>
    <span class="c"># Collect all possible tag keys</span>
    all_tags <span class="o">=</span> <span class="nb">set</span><span class="o">()</span>
    <span class="k">for </span>instance <span class="k">in </span>instances:
        all_tags.update<span class="o">(</span>instance.keys<span class="o">())</span>
    
    <span class="c"># Ensure mandatory tags are included in the header</span>
    all_tags.update<span class="o">(</span>MANDATORY_TAGS<span class="o">)</span>
    fieldnames <span class="o">=</span> <span class="o">[</span><span class="s1">'InstanceId'</span>, <span class="s1">'DefaultName'</span><span class="o">]</span> + sorted<span class="o">(</span>all_tags - <span class="o">{</span><span class="s1">'InstanceId'</span>, <span class="s1">'DefaultName'</span><span class="o">})</span>
    
    with open<span class="o">(</span>filename, <span class="s1">'w'</span>, <span class="nv">newline</span><span class="o">=</span><span class="s1">''</span><span class="o">)</span> as csvfile:
        writer <span class="o">=</span> csv.DictWriter<span class="o">(</span>csvfile, <span class="nv">fieldnames</span><span class="o">=</span>fieldnames<span class="o">)</span>
        writer.writeheader<span class="o">()</span>
        <span class="k">for </span>instance <span class="k">in </span>instances:
            writer.writerow<span class="o">(</span>instance<span class="o">)</span>

def process_all_profiles<span class="o">()</span>:
    <span class="k">for </span>profile <span class="k">in </span>AWS_PROFILES:
        try:
            print<span class="o">(</span>f<span class="s2">"Processing profile: {profile}"</span><span class="o">)</span>
            instances <span class="o">=</span> list_ec2_instances<span class="o">(</span>profile<span class="o">)</span>
            export_to_csv<span class="o">(</span>instances, profile<span class="o">)</span>
            print<span class="o">(</span>f<span class="s2">"CSV export complete for profile {profile}. Please update the mandatory tags in 'ec2_instances_{profile}.csv'."</span><span class="o">)</span>
        except ProfileNotFound:
            print<span class="o">(</span>f<span class="s2">"Profile {profile} not found. Skipping."</span><span class="o">)</span>

<span class="k">if </span>__name__ <span class="o">==</span> <span class="s1">'__main__'</span>:
    process_all_profiles<span class="o">()</span></code></pre></figure>

<ul>
  <li>export 2 csv files for each aws account based on given profile, update mandatory tags in the 2 csv files, then upload and rename as <em>updated</em></li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># export 2 csv files</span>
root@ubt-server:~/pythonwork/mutiple-aws# python3 export2.py 
Processing profile: aws_account_zackblog
CSV <span class="nb">export complete </span><span class="k">for </span>profile aws_account_zackblog. Please update the mandatory tags <span class="k">in</span> <span class="s1">'ec2_instances_aws_account_zackblog.csv'</span><span class="nb">.</span>
Processing profile: aws_account_joesite
CSV <span class="nb">export complete </span><span class="k">for </span>profile aws_account_joesite. Please update the mandatory tags <span class="k">in</span> <span class="s1">'ec2_instances_aws_account_joesite.csv'</span><span class="nb">.</span>

<span class="c"># update all mandatory tags in the files</span>
root@ubt-server:~/pythonwork/mutiple-aws# <span class="nb">cat </span>ec2_instances_updated_aws_account_zackblog.csv 

InstanceId,DefaultName,BizOwner,Env,Name,Project,Technology,Tuned,zz1,zz2
i-076226daa5aaf7cf2,zack-blog,Zack,Prod,zack-blog,zack-web,Jekyll,,aa1,aa2
i-0b5c0fec84073a6d9,Py_test_zackweb,Zack,Testing,Py_test_zackweb,python-test,None,Yes,,

root@ubt-server:~/pythonwork/mutiple-aws# <span class="nb">cat </span>ec2_instances_updated_aws_account_joesite.csv 

InstanceId,DefaultName,BizOwner,Env,Location,Name,Project,Technology,TimeLaunched
i-012fb886802435ff2,joe-account-py-test,Joe,Prod,SYD,joe-account-py-test,joesite,Ruby-Jekyll,
i-052b0511339457efc,joe-site,Joe,Testing,,joe-site,Python-test,None,20240301</code></pre></figure>

<ul>
  <li>now create python script “update_tags_2.p” to apply new tags for 2 aws accounts by given profile</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~/pythonwork/mutiple-aws# vim update_tags_2.py
<span class="c"># Import libiaries</span>
import boto3
import csv
from botocore.exceptions import ProfileNotFound

<span class="c"># Define the mandatory tags</span>
MANDATORY_TAGS <span class="o">=</span> <span class="o">[</span><span class="s2">"Env"</span>, <span class="s2">"BizOwner"</span>, <span class="s2">"Technology"</span>, <span class="s2">"Project"</span><span class="o">]</span>

<span class="c"># List of AWS account profiles</span>
AWS_PROFILES <span class="o">=</span> <span class="o">[</span><span class="s2">"aws_account_zackblog"</span>, <span class="s2">"aws_account_joesite"</span><span class="o">]</span>  <span class="c"># Add more profiles as needed</span>

def update_tags_from_csv<span class="o">(</span>profile_name<span class="o">)</span>:
    filename <span class="o">=</span> f<span class="s2">"ec2_instances_updated_{profile_name}.csv"</span>
    session <span class="o">=</span> boto3.Session<span class="o">(</span><span class="nv">profile_name</span><span class="o">=</span>profile_name<span class="o">)</span>
    ec2 <span class="o">=</span> session.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>
    with open<span class="o">(</span>filename, <span class="nv">newline</span><span class="o">=</span><span class="s1">''</span><span class="o">)</span> as csvfile:
        reader <span class="o">=</span> csv.DictReader<span class="o">(</span>csvfile<span class="o">)</span>
        <span class="k">for </span>row <span class="k">in </span>reader:
            instance_id <span class="o">=</span> row[<span class="s1">'InstanceId'</span><span class="o">]</span>
            tags <span class="o">=</span> <span class="o">[{</span><span class="s1">'Key'</span>: tag, <span class="s1">'Value'</span>: row[tag]<span class="o">}</span> <span class="k">for </span>tag <span class="k">in </span>MANDATORY_TAGS <span class="k">if </span>row[tag]]
            <span class="k">if </span>tags:
                ec2.create_tags<span class="o">(</span><span class="nv">Resources</span><span class="o">=[</span>instance_id], <span class="nv">Tags</span><span class="o">=</span>tags<span class="o">)</span>

def process_all_profiles<span class="o">()</span>:
    <span class="k">for </span>profile <span class="k">in </span>AWS_PROFILES:
        try:
            print<span class="o">(</span>f<span class="s2">"Processing profile: {profile}"</span><span class="o">)</span>
            update_tags_from_csv<span class="o">(</span>profile<span class="o">)</span>
            print<span class="o">(</span>f<span class="s2">"Tags updated successfully from 'ec2_instances_updated_{profile}.csv' for profile {profile}."</span><span class="o">)</span>
        except ProfileNotFound:
            print<span class="o">(</span>f<span class="s2">"Profile {profile} not found. Skipping."</span><span class="o">)</span>
        except FileNotFoundError:
            print<span class="o">(</span>f<span class="s2">"Updated CSV file for profile {profile} not found. Skipping."</span><span class="o">)</span>

<span class="k">if </span>__name__ <span class="o">==</span> <span class="s1">'__main__'</span>:
    process_all_profiles<span class="o">()</span>

<span class="c"># run to apply tags</span>
root@ubt-server:~/pythonwork/mutiple-aws# python3 update_tags_2.py 
Processing profile: aws_account_zackblog
Tags updated successfully from <span class="s1">'ec2_instances_updated_aws_account_zackblog.csv'</span> <span class="k">for </span>profile aws_account_zackblog.

Processing profile: aws_account_joesite
Tags updated successfully from <span class="s1">'ec2_instances_updated_aws_account_joesite.csv'</span> <span class="k">for </span>profile aws_account_joesite.</code></pre></figure>

<ul>
  <li>now double check the tags for both accounts</li>
</ul>

<p><img src="/assets/py2-3.png" alt="image tooltip here" />
<img src="/assets/py2-4.png" alt="image tooltip here" /></p>

<p><b> Conclusion</b></p>

<p>Now we can use Python Boto3 and file handling to achieve mutiple-aws account EC2 tagging. With Python “csv” library, functions like “csv.DictReader”, “with open” and “csv.DictWriter” to open, update and export CSV file, Python also supports handling data in JSON format with dictionary.</p>

<p>In the next post I will see how to use Python Flask to redesign Zack’s blog for Web application development.</p>

<h1>====================</h1>

<p><b> Just got another task to create more users in all account with python boto3</b></p>

<ul>
  <li>a csv file with list of users will be created, aws profile configured for all aws accounts</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~/pythonwork/user_creation# <span class="nb">cat </span>all_users.csv 
Username
ZackZ
BobJ
MattS</code></pre></figure>

<ul>
  <li>a python script with bellow IAM user creation</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~/pythonwork/user_creation# vim more_user_from_csv.py

import boto3
import csv
from botocore.exceptions import ProfileNotFound, ClientError

<span class="c"># List of AWS account profiles</span>
AWS_PROFILES <span class="o">=</span> <span class="o">[</span><span class="s2">"aws_account_zackblog"</span>, <span class="s2">"aws_account_joesite"</span><span class="o">]</span>  <span class="c"># Add more profiles as needed</span>

<span class="c"># IAM User details</span>
PASSWORD <span class="o">=</span> <span class="s2">"xxxxxxxx"</span>
POLICY_ARN <span class="o">=</span> <span class="s2">"arn:aws:iam::aws:policy/AdministratorAccess"</span>
CSV_FILE <span class="o">=</span> <span class="s2">"all_users.csv"</span>

def read_users_from_csv<span class="o">(</span>filename<span class="o">)</span>:
    <span class="nb">users</span> <span class="o">=</span> <span class="o">[]</span>
    with open<span class="o">(</span>filename, <span class="nv">newline</span><span class="o">=</span><span class="s1">''</span><span class="o">)</span> as csvfile:
        reader <span class="o">=</span> csv.DictReader<span class="o">(</span>csvfile<span class="o">)</span>
        <span class="k">for </span>row <span class="k">in </span>reader:
            users.append<span class="o">(</span>row[<span class="s1">'Username'</span><span class="o">])</span>
    <span class="k">return </span><span class="nb">users

</span>def create_iam_user<span class="o">(</span>profile_name, user_name<span class="o">)</span>:
    session <span class="o">=</span> boto3.Session<span class="o">(</span><span class="nv">profile_name</span><span class="o">=</span>profile_name<span class="o">)</span>
    iam <span class="o">=</span> session.client<span class="o">(</span><span class="s1">'iam'</span><span class="o">)</span>
    
    try:
        <span class="c"># Create IAM user</span>
        iam.create_user<span class="o">(</span><span class="nv">UserName</span><span class="o">=</span>user_name<span class="o">)</span>
        print<span class="o">(</span>f<span class="s2">"User {user_name} created in profile {profile_name}."</span><span class="o">)</span>

        <span class="c"># Create login profile for console access</span>
        iam.create_login_profile<span class="o">(</span>
            <span class="nv">UserName</span><span class="o">=</span>user_name,
            <span class="nv">Password</span><span class="o">=</span>PASSWORD,
            <span class="nv">PasswordResetRequired</span><span class="o">=</span>False
        <span class="o">)</span>
        print<span class="o">(</span>f<span class="s2">"Login profile created for user {user_name} in profile {profile_name}."</span><span class="o">)</span>

        <span class="c"># Attach AdministratorAccess policy</span>
        iam.attach_user_policy<span class="o">(</span>
            <span class="nv">UserName</span><span class="o">=</span>user_name,
            <span class="nv">PolicyArn</span><span class="o">=</span>POLICY_ARN
        <span class="o">)</span>
        print<span class="o">(</span>f<span class="s2">"AdministratorAccess policy attached to user {user_name} in profile {profile_name}."</span><span class="o">)</span>

    except ClientError as e:
        <span class="k">if </span>e.response[<span class="s1">'Error'</span><span class="o">][</span><span class="s1">'Code'</span><span class="o">]</span> <span class="o">==</span> <span class="s1">'EntityAlreadyExists'</span>:
            print<span class="o">(</span>f<span class="s2">"User {user_name} already exists in profile {profile_name}."</span><span class="o">)</span>
        <span class="k">else</span>:
            print<span class="o">(</span>f<span class="s2">"Error creating user {user_name} in profile {profile_name}: {e}"</span><span class="o">)</span>

def process_all_profiles<span class="o">(</span><span class="nb">users</span><span class="o">)</span>:
    <span class="k">for </span>profile <span class="k">in </span>AWS_PROFILES:
        try:
            print<span class="o">(</span>f<span class="s2">"Processing profile: {profile}"</span><span class="o">)</span>
            <span class="k">for </span>user <span class="k">in </span><span class="nb">users</span>:
                create_iam_user<span class="o">(</span>profile, user<span class="o">)</span>
        except ProfileNotFound:
            print<span class="o">(</span>f<span class="s2">"Profile {profile} not found. Skipping."</span><span class="o">)</span>

<span class="k">if </span>__name__ <span class="o">==</span> <span class="s1">'__main__'</span>:
    <span class="nb">users</span> <span class="o">=</span> read_users_from_csv<span class="o">(</span>CSV_FILE<span class="o">)</span>
    process_all_profiles<span class="o">(</span><span class="nb">users</span><span class="o">)</span></code></pre></figure>

<ul>
  <li>Go to create all users</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~/pythonwork/user_creation# python3 more_user_from_csv.py 

Processing profile: aws_account_zackblog

User ZackZ created <span class="k">in </span>profile aws_account_zackblog.
Login profile created <span class="k">for </span>user ZackZ <span class="k">in </span>profile aws_account_zackblog.
AdministratorAccess policy attached to user ZackZ <span class="k">in </span>profile aws_account_zackblog.

User BobJ created <span class="k">in </span>profile aws_account_zackblog.
Login profile created <span class="k">for </span>user BobJ <span class="k">in </span>profile aws_account_zackblog.
AdministratorAccess policy attached to user BobJ <span class="k">in </span>profile aws_account_zackblog.

User MattS created <span class="k">in </span>profile aws_account_zackblog.
Login profile created <span class="k">for </span>user MattS <span class="k">in </span>profile aws_account_zackblog.
AdministratorAccess policy attached to user MattS <span class="k">in </span>profile aws_account_zackblog.

Processing profile: aws_account_joesite

User ZackZ created <span class="k">in </span>profile aws_account_joesite.
Login profile created <span class="k">for </span>user ZackZ <span class="k">in </span>profile aws_account_joesite.
AdministratorAccess policy attached to user ZackZ <span class="k">in </span>profile aws_account_joesite.

User BobJ created <span class="k">in </span>profile aws_account_joesite.
Login profile created <span class="k">for </span>user BobJ <span class="k">in </span>profile aws_account_joesite.
AdministratorAccess policy attached to user BobJ <span class="k">in </span>profile aws_account_joesite.

User MattS created <span class="k">in </span>profile aws_account_joesite.
Login profile created <span class="k">for </span>user MattS <span class="k">in </span>profile aws_account_joesite.
AdministratorAccess policy attached to user MattS <span class="k">in </span>profile aws_account_joesite.</code></pre></figure>

<p><img src="/assets/py2-5.png" alt="image tooltip here" /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Python File Handling]]></summary></entry><entry><title type="html">Python: Boto3 for AWS</title><link href="http://localhost:4000/jekyll/cat2/2024/05/16/py1.html" rel="alternate" type="text/html" title="Python: Boto3 for AWS" /><published>2024-05-16T10:15:29+10:00</published><updated>2024-05-16T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/16/py1</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/16/py1.html"><![CDATA[<p><b> About Boto3 </b></p>

<p>Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python. It enables developers to build software that uses Amazon services like EC2, S3, RDS, etc.</p>

<p>I will build a portable python3.9 + Boto3 docker environment to test some AWS automation tasks.</p>

<p><b> Build and run a docker with Python3.9 + Boto3 </b></p>

<p>As I do not want to install Python, Boto3, and AWScli on my local PC, creating a docker image with all software ready as a portable env is the best way to start.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~# vim Dockerfile
<span class="c"># Build from python:3.9.19-alpine3.19</span>
From python:3.9.19-alpine3.19 
<span class="c"># install boto3 and alwcli</span>
RUN pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip <span class="o">&amp;&amp;</span> <span class="se">\</span>
    pip <span class="nb">install</span> <span class="nt">--upgrade</span> awscli <span class="o">&amp;&amp;</span> <span class="se">\</span>
    pip <span class="nb">install</span> <span class="nt">--upgrade</span> boto3
<span class="c"># set work dir</span>
WORKDIR /work
<span class="c"># run Python</span>
CMD <span class="s2">"python"</span>
<span class="c"># build a docker image from the above Dockerfile </span>
root@ubt-server:~# docker image build <span class="nt">-t</span> zack_aws_boto3 <span class="nb">.</span>

<span class="c"># ls docker images </span>
root@ubt-server:~# docker image <span class="nb">ls
</span>REPOSITORY               TAG             IMAGE ID       CREATED         SIZE
zack_aws_boto3           v1              07a13f7801ed   1 days ago     998MB
zackpy                   latest          287ba6873741   4 days ago      48.2MB
zackz001/gitops-jekyll   latest          d92894f7be6d   6 days ago      70.9MB
postgres                 15.0            027eba2e8939   19 months ago   377MB

<span class="c"># run docker and mount local python work dir</span>
root@ubt-server:~/pythonwork# docker run <span class="nt">-ti</span> <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>:/work zack_aws_boto3:v1 bash
root@c04670a43564:/# 
root@c04670a43564:/# <span class="nb">cd </span>work <span class="o">&amp;&amp;</span> <span class="nb">ls</span>

<span class="c"># configure aws in the container</span>
root@c04670a43564:/work# aws configure 
AWS Access Key ID <span class="o">[</span><span class="k">****************</span>GFNW]: 
AWS Secret Access Key <span class="o">[</span><span class="k">****************</span>Db7O]: 
Default region name <span class="o">[</span>ap-southeast-2]: 
Default output format <span class="o">[</span>None]: 

<span class="c"># validate aws cred by listing ec2 instance id </span>
root@c04670a43564:/work# aws ec2 describe-instances <span class="nt">--query</span> <span class="s2">"Reservations[*].Instances[*].InstanceId"</span> <span class="nt">--output</span> json
<span class="o">[</span>
    <span class="o">[</span>
        <span class="s2">"i-076226daa5aaf7cf2"</span>
    <span class="o">]</span>
<span class="o">]</span></code></pre></figure>

<p><b>Manage AWS resource with Python Boto3 script</b></p>

<p>Here we have Python and boto3 env ready; I will list some aws tasks that I want to be achieved by Python scripts</p>

<ul>
  <li>List ec2 instance name, instanceID and state</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~/pythonwork# vim app.py
<span class="c"># import boto3 library</span>
import boto3

def list_ec2_instances<span class="o">()</span>:
    <span class="c"># Create a session using default AWS profile</span>
    session <span class="o">=</span> boto3.Session<span class="o">()</span>
    <span class="c"># Create an EC2 client</span>
    ec2_client <span class="o">=</span> session.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>

    <span class="c"># Describe EC2 instances</span>
    response <span class="o">=</span> ec2_client.describe_instances<span class="o">()</span>

    <span class="c"># Iterate over the instances</span>
    <span class="k">for </span>reservation <span class="k">in </span>response[<span class="s1">'Reservations'</span><span class="o">]</span>:
        <span class="k">for </span>instance <span class="k">in </span>reservation[<span class="s1">'Instances'</span><span class="o">]</span>:
            <span class="c"># Get the instance ID</span>
            instance_id <span class="o">=</span> instance[<span class="s1">'InstanceId'</span><span class="o">]</span>
            
            <span class="c"># Get the instance state</span>
            instance_state <span class="o">=</span> instance[<span class="s1">'State'</span><span class="o">][</span><span class="s1">'Name'</span><span class="o">]</span>
            
            <span class="c"># Get the instance Name tag if exists</span>
            instance_name <span class="o">=</span> <span class="s1">'No Name'</span>
            <span class="k">if</span> <span class="s1">'Tags'</span> <span class="k">in </span>instance:
                <span class="k">for </span>tag <span class="k">in </span>instance[<span class="s1">'Tags'</span><span class="o">]</span>:
                    <span class="k">if </span>tag[<span class="s1">'Key'</span><span class="o">]</span> <span class="o">==</span> <span class="s1">'Name'</span>:
                        instance_name <span class="o">=</span> tag[<span class="s1">'Value'</span><span class="o">]</span>
                        <span class="nb">break</span>
            
            <span class="c"># Print instance ID, Name, and State</span>
            print<span class="o">(</span>f<span class="s2">"Instance ID: {instance_id}, Name: {instance_name}, State: {instance_state}"</span><span class="o">)</span>

<span class="k">if </span>__name__ <span class="o">==</span> <span class="s2">"__main__"</span>:
    list_ec2_instances<span class="o">()</span>

root@c04670a43564:/work# python app.py 
Instance ID: i-076226daa5aaf7cf2, Name: zack-blog, State: stopped</code></pre></figure>

<ul>
  <li>Filter ec2 instance without tag “owner”</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create app-untagged.py</span>
root@ubt-server:~/pythonwork# vim app-untagged.py
import boto3

def get_untagged_ec2_instances<span class="o">()</span>:
    ec2_client <span class="o">=</span> boto3.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>
    response <span class="o">=</span> ec2_client.describe_instances<span class="o">()</span>
    
    untagged_instances <span class="o">=</span> <span class="o">[]</span>
    
    <span class="k">for </span>reservation <span class="k">in </span>response[<span class="s1">'Reservations'</span><span class="o">]</span>:
        <span class="k">for </span>instance <span class="k">in </span>reservation[<span class="s1">'Instances'</span><span class="o">]</span>:
            has_owner_tag <span class="o">=</span> False
            <span class="k">if</span> <span class="s1">'Tags'</span> <span class="k">in </span>instance:
                <span class="k">for </span>tag <span class="k">in </span>instance[<span class="s1">'Tags'</span><span class="o">]</span>:
                    <span class="k">if </span>tag[<span class="s1">'Key'</span><span class="o">]</span>.lower<span class="o">()</span> <span class="o">==</span> <span class="s1">'owner'</span>:
                        has_owner_tag <span class="o">=</span> True
                        <span class="nb">break</span>
            
            <span class="k">if </span>not has_owner_tag:
                instance_id <span class="o">=</span> instance[<span class="s1">'InstanceId'</span><span class="o">]</span>
                instance_state <span class="o">=</span> instance[<span class="s1">'State'</span><span class="o">][</span><span class="s1">'Name'</span><span class="o">]</span>
                untagged_instances.append<span class="o">({</span><span class="s1">'InstanceId'</span>: instance_id, <span class="s1">'State'</span>: instance_state<span class="o">})</span>
    
    <span class="k">return </span>untagged_instances

untagged_instances <span class="o">=</span> get_untagged_ec2_instances<span class="o">()</span>
print<span class="o">(</span><span class="s2">"Untagged Instances:"</span>, untagged_instances<span class="o">)</span>

<span class="c"># run script to filter untagged "owner" ec2 </span>
root@c04670a43564:/work# python app-untagged.py 
Untagged Instances: <span class="o">[{</span><span class="s1">'InstanceId'</span>: <span class="s1">'i-076226daa5aaf7cf2'</span>, <span class="s1">'State'</span>: <span class="s1">'stopped'</span><span class="o">}]</span></code></pre></figure>

<ul>
  <li>Create lambda function to list ebs volume snapshots older than 30 days and delete them</li>
</ul>

<p>To achieve this we need :</p>

<ol>
  <li>create lambda IAM role for lambda to manage EBS volume snapshot</li>
  <li>create bellow python lambda function</li>
  <li>zip and upload zip function</li>
  <li>create CloudWatch Event to Trigger run it every 30 days</li>
</ol>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create lambda function to delete snapshots older than 30 days</span>
root@ubt-server:~/pythonwork# vim app-snapshot-older-30days.py
import boto3
from datetime import datetime, timezone, timedelta

def lambda_handler<span class="o">(</span>event, context<span class="o">)</span>:
    ec2_client <span class="o">=</span> boto3.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>
    
    <span class="c"># Get the current time</span>
    now <span class="o">=</span> datetime.now<span class="o">(</span>timezone.utc<span class="o">)</span>
    
    <span class="c"># Define the time threshold</span>
    time_threshold <span class="o">=</span> now - <span class="nb">time </span>delta<span class="o">(</span><span class="nv">days</span><span class="o">=</span>30<span class="o">)</span>
    
    <span class="c"># Describe snapshots</span>
    snapshots <span class="o">=</span> ec2_client.describe_snapshots<span class="o">(</span><span class="nv">OwnerIds</span><span class="o">=[</span><span class="s1">'self'</span><span class="o">])[</span><span class="s1">'Snapshots'</span><span class="o">]</span>
    
    <span class="c"># Filter snapshots older than 30 days</span>
    old_snapshots <span class="o">=</span> <span class="o">[</span>snap <span class="k">for </span>snap <span class="k">in </span>snapshots <span class="k">if </span>snap[<span class="s1">'StartTime'</span><span class="o">]</span> &lt; time_threshold]
    
    <span class="c"># Delete old snapshots</span>
    <span class="k">for </span>snapshot <span class="k">in </span>old_snapshots:
        snapshot_id <span class="o">=</span> snapshot[<span class="s1">'SnapshotId'</span><span class="o">]</span>
        ec2_client.delete_snapshot<span class="o">(</span><span class="nv">SnapshotId</span><span class="o">=</span>snapshot_id<span class="o">)</span>
        print<span class="o">(</span>f<span class="s2">"Deleted snapshot: {snapshot_id}"</span><span class="o">)</span>
    
    <span class="k">return</span> <span class="o">{</span>
        <span class="s1">'statusCode'</span>: 200,
        <span class="s1">'body'</span>: f<span class="s2">"Deleted {len(old_snapshots)} snapshots."</span>
    <span class="o">}</span>

<span class="c"># zip Package for the Lambda Function</span>
root@ubt-server:~/pythonwork# zip <span class="k">function</span>.zip app-snapshot-older-30days.py</code></pre></figure>

<ul>
  <li>Email me when a security group allow inbound SSH (port 22) from everywhere (0.0.0.0/0)</li>
</ul>

<p>To achieve this, we need:</p>

<ol>
  <li>AWS CloudTrail enable</li>
  <li>Create CloudWatch Event Rule to capture AWS CloudTrail logs for security group changes</li>
  <li>Create bellow Lambda Function if inbound allow port 22 from everywhere are met</li>
  <li>Allow CloudWatch Events to Invoke the Lambda Function</li>
  <li>Add the Lambda Function as a Target for the CloudWatch Event Rule</li>
</ol>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~/pythonwork# vim app-sg-allow-22.py
import boto3
import json

def lambda_handler<span class="o">(</span>event, context<span class="o">)</span>:
    <span class="c"># Initialize boto3 clients</span>
    ses_client <span class="o">=</span> boto3.client<span class="o">(</span><span class="s1">'ses'</span><span class="o">)</span>
    ec2_client <span class="o">=</span> boto3.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>
    
    <span class="c"># Email details</span>
    sender <span class="o">=</span> <span class="s1">'zhbsoftboy1@gmail.com'</span>
    recipient <span class="o">=</span> <span class="s1">'zhbsoftboy1@gmail.com'</span>
    subject <span class="o">=</span> <span class="s1">'Security Group Alert: Port 22 Open to everywhere'</span>
    
    <span class="c"># Extract details from the event</span>
    detail <span class="o">=</span> event[<span class="s1">'detail'</span><span class="o">]</span>
    event_name <span class="o">=</span> detail[<span class="s1">'eventName'</span><span class="o">]</span>
    security_group_id <span class="o">=</span> None
    
    <span class="k">if </span>event_name <span class="o">==</span> <span class="s1">'AuthorizeSecurityGroupIngress'</span>:
        security_group_id <span class="o">=</span> detail[<span class="s1">'requestParameters'</span><span class="o">][</span><span class="s1">'groupId'</span><span class="o">]</span>
        ip_permissions <span class="o">=</span> detail[<span class="s1">'requestParameters'</span><span class="o">][</span><span class="s1">'ipPermissions'</span><span class="o">][</span><span class="s1">'items'</span><span class="o">]</span>
    <span class="k">elif </span>event_name <span class="o">==</span> <span class="s1">'CreateSecurityGroup'</span>:
        security_group_id <span class="o">=</span> detail[<span class="s1">'responseElements'</span><span class="o">][</span><span class="s1">'groupId'</span><span class="o">]</span>
        ip_permissions <span class="o">=</span> detail[<span class="s1">'requestParameters'</span><span class="o">][</span><span class="s1">'ipPermissionsEgress'</span><span class="o">][</span><span class="s1">'items'</span><span class="o">]</span>
    
    <span class="c"># Check if port 22 is open to 0.0.0.0/0</span>
    <span class="k">if </span>security_group_id and ip_permissions:
        <span class="k">for </span>permission <span class="k">in </span>ip_permissions:
            <span class="k">if</span> <span class="s1">'ipRanges'</span> <span class="k">in </span>permission:
                <span class="k">for </span>ip_range <span class="k">in </span>permission[<span class="s1">'ipRanges'</span><span class="o">][</span><span class="s1">'items'</span><span class="o">]</span>:
                    <span class="k">if </span>ip_range[<span class="s1">'cidrIp'</span><span class="o">]</span> <span class="o">==</span> <span class="s1">'0.0.0.0/0'</span> and permission[<span class="s1">'fromPort'</span><span class="o">]</span> <span class="o">==</span> 22 and permission[<span class="s1">'toPort'</span><span class="o">]</span> <span class="o">==</span> 22:
                        <span class="c"># Compose email body</span>
                        body_text <span class="o">=</span> <span class="o">(</span>f<span class="s2">"Security Group ID: {security_group_id} has been modified to allow port 22 from everywhere (0.0.0.0/0)."</span><span class="o">)</span>
                        body_html <span class="o">=</span> f<span class="s2">"""&lt;html&gt;
                        &lt;head&gt;&lt;/head&gt;
                        &lt;body&gt;
                          &lt;h1&gt;Security Group Alert&lt;/h1&gt;
                          &lt;p&gt;Security Group ID: &lt;b&gt;{security_group_id}&lt;/b&gt; has been modified to allow port 22 from everywhere (0.0.0.0/0).&lt;/p&gt;
                        &lt;/body&gt;
                        &lt;/html&gt;"""</span>
                        
                        <span class="c"># Send email</span>
                        response <span class="o">=</span> ses_client.send_email<span class="o">(</span>
                            <span class="nv">Source</span><span class="o">=</span>sender,
                            <span class="nv">Destination</span><span class="o">={</span><span class="s1">'ToAddresses'</span>: <span class="o">[</span>recipient]<span class="o">}</span>,
                            <span class="nv">Message</span><span class="o">={</span>
                                <span class="s1">'Subject'</span>: <span class="o">{</span><span class="s1">'Data'</span>: subject<span class="o">}</span>,
                                <span class="s1">'Body'</span>: <span class="o">{</span>
                                    <span class="s1">'Text'</span>: <span class="o">{</span><span class="s1">'Data'</span>: body_text<span class="o">}</span>,
                                    <span class="s1">'Html'</span>: <span class="o">{</span><span class="s1">'Data'</span>: body_html<span class="o">}</span>
                                <span class="o">}</span>
                            <span class="o">}</span>
                        <span class="o">)</span>
                        print<span class="o">(</span>f<span class="s2">"Email sent! Message ID: {response['MessageId']}"</span><span class="o">)</span>
    
    <span class="k">return</span> <span class="o">{</span>
        <span class="s1">'statusCode'</span>: 200,
        <span class="s1">'body'</span>: json.dumps<span class="o">(</span><span class="s1">'Lambda function executed successfully!'</span><span class="o">)</span>
    <span class="o">}</span></code></pre></figure>

<p><b> Conclusion</b></p>

<p>There are many ways to automate AWS tasks using Python Boto3 script. Together with Lambda and trigger, many resource tasks can be scheduled and managed in a scripted way.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Boto3]]></summary></entry><entry><title type="html">Customize Helm Chart for Zack’ Blog</title><link href="http://localhost:4000/jekyll/cat2/2024/05/12/Helm.html" rel="alternate" type="text/html" title="Customize Helm Chart for Zack’ Blog" /><published>2024-05-12T10:15:29+10:00</published><updated>2024-05-12T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/12/Helm</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/12/Helm.html"><![CDATA[<p><b> About Helm </b></p>

<p>Helm is the popular package manager for Kubernetes application deployment, not new to me as I had tried many charts previously with Kafka, and Redis helm charts installation, today I am going to explore how to build my own helm chart for this zack blog, also deep dive into the chart development for advanced templating, together with helm release management and version control, finally integrate my own chart with CI/CD for GitOps.  </p>

<p><b> Common Helm command</b></p>

<ul>
  <li>
    <p>helm list -A   # list releases across all namespaces</p>
  </li>
  <li>
    <p>helm pull bitnami/postgresql-ha –untar  # untar the chart after pull online chart</p>
  </li>
  <li>
    <p>helm repo add bitnami https://charts.bitnami.com/bitnami  # add a repo</p>
  </li>
  <li>
    <p>helm create zackblog-helm  # create a new chart</p>
  </li>
  <li>
    <p>helm install zackblog-helm ~/zackblog-helm -n NAMESPACE -f dev-values.yaml # define ns and override with a new value file</p>
  </li>
  <li>
    <p>helm upgrade zackblog-helm ~/zackblog-helm –set image.repository=<new-image-repository> --set image.tag=<new-image-tag> # --set to upgrade chart with override a new value</new-image-tag></new-image-repository></p>
  </li>
  <li>
    <p>helm lint ~/zackblog-helm  # lint syntax</p>
  </li>
  <li>
    <p>helm rollback zackblog-helm 2   # rollback to revision 2 of a release</p>
  </li>
  <li>
    <p>helm uninstall zackblog-helm -n Production  # uninstall a chart from a ns</p>
  </li>
</ul>

<p><b> Start with own chart</b></p>

<ul>
  <li>create a new helm chart</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@freeipa-server ~]# helm create zackblog-helm
Creating zackblog-helm

<span class="c"># modify values.yaml</span>
<span class="o">[</span>root@freeipa-server zackblog]# vim values.yaml

replicaCount: 3

image:
  repository: zackz001/gitops-jekyll
  pullPolicy: IfNotPresent
  <span class="c"># Overrides the image tag.</span>
  tag: <span class="s2">"latest"</span>

service:
  <span class="nb">type</span>: NodePort
  port: 80</code></pre></figure>

<ul>
  <li>Lint chart syntacx before install</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># lint syntax</span>
<span class="o">[</span>root@freeipa-server ~]# helm lint zackblog-helm/
<span class="o">==&gt;</span> Linting zackblog-helm/
<span class="o">[</span>INFO] Chart.yaml: icon is recommended

1 chart<span class="o">(</span>s<span class="o">)</span> linted, 0 chart<span class="o">(</span>s<span class="o">)</span> failed

<span class="c"># install own chart</span>

<span class="o">[</span>root@freeipa-server ~]# helm <span class="nb">install </span>zackblog-helm zackblog-helm
NAME: zackblog-helm
LAST DEPLOYED: Mon May 13 21:27:14 2024
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  <span class="nb">export </span><span class="nv">NODE_PORT</span><span class="o">=</span><span class="si">$(</span>kubectl get <span class="nt">--namespace</span> default <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.ports[0].nodePort}"</span> services zackblog-helm<span class="si">)</span>
  <span class="nb">export </span><span class="nv">NODE_IP</span><span class="o">=</span><span class="si">$(</span>kubectl get nodes <span class="nt">--namespace</span> default <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[0].status.addresses[0].address}"</span><span class="si">)</span>
  <span class="nb">echo </span>http://<span class="nv">$NODE_IP</span>:<span class="nv">$NODE_PORT</span>

<span class="o">[</span>root@freeipa-server ~]# helm list <span class="nt">-a</span>
NAME            NAMESPACE   REVISION    UPDATED                                     STATUS      CHART               APP VERSION
argo-cd         default     1           2024-04-29 06:14:18.117158759 +0000 UTC     deployed    argo-cd-6.7.17      v2.10.8    
zackblog-helm   default     1           2024-05-13 21:27:14.825391301 +1000 AEST    deployed    zackblog-helm-0.1.0 1.16.0 

<span class="o">[</span>root@freeipa-server ~]# kubectl get deployments.apps | <span class="nb">grep </span>zack
zackblog-helm                              3/3     3            3           113s
<span class="o">[</span>root@freeipa-server ~]# kubectl get svc | <span class="nb">grep </span>zack
zackblog-helm                              NodePort    10.43.90.209    &lt;none&gt;        80:31070/TCP                 2m7s</code></pre></figure>

<ul>
  <li>Customize vaule.yaml by change replica and image tag</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># modify vaule.yaml to scale down and change image to v138</span>
<span class="o">[</span>root@freeipa-server ~]# vim zackblog-helm/values.yaml
replicaCount: 1

image:
  repository: zackz001/gitops-jekyll
  pullPolicy: IfNotPresent
  <span class="c"># Overrides the image tag.</span>
  tag: <span class="s2">"v139"</span>

<span class="o">[</span>root@freeipa-server ~]# helm list <span class="nt">-a</span>
NAME            NAMESPACE   REVISION    UPDATED                                     STATUS      CHART               APP VERSION
argo-cd         default     1           2024-04-29 06:14:18.117158759 +0000 UTC     deployed    argo-cd-6.7.17      v2.10.8    
zackblog-helm   default     2           2024-05-13 21:31:16.523093364 +1000 AEST    deployed    zackblog-helm-0.1.0 1.16.0     
<span class="o">[</span>root@freeipa-server ~]# kubectl get deployments.apps | <span class="nb">grep </span>zack
zackblog-helm                              1/1     1            1           4m31s</code></pre></figure>

<ul>
  <li>Override values.yaml by -f and deploy same chart to different environments</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create a dev ns then deploy and override with dev-values.yaml</span>
<span class="o">[</span>root@freeipa-server ~]# vim zackblog-helm/dev-values.yaml
image:
  repository: zackz001/gitops-jekyll
  tag: v140
replicaCount: 2
service:
  <span class="nb">type</span>: NodePort
  port: 80

<span class="o">[</span>root@freeipa-server ~]# kubectl create ns dev
namespace/dev created
<span class="o">[</span>root@freeipa-server ~]# helm <span class="nb">install </span>dev-zackblog-helm zackblog-helm <span class="nt">-f</span> dev-values.yaml <span class="nt">-n</span> dev
Error: INSTALLATION FAILED: open dev-values.yaml: no such file or directory
<span class="o">[</span>root@freeipa-server ~]# helm <span class="nb">install </span>dev-zackblog-helm zackblog-helm <span class="nt">-f</span> zackblog-helm/dev-values.yaml <span class="nt">-n</span> dev
NAME: dev-zackblog-helm
LAST DEPLOYED: Mon May 13 21:36:39 2024
NAMESPACE: dev
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  <span class="nb">export </span><span class="nv">NODE_PORT</span><span class="o">=</span><span class="si">$(</span>kubectl get <span class="nt">--namespace</span> dev <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.ports[0].nodePort}"</span> services dev-zackblog-helm<span class="si">)</span>
  <span class="nb">export </span><span class="nv">NODE_IP</span><span class="o">=</span><span class="si">$(</span>kubectl get nodes <span class="nt">--namespace</span> dev <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[0].status.addresses[0].address}"</span><span class="si">)</span>
  <span class="nb">echo </span>http://<span class="nv">$NODE_IP</span>:<span class="nv">$NODE_PORT</span>

<span class="o">[</span>root@freeipa-server ~]# kubectl get deployments.apps <span class="nt">-n</span> dev
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
dev-zackblog-helm   2/2     2            2           96s
<span class="o">[</span>root@freeipa-server ~]# kubectl get svc <span class="nt">-n</span> dev
NAME                TYPE       CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>        AGE
dev-zackblog-helm   NodePort   10.43.239.229   &lt;none&gt;        80:31391/TCP   103s</code></pre></figure>

<ul>
  <li>Advanced templating to add pvc into chart</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># add templates/pvc.yaml</span>
<span class="o">[</span>root@freeipa-server ~]# vim zackblog-helm/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhron-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn
  resources:
    requests:
      storage: 1Gi

<span class="c"># add pvc in values.yaml</span>
<span class="o">[</span>root@freeipa-server ~]# vim zackblog-helm/values.yaml
pvc:
  enabled: <span class="nb">true</span>
  templateFiles:
    - pvc.yaml

<span class="c"># add persistentVolumeClaim in templates/deployment.yaml</span>
<span class="o">[</span>root@freeipa-server ~]# vim zackblog-helm/templates/deployment.yaml
...
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: longhron-pvc
...
<span class="o">[</span>root@freeipa-server ~]# helm upgrade zackblog-helm zackblog-helm
Release <span class="s2">"zackblog-helm"</span> has been upgraded. Happy Helming!</code></pre></figure>

<p><b> CICD integration to deploy chart with ArgoCD </b></p>

<p>Now commit and upload “zackblog-helm” folder into github repo, create ArgoCD application manifest to sync with from  path of own helm chart</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># argoCD application manifest</span>
project: default
<span class="nb">source</span>:
  repoURL: <span class="s1">'https://github.com/ZackZhouHB/zack-gitops-project.git'</span>
  path: argo-helm-zackblog
  targetRevision: editing
  helm:
    valueFiles:
      - values.yaml
destination:
  server: <span class="s1">'https://kubernetes.default.svc'</span>
  namespace: helm
syncPolicy:
  automated: <span class="o">{}</span>
  syncOptions:
    - <span class="nv">CreateNamespace</span><span class="o">=</span><span class="nb">true</span></code></pre></figure>

<p><img src="/assets/helm1.png" alt="image tooltip here" /></p>

<p><b> Conclusion</b></p>

<p>Finally, I had a chance to go over helm, it makes package management easier and more convenient, through charts, k8s deployment can be more flexible with values and templates that can be deployed and reusable into different environments, it provides versioning and rollbacks, also allow customization of the template. however using on-line chart can also be risky in a production environment with quality, dependency and security risks.</p>

<p>Overall I think helm chart is a very good way to start deployment into k8s.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Helm]]></summary></entry><entry><title type="html">PostgreSQL: Prod-Grade with k8s Operator</title><link href="http://localhost:4000/jekyll/cat2/2024/05/11/PS4.html" rel="alternate" type="text/html" title="PostgreSQL: Prod-Grade with k8s Operator" /><published>2024-05-11T10:15:29+10:00</published><updated>2024-05-11T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/11/PS4</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/11/PS4.html"><![CDATA[<p><b> Production grade PostgreSQL in K8S</b></p>

<p>Despite all the challenges, in the last 2 years, clever people still managed ways to deploy production-grade database within a Kubernetes cluster by using Kubernetes as a platform to develop custom resource definition (CRDs) like helm charts like bitnami/postgresql-ha, and PostgreSQL Operator like CrunchyData/postgres-operator or zalando/postgres-operator.</p>

<p>Last post I was able to deploy a single PostgreSQL in local k8s, but I had to manually create Kubernetes namespaces, define database creds, configuration and environment variables into k8s secret and configmap, also to define the statefulset yaml with volume claim template.</p>

<p>Still I was not able to configure HA and failover as I found it is so limited and a headache within K8S if only relying on statefulset. Luckily there are engineers out there to develop helm and opeartor to get the job done.</p>

<p><b> CrunchyData Postgres-Operator</b></p>

<p>In this session, I will follow bellow steps to</p>

<ul>
  <li>Deploy PostgreSQL Operator</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Clone the CrunchyData Postgres Operator</span>
<span class="o">[</span>root@freeipa-server ~]# git clone https://github.com/CrunchyData/postgres-operator-examples.git

<span class="c"># create namespace and deploy GPO Postgres Operatorusing kustomize</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl apply <span class="nt">-k</span> kustomize/install/namespace
namespace/postgres-operator created
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl apply <span class="nt">--server-side</span> <span class="nt">-k</span> kustomize/install/default
customresourcedefinition.apiextensions.k8s.io/pgadmins.postgres-operator.crunchydata.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/pgupgrades.postgres-operator.crunchydata.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/postgresclusters.postgres-operator.crunchydata.com serverside-applied
serviceaccount/pgo serverside-applied
clusterrole.rbac.authorization.k8s.io/postgres-operator serverside-applied
clusterrolebinding.rbac.authorization.k8s.io/postgres-operator serverside-applied
deployment.apps/pgo serverside-applied

<span class="c"># validate deploy status</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl get all <span class="nt">-n</span> postgres-operator
NAME                      READY   STATUS    RESTARTS   AGE
pod/pgo-77d6b49b8-wrdjp   1/1     Running   0          2m47s

NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pgo   1/1     1            1           2m47s

NAME                            DESIRED   CURRENT   READY   AGE
replicaset.apps/pgo-77d6b49b8   1         1         1       2m47s</code></pre></figure>

<ul>
  <li>Deploy HA PostgreSQL Cluster</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Create a Postgres Cluster named "hippo" in "postgres-operator" ns</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl apply <span class="nt">-k</span> kustomize/postgres
postgrescluster.postgres-operator.crunchydata.com/hippo created

<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl get all <span class="nt">-n</span> postgres-operator
NAME                          READY   STATUS    RESTARTS   AGE
pod/hippo-backup-dvks-m4z5m   1/1     Running   0          56s
pod/hippo-instance1-582s-0    4/4     Running   0          2m14s
pod/hippo-repo-host-0         2/2     Running   0          2m14s
pod/pgo-77d6b49b8-wrdjp       1/1     Running   0          6m38s

NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>    AGE
service/hippo-ha          ClusterIP   10.43.249.159   &lt;none&gt;        5432/TCP   2m14s
service/hippo-ha-config   ClusterIP   None            &lt;none&gt;        &lt;none&gt;     2m14s
service/hippo-pods        ClusterIP   None            &lt;none&gt;        &lt;none&gt;     2m14s
service/hippo-primary     ClusterIP   None            &lt;none&gt;        5432/TCP   2m14s
service/hippo-replicas    ClusterIP   10.43.17.57     &lt;none&gt;        5432/TCP   2m14s

NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pgo   1/1     1            1           6m38s

NAME                            DESIRED   CURRENT   READY   AGE
replicaset.apps/pgo-77d6b49b8   1         1         1       6m38s

NAME                                    READY   AGE
statefulset.apps/hippo-instance1-582s   1/1     2m14s
statefulset.apps/hippo-repo-host        1/1     2m14s

NAME                          COMPLETIONS   DURATION   AGE
job.batch/hippo-backup-dvks   0/1           56s        56s

<span class="c"># retrieve database password from Kubernetes secret</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl get secret hippo-pguser-hippo <span class="nt">-n</span> postgres-operator <span class="nt">-o</span><span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.data.password}'</span> | <span class="nb">base64</span> <span class="nt">--decode</span>
jZiBWXMGRiEOA6wAEj<span class="p">;</span>lRhsM</code></pre></figure>

<p>Connect an application to PostgreSQL cluster</p>

<p>Here we use Keycloak, a popular open-source identity management tool that is backed by a PostgreSQL database. Using the hippo cluster we created, we can deploy the following manifest file</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create deployment keycloak to connect PostgreSQL database</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# vim kustomize/keycloak/keycloak.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: keycloak
  namespace: postgres-operator
  labels:
    app.kubernetes.io/name: keycloak
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: keycloak
  template:
    metadata:
      labels:
        app.kubernetes.io/name: keycloak
    spec:
      containers:
      - image: quay.io/keycloak/keycloak:latest
        args: <span class="o">[</span><span class="s2">"start-dev"</span><span class="o">]</span>
        name: keycloak
        <span class="nb">env</span>:
        - name: DB_VENDOR
          value: <span class="s2">"postgres"</span>
        - name: DB_ADDR
          valueFrom: <span class="o">{</span> secretKeyRef: <span class="o">{</span> name: hippo-pguser-hippo, key: host <span class="o">}</span> <span class="o">}</span>
        - name: DB_PORT
          valueFrom: <span class="o">{</span> secretKeyRef: <span class="o">{</span> name: hippo-pguser-hippo, key: port <span class="o">}</span> <span class="o">}</span>
        - name: DB_DATABASE
          valueFrom: <span class="o">{</span> secretKeyRef: <span class="o">{</span> name: hippo-pguser-hippo, key: dbname <span class="o">}</span> <span class="o">}</span>
        - name: DB_USER
          valueFrom: <span class="o">{</span> secretKeyRef: <span class="o">{</span> name: hippo-pguser-hippo, key: user <span class="o">}</span> <span class="o">}</span>
        - name: DB_PASSWORD
          valueFrom: <span class="o">{</span> secretKeyRef: <span class="o">{</span> name: hippo-pguser-hippo, key: password <span class="o">}</span> <span class="o">}</span>
        - name: KEYCLOAK_ADMIN
          value: <span class="s2">"admin"</span>
        - name: KEYCLOAK_ADMIN_PASSWORD
          value: <span class="s2">"admin"</span>
        - name: KC_PROXY
          value: <span class="s2">"edge"</span>
        ports:
        - name: http
          containerPort: 8080
        - name: https
          containerPort: 8443
        readinessProbe:
          httpGet:
            path: /realms/master
            port: 8080
      restartPolicy: Always

<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl apply <span class="nt">-f</span> kustomize/keycloak/keycloak.yaml
deployment.apps/keycloak created

<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl get deployment <span class="nt">-n</span> postgres-operator 
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
keycloak   1/1     1            1           4m27s
pgo        1/1     1            1           176m</code></pre></figure>

<ul>
  <li>Scale Up / Down</li>
</ul>

<p>Edit manifest to add 2 more replicas</p>

<p><img src="/assets/ps3-1.png" alt="image tooltip here" /></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@freeipa-server kustomize]# kubectl apply <span class="nt">-k</span> postgres <span class="nt">-n</span> postgres-operator
postgrescluster.postgres-operator.crunchydata.com/hippo configured
<span class="c"># watch change</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# watch kubectl get pod <span class="nt">-L</span> postgres-operator.crunchydata.com/role <span class="nt">-l</span> postgres-operator.crunchydata.com/instance <span class="nt">-n</span> postgres-operator</code></pre></figure>

<p><img src="/assets/ps3-2.png" alt="image tooltip here" /></p>

<p>Failover testing:</p>

<p>Now I am going to delete the primary instance, one of the standby pod will take over and become primary automatically</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># delete the primary pod hippo-instance1-nhbc-0, then previous replica pod hippo-instance1-q8kk-0 promoted as master</span>
<span class="c"># pod hippo-instance1-nhbc-0 will up again as a replica</span>
<span class="o">[</span>root@freeipa-server kustomize]# kubectl delete po hippo-instance1-nhbc-0 <span class="nt">-n</span> postgres-operator 
pod <span class="s2">"hippo-instance1-nhbc-0"</span> deleted</code></pre></figure>

<p><img src="/assets/ps3-3.png" alt="image tooltip here" /></p>

<ul>
  <li>Perform Minor version rolling upgrade</li>
</ul>

<p>Here I changed the database version to 16.1, the cluster will start a rolling update by</p>

<ol>
  <li>
    <p>Applying new version to one of the standby pod first</p>
  </li>
  <li>
    <p>Then update another replica pod</p>
  </li>
  <li>
    <p>Promote the first upgraded replica as master</p>
  </li>
  <li>
    <p>Lastly the previous master pod will be updated and become a replica</p>
  </li>
</ol>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># validate DB version before miner upgrade</span>
<span class="o">[</span>root@freeipa-server kustomize]# kubectl <span class="nb">exec</span> <span class="nt">-it</span> hippo-instance1-q8kk-0 <span class="nt">-n</span> postgres-operator <span class="nt">--</span> psql <span class="nt">--version</span>
Defaulted container <span class="s2">"database"</span> out of: database, replication-cert-copy, pgbackrest, pgbackrest-config, postgres-startup <span class="o">(</span>init<span class="o">)</span>, nss-wrapper-init <span class="o">(</span>init<span class="o">)</span>
psql <span class="o">(</span>PostgreSQL<span class="o">)</span> 16.2</code></pre></figure>

<p><img src="/assets/ps3-5.png" alt="image tooltip here" /></p>

<p><img src="/assets/ps3-4.png" alt="image tooltip here" /></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># validate DB version after miner version change</span>
<span class="o">[</span>root@freeipa-server kustomize]# kubectl <span class="nb">exec</span> <span class="nt">-it</span> hippo-instance1-q8kk-0 <span class="nt">-n</span> postgres-operator <span class="nt">--</span> psql <span class="nt">--version</span>
Defaulted container <span class="s2">"database"</span> out of: database, replication-cert-copy, pgbackrest, pgbackrest-config, postgres-startup <span class="o">(</span>init<span class="o">)</span>, nss-wrapper-init <span class="o">(</span>init<span class="o">)</span>
psql <span class="o">(</span>PostgreSQL<span class="o">)</span> 16.1</code></pre></figure>

<ul>
  <li>Backup</li>
</ul>

<p>Add backup Cron job into manifest to add weekly full backup and daily incremental</p>

<p><img src="/assets/ps3-6.png" alt="image tooltip here" /></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@freeipa-server ~]# kubectl get cronjobs <span class="nt">-n</span> postgres-operator 
NAME               SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hippo-repo1-full   0 1 <span class="k">*</span> <span class="k">*</span> 0     False     0        &lt;none&gt;          5m21s
hippo-repo1-incr   0 1 <span class="k">*</span> <span class="k">*</span> 1-6   False     0        &lt;none&gt;          5m21s</code></pre></figure>

<ul>
  <li>Deploy Monitoring (Prom + Grafaba)</li>
</ul>

<p>Finally, let’s set up the monitoring stack for PostgreSQL by using Pormthues and Grafana.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># deploy monitoring stack</span>
<span class="o">[</span>root@freeipa-server kustomize]# kubectl apply <span class="nt">-k</span> monitoring
serviceaccount/alertmanager created
serviceaccount/grafana created
serviceaccount/prometheus created
clusterrole.rbac.authorization.k8s.io/prometheus created
clusterrolebinding.rbac.authorization.k8s.io/prometheus created
configmap/alert-rules-config created
configmap/alertmanager-config created
configmap/crunchy-prometheus created
configmap/grafana-dashboards created
configmap/grafana-datasources created
secret/grafana-admin created
service/crunchy-alertmanager created
service/crunchy-grafana created
service/crunchy-prometheus created
persistentvolumeclaim/alertmanagerdata created
persistentvolumeclaim/grafanadata created
persistentvolumeclaim/prometheusdata created
deployment.apps/crunchy-alertmanager created
deployment.apps/crunchy-grafana created
deployment.apps/crunchy-prometheus created
<span class="c"># Edit Grafana service to NodePort</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl edit svc crunchy-grafana <span class="nt">-n</span> postgres-operator
service/crunchy-grafana edited
<span class="c"># exec into master database container, using pgbench to generate tables</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl <span class="nb">exec</span> <span class="nt">-it</span> hippo-instance1-nhbc-0 <span class="nt">-c</span> database <span class="nt">-n</span> postgres-operator <span class="nt">--</span> bash

bash-4.4<span class="nv">$ </span>pgbench <span class="nt">-i</span> <span class="nt">-s</span> 100 <span class="nt">-U</span> postgres <span class="nt">-d</span> postgres
dropping old tables...
NOTICE:  table <span class="s2">"pgbench_accounts"</span> does not exist, skipping
NOTICE:  table <span class="s2">"pgbench_branches"</span> does not exist, skipping
NOTICE:  table <span class="s2">"pgbench_history"</span> does not exist, skipping
NOTICE:  table <span class="s2">"pgbench_tellers"</span> does not exist, skipping
creating tables...
generating data <span class="o">(</span>client-side<span class="o">)</span>...
10000000 of 10000000 tuples <span class="o">(</span>100%<span class="o">)</span> <span class="k">done</span> <span class="o">(</span>elapsed 45.61 s, remaining 0.00 s<span class="o">)</span></code></pre></figure>

<p><img src="/assets/ps3-7.png" alt="image tooltip here" /></p>

<p>Some Grafana predefined PostgreSQL dashboard, unfortunately I donot have much data in it to show more monitoring status.</p>

<p><img src="/assets/ps3-8.png" alt="image tooltip here" /></p>

<p><img src="/assets/ps3-9.png" alt="image tooltip here" /></p>

<p><b> Conclusion</b></p>

<p>This is the final session of this PostgreSQL series, together I have explored PostgreSQL from very basic docker deployment with replica, to production-grade deployment in Kubernetes using operator, practise from backup, monitoring, rolling update, to HA, failover and scale up. HAHA!</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Production grade PostgreSQL in K8S]]></summary></entry><entry><title type="html">PostgreSQL: Deploy into K8S</title><link href="http://localhost:4000/jekyll/cat2/2024/05/10/PS3.html" rel="alternate" type="text/html" title="PostgreSQL: Deploy into K8S" /><published>2024-05-10T10:15:29+10:00</published><updated>2024-05-10T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/10/PS3</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/10/PS3.html"><![CDATA[<p><b> Single PostgreSQL deployment in K8S</b></p>

<p>PostgreSQL by default doesnot build for kubernetes, and a database with statefulset workload in k8s can be brutal to manage. In my lab k8s cluster, here we create namespace, secret, configuremap, PVC and statefulset to run a single PostgreSQL</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create namespace "postgresql"</span>
<span class="o">[</span>root@freeipa-server ~]# kubectl create ns postgresql
namespace/postgresql created

<span class="c"># create secret to store database creds</span>
<span class="o">[</span>root@freeipa-server ~]# kubectl <span class="nt">-n</span> postgresql create secret generic postgresql <span class="nt">--from-literal</span> <span class="nv">POSTGRES_USER</span><span class="o">=</span><span class="s2">"postgresadmin"</span> <span class="nt">--from-literal</span> <span class="nv">POSTGRES_PASSWORD</span><span class="o">=</span><span class="s1">'admin123'</span> <span class="nt">--from-literal</span> <span class="nv">POSTGRES_DB</span><span class="o">=</span><span class="s2">"postgresdb"</span> <span class="nt">--from-literal</span> <span class="nv">REPLICATION_USER</span><span class="o">=</span><span class="s2">"replicationuser"</span> <span class="nt">--from-literal</span> <span class="nv">REPLICATION_PASSWORD</span><span class="o">=</span><span class="s1">'replicationPassword'</span>
secret/postgresql created

<span class="c"># create configmap, pvc, statefulset with init container to run postgresql</span>
<span class="o">[</span>root@freeipa-server ~]# vim stateful.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres
data: 
  pg_hba.conf: |+
    <span class="c"># TYPE  DATABASE        USER            ADDRESS                 METHOD</span>
    host     replication     replicationuser         0.0.0.0/0        md5
    <span class="c"># "local" is for Unix domain socket connections only</span>
    <span class="nb">local   </span>all             all                                     trust
    <span class="c"># IPv4 local connections:</span>
    host    all             all             127.0.0.1/32            trust
    <span class="c"># IPv6 local connections:</span>
    host    all             all             ::1/128                 trust
    <span class="c"># Allow replication connections from localhost, by a user with the</span>
    <span class="c"># replication privilege.</span>
    <span class="nb">local   </span>replication     all                                     trust
    host    replication     all             127.0.0.1/32            trust
    host    replication     all             ::1/128                 trust

    host all all all scram-sha-256
  postgresql.conf: |+
    data_directory <span class="o">=</span> <span class="s1">'/data/pgdata'</span>
    hba_file <span class="o">=</span> <span class="s1">'/config/pg_hba.conf'</span>
    ident_file <span class="o">=</span> <span class="s1">'/config/pg_ident.conf'</span>

    port <span class="o">=</span> 5432
    listen_addresses <span class="o">=</span> <span class="s1">'*'</span>
    max_connections <span class="o">=</span> 100
    shared_buffers <span class="o">=</span> 128MB
    dynamic_shared_memory_type <span class="o">=</span> posix
    max_wal_size <span class="o">=</span> 1GB
    min_wal_size <span class="o">=</span> 80MB
    log_timezone <span class="o">=</span> <span class="s1">'Etc/UTC'</span>
    datestyle <span class="o">=</span> <span class="s1">'iso, mdy'</span>
    timezone <span class="o">=</span> <span class="s1">'Etc/UTC'</span>

    <span class="c">#locale settings</span>
    lc_messages <span class="o">=</span> <span class="s1">'en_US.utf8'</span>			<span class="c"># locale for system error message</span>
    lc_monetary <span class="o">=</span> <span class="s1">'en_US.utf8'</span>			<span class="c"># locale for monetary formatting</span>
    lc_numeric <span class="o">=</span> <span class="s1">'en_US.utf8'</span>			<span class="c"># locale for number formatting</span>
    lc_time <span class="o">=</span> <span class="s1">'en_US.utf8'</span>				<span class="c"># locale for time formatting</span>

    default_text_search_config <span class="o">=</span> <span class="s1">'pg_catalog.english'</span>

    <span class="c">#replication</span>
    wal_level <span class="o">=</span> replica
    archive_mode <span class="o">=</span> on
    archive_command <span class="o">=</span> <span class="s1">'test ! -f /data/archive/%f &amp;&amp; cp %p /data/archive/%f'</span>
    max_wal_senders <span class="o">=</span> 3
<span class="nt">---</span>
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  selector:
    matchLabels:
      app: postgres
  serviceName: <span class="s2">"postgres"</span>
  replicas: 1
  template:
    metadata:
      labels:
        app: postgres
    spec:
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: init
        image: postgres:15.0
        <span class="nb">command</span>: <span class="o">[</span> <span class="s2">"bash"</span>, <span class="s2">"-c"</span> <span class="o">]</span>
        args:
        - |
          <span class="c">#create archive directory</span>
          <span class="nb">mkdir</span> <span class="nt">-p</span> /data/archive <span class="o">&amp;&amp;</span> <span class="nb">chown</span> <span class="nt">-R</span> 999:999 /data/archive
        volumeMounts:
        - name: data
          mountPath: /data
          readOnly: <span class="nb">false
      </span>containers:
      - name: postgres
        image: postgres:15.0
        args: <span class="o">[</span><span class="s2">"-c"</span>, <span class="s2">"config_file=/config/postgresql.conf"</span><span class="o">]</span>
        ports:
        - containerPort: 5432
          name: database
        <span class="nb">env</span>:
        - name: PGDATA
          value: <span class="s2">"/data/pgdata"</span>
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: postgresql
              key: POSTGRES_USER
              optional: <span class="nb">false</span>
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgresql
              key: POSTGRES_PASSWORD
              optional: <span class="nb">false</span>
        - name: POSTGRES_DB
          valueFrom:
            secretKeyRef:
              name: postgresql
              key: POSTGRES_DB
              optional: <span class="nb">false
        </span>volumeMounts:
        - name: config
          mountPath: /config
          readOnly: <span class="nb">false</span>
        - name: data
          mountPath: /data
          readOnly: <span class="nb">false
      </span>volumes:
      - name: config
        configMap:
          name: postgres
          defaultMode: 0755
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: <span class="o">[</span> <span class="s2">"ReadWriteOnce"</span> <span class="o">]</span>
      storageClassName: <span class="s2">"standard"</span>
      resources:
        requests:
          storage: 100Mi
<span class="nt">---</span>
apiVersion: v1
kind: Service
metadata:
  name: postgres
  labels:
    app: postgres
spec:
  ports:
  - port: 5432
    targetPort: 5432
    name: postgres
  clusterIP: None
  selector:
    app: postgres

<span class="o">[</span>root@freeipa-server ~]# kubectl create <span class="nt">-f</span> stateful.yaml <span class="nt">-n</span> postgresql 
configmap/postgres created
statefulset.apps/postgres created
service/postgres created

<span class="c"># validate for pvc, pods</span>
<span class="o">[</span>root@freeipa-server ~]# kubectl get pvc <span class="nt">-n</span> postgresql 
NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-postgres-0   Bound    pvc-dd89fc0a-915f-40eb-b61f-917234074a61   100Mi      RWO            longhorn       19m

<span class="o">[</span>root@freeipa-server ~]# kubectl get all <span class="nt">-n</span> postgresql 
NAME             READY   STATUS    RESTARTS   AGE
pod/postgres-0   1/1     Running   0          6m29s

NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>    AGE
service/postgres   ClusterIP   None         &lt;none&gt;        5432/TCP   6m29s

NAME                        READY   AGE
statefulset.apps/postgres   1/1     6m29s

<span class="c"># check container logs for database connection status</span>
<span class="o">[</span>root@freeipa-server ~]# kubectl logs <span class="nt">-n</span> postgresql postgres-0 
Defaulted container <span class="s2">"postgres"</span> out of: postgres, init <span class="o">(</span>init<span class="o">)</span>
The files belonging to this database system will be owned by user <span class="s2">"postgres"</span><span class="nb">.</span>
This user must also own the server process.

The database cluster will be initialized with locale <span class="s2">"en_US.utf8"</span><span class="nb">.</span>
The default database encoding has accordingly been <span class="nb">set </span>to <span class="s2">"UTF8"</span><span class="nb">.</span>
The default text search configuration will be <span class="nb">set </span>to <span class="s2">"english"</span><span class="nb">.</span>

Data page checksums are disabled.

fixing permissions on existing directory /data/pgdata ... ok
creating subdirectories ... ok
selecting dynamic shared memory implementation ... posix
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
selecting default <span class="nb">time </span>zone ... Etc/UTC
creating configuration files ... ok
running bootstrap script ... ok
performing post-bootstrap initialization ... ok
initdb: warning: enabling <span class="s2">"trust"</span> authentication <span class="k">for </span><span class="nb">local </span>connections
initdb: hint: You can change this by editing pg_hba.conf or using the option <span class="nt">-A</span>, or <span class="nt">--auth-local</span> and <span class="nt">--auth-host</span>, the next <span class="nb">time </span>you run initdb.
syncing data to disk ... ok


Success. You can now start the database server using:

    pg_ctl <span class="nt">-D</span> /data/pgdata <span class="nt">-l</span> logfile start

waiting <span class="k">for </span>server to start....2024-05-12 00:52:56.718 UTC <span class="o">[</span>49] LOG:  starting PostgreSQL 15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">)</span> on x86_64-pc-linux-gnu, compiled by gcc <span class="o">(</span>Debian 10.2.1-6<span class="o">)</span> 10.2.1 20210110, 64-bit
2024-05-12 00:52:56.719 UTC <span class="o">[</span>49] LOG:  listening on Unix socket <span class="s2">"/var/run/postgresql/.s.PGSQL.5432"</span>
2024-05-12 00:52:56.730 UTC <span class="o">[</span>49] LOG:  could not open usermap file <span class="s2">"/config/pg_ident.conf"</span>: No such file or directory
2024-05-12 00:52:56.733 UTC <span class="o">[</span>52] LOG:  database system was shut down at 2024-05-12 00:52:55 UTC
2024-05-12 00:52:56.744 UTC <span class="o">[</span>49] LOG:  database system is ready to accept connections
 <span class="k">done
</span>server started
CREATE DATABASE


/usr/local/bin/docker-entrypoint.sh: ignoring /docker-entrypoint-initdb.d/<span class="k">*</span>

2024-05-12 00:52:56.957 UTC <span class="o">[</span>49] LOG:  received fast shutdown request
waiting <span class="k">for </span>server to shut down....2024-05-12 00:52:56.961 UTC <span class="o">[</span>49] LOG:  aborting any active transactions
2024-05-12 00:52:56.962 UTC <span class="o">[</span>49] LOG:  background worker <span class="s2">"logical replication launcher"</span> <span class="o">(</span>PID 56<span class="o">)</span> exited with <span class="nb">exit </span>code 1
2024-05-12 00:52:56.963 UTC <span class="o">[</span>50] LOG:  shutting down
2024-05-12 00:52:57.042 UTC <span class="o">[</span>50] LOG:  checkpoint starting: shutdown immediate
..2024-05-12 00:52:59.314 UTC <span class="o">[</span>50] LOG:  checkpoint <span class="nb">complete</span>: wrote 918 buffers <span class="o">(</span>5.6%<span class="o">)</span><span class="p">;</span> 0 WAL file<span class="o">(</span>s<span class="o">)</span> added, 0 removed, 1 recycled<span class="p">;</span> <span class="nv">write</span><span class="o">=</span>0.434 s, <span class="nb">sync</span><span class="o">=</span>0.014 s, <span class="nv">total</span><span class="o">=</span>2.279 s<span class="p">;</span> <span class="nb">sync </span><span class="nv">files</span><span class="o">=</span>250, <span class="nv">longest</span><span class="o">=</span>0.007 s, <span class="nv">average</span><span class="o">=</span>0.001 s<span class="p">;</span> <span class="nv">distance</span><span class="o">=</span>11271 kB, <span class="nv">estimate</span><span class="o">=</span>11271 kB
2024-05-12 00:52:59.318 UTC <span class="o">[</span>49] LOG:  database system is shut down
 <span class="k">done
</span>server stopped

PostgreSQL init process <span class="nb">complete</span><span class="p">;</span> ready <span class="k">for </span>start up.

2024-05-12 00:52:59.385 UTC <span class="o">[</span>1] LOG:  starting PostgreSQL 15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">)</span> on x86_64-pc-linux-gnu, compiled by gcc <span class="o">(</span>Debian 10.2.1-6<span class="o">)</span> 10.2.1 20210110, 64-bit
2024-05-12 00:52:59.385 UTC <span class="o">[</span>1] LOG:  listening on IPv4 address <span class="s2">"0.0.0.0"</span>, port 5432
2024-05-12 00:52:59.385 UTC <span class="o">[</span>1] LOG:  listening on IPv6 address <span class="s2">"::"</span>, port 5432
2024-05-12 00:52:59.389 UTC <span class="o">[</span>1] LOG:  listening on Unix socket <span class="s2">"/var/run/postgresql/.s.PGSQL.5432"</span>
2024-05-12 00:52:59.398 UTC <span class="o">[</span>1] LOG:  could not open usermap file <span class="s2">"/config/pg_ident.conf"</span>: No such file or directory
2024-05-12 00:52:59.404 UTC <span class="o">[</span>67] LOG:  database system was shut down at 2024-05-12 00:52:59 UTC
2024-05-12 00:52:59.415 UTC <span class="o">[</span>1] LOG:  database system is ready to accept connections</code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we are able to deploy a PostgreSQL in local k8s cluster, with defined environment variables in kubernetes secret and configmap, together with init container to create data archive volume in presistent storage class, the next blog I will discover how to run PostgreSQL HA with presistent volume on kubernetes with both Helm and operater, then validate scale up and down, backup using cronjob and etc.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Single PostgreSQL deployment in K8S]]></summary></entry><entry><title type="html">PostgreSQL: Replication &amp;amp; Failover</title><link href="http://localhost:4000/jekyll/cat2/2024/05/08/PS2.html" rel="alternate" type="text/html" title="PostgreSQL: Replication &amp;amp; Failover" /><published>2024-05-08T10:15:29+10:00</published><updated>2024-05-08T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/08/PS2</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/08/PS2.html"><![CDATA[<p><b> The Replication</b></p>

<p>Last post I started hands-on session with PostgreSQL installation on both docker and docker-compose, explored important PostgreSQL configuration and was able to mount persistent volumes and config files to customize PostgreSQL.</p>

<p>In this post I will setup a second PostgreSQL instance to setup a primaty and standby replication for PostgreSQL HA, by using some pg tools, last we will test failover by shut down primary and promot standby instance.</p>

<p><img src="/assets/ps2-1.png" alt="image tooltip here" /></p>

<p>To achieve this, steps can be followed by:  </p>

<ul>
  <li>Setup docker network and Create Replication User in Primary instance</li>
</ul>

<p>To establish PostgreSQL replication, it is necessary to set unique data volumes for data between instances and unique config files for each instance.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create both primary and standby folders</span>
root@ubt-server:~# <span class="nb">mkdir </span>postgres-1
root@ubt-server:~# <span class="nb">mkdir </span>postgres-2

<span class="c"># move previous post config file to postgres-1 and postgres-2 </span>
root@ubt-server:~# <span class="nb">cp</span> <span class="nt">-r</span> config/<span class="k">*</span> postgres-1/config/
root@ubt-server:~# <span class="nb">mv </span>config/<span class="k">*</span> postgres-2/config/

<span class="c"># create docker network so PostgreSQL containers on the same network</span>
root@ubt-server:~/postgres-1/config# docker network create postgres
9891c6d9cd3bdbeea2fdfc2b287c868a0f67a3cec7f2939e1299cfb0ae293021

<span class="c"># run primary </span>
root@ubt-server:~/postgres-1# docker run <span class="nt">-it</span> <span class="nt">--rm</span> <span class="nt">--name</span> postgres-1 <span class="se">\</span>
<span class="nt">--net</span> postgres <span class="se">\</span>
<span class="nt">-e</span> <span class="nv">POSTGRES_USER</span><span class="o">=</span>postgresadmin <span class="se">\</span>
<span class="nt">-e</span> <span class="nv">POSTGRES_PASSWORD</span><span class="o">=</span>admin123 <span class="se">\</span>
<span class="nt">-e</span> <span class="nv">POSTGRES_DB</span><span class="o">=</span>postgresdb <span class="se">\</span>
<span class="nt">-e</span> <span class="nv">PGDATA</span><span class="o">=</span><span class="s2">"/data"</span> <span class="se">\</span>
<span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/postgres-1/pgdata:/data <span class="se">\</span>
<span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/postgres-1/config:/config <span class="se">\</span>
<span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/postgres-1/archive:/mnt/server/archive <span class="se">\</span>
<span class="nt">-p</span> 5000:5432 postgres:15.0 <span class="se">\</span>
<span class="nt">-c</span> <span class="s1">'config_file=/config/postgresql.conf'</span>

2024-05-11 13:15:07.762 UTC <span class="o">[</span>1] LOG:  starting PostgreSQL 15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">)</span> on x86_64-pc-linux-gnu, compiled by gcc <span class="o">(</span>Debian 10.2.1-6<span class="o">)</span> 10.2.1 20210110, 64-bit
2024-05-11 13:15:07.763 UTC <span class="o">[</span>1] LOG:  listening on IPv4 address <span class="s2">"0.0.0.0"</span>, port 5432
2024-05-11 13:15:07.763 UTC <span class="o">[</span>1] LOG:  listening on IPv6 address <span class="s2">"::"</span>, port 5432
2024-05-11 13:15:07.764 UTC <span class="o">[</span>1] LOG:  listening on Unix socket <span class="s2">"/var/run/postgresql/.s.PGSQL.5432"</span>
2024-05-11 13:15:07.766 UTC <span class="o">[</span>63] LOG:  database system was shut down at 2024-05-11 13:15:07 UTC
2024-05-11 13:15:07.768 UTC <span class="o">[</span>1] LOG:  database system is ready to accept connections

<span class="c"># create Replication User, chown postgres to access archive folder</span>
root@ubt-server:~# docker <span class="nb">exec</span> <span class="nt">-it</span> postgres-1 bash
root@2bf6be3e4fa8:/# createuser <span class="nt">-U</span> postgresadmin <span class="nt">-P</span> <span class="nt">-c</span> 5 <span class="nt">--replication</span> replicationUser
Enter password <span class="k">for </span>new role: 
Enter it again: 
root@2bf6be3e4fa8:/# <span class="nb">chown </span>postgres:postgres /mnt/server/archive

<span class="c"># add replication into configration file</span>
root@ubt-server:~/postgres-1/config# vim pg_hba.conf
<span class="c"># TYPE  DATABASE        USER            ADDRESS                 METHOD</span>
<span class="c"># add replication user</span>
host     replication     replicationUser         0.0.0.0/0        md5</code></pre></figure>

<ul>
  <li>Enable Write-Ahead Log, archive and Replication</li>
</ul>

<p>Write-Ahead Log (WAL) is a PostgreSQL data integrity mechanism of writing transaction logs to file and does not accept the transaction until it has been written to the transaction log and flushed to disk. This ensures that if there is a crash in the system, that the database can be recovered from the transaction log.</p>

<p>So we need to add bellow lines into postgresql.conf to enable Write-Ahead Log and replica and archive</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~/postgres-1/config# vim postgresql.conf

<span class="c">#replication</span>
wal_level <span class="o">=</span> replica
archive_mode <span class="o">=</span> on
archive_command <span class="o">=</span> <span class="s1">'test ! -f /mnt/server/archive/%f &amp;&amp; cp %p /mnt/server/archive/%f'</span>
max_wal_senders <span class="o">=</span> 3</code></pre></figure>

<ul>
  <li>set up standby instance and validate replication
here we need to use tool “pgbase_backup” to create standby instance by taking a primary instance base backup, type “replicationUser” passwd, then postgres-1 database will be back up into postgres-2 pgdata folder</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~# docker run <span class="nt">-it</span> <span class="nt">--rm</span> <span class="nt">--net</span> postgres <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/postgres-2/pgdata:/data <span class="nt">--entrypoint</span> /bin/bash postgres:15.0

root@56e38636a87b:/# pg_basebackup <span class="nt">-h</span> postgres-1 <span class="nt">-p</span> 5432 <span class="nt">-U</span> replicationUser <span class="nt">-D</span> /data/ <span class="nt">-Fp</span> <span class="nt">-Xs</span> <span class="nt">-R</span>
Password: </code></pre></figure>

<p>Now, we start the standby instance. See the log below. Postgres-2 is entering standby mode, ready to accept read-only connections, and starting streaming WAL from the primary.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~# docker run <span class="nt">-it</span> <span class="nt">--rm</span> <span class="nt">--name</span> postgres-2 <span class="nt">--net</span> postgres <span class="nt">-e</span> <span class="nv">POSTGRES_USER</span><span class="o">=</span>postgresadmin <span class="nt">-e</span> <span class="nv">POSTGRES_PASSWORD</span><span class="o">=</span>admin123 <span class="nt">-e</span> <span class="nv">POSTGRES_DB</span><span class="o">=</span>postgresdb <span class="nt">-e</span> <span class="nv">PGDATA</span><span class="o">=</span><span class="s2">"/data"</span> <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/postgres-2/pgdata:/data <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/postgres-2/config:/config <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/postgres-2/archive:/mnt/server/archive <span class="nt">-p</span> 5001:5432 postgres:15.0 <span class="nt">-c</span> <span class="s1">'config_file=/config/postgresql.conf'</span>

PostgreSQL Database directory appears to contain a database<span class="p">;</span> Skipping initialization

2024-05-11 14:25:21.008 UTC <span class="o">[</span>1] LOG:  starting PostgreSQL 15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">)</span> on x86_64-pc-linux-gnu, compiled by gcc <span class="o">(</span>Debian 10.2.1-6<span class="o">)</span> 10.2.1 20210110, 64-bit
2024-05-11 14:25:21.008 UTC <span class="o">[</span>1] LOG:  listening on IPv4 address <span class="s2">"0.0.0.0"</span>, port 5432
2024-05-11 14:25:21.008 UTC <span class="o">[</span>1] LOG:  listening on IPv6 address <span class="s2">"::"</span>, port 5432
2024-05-11 14:25:21.010 UTC <span class="o">[</span>1] LOG:  listening on Unix socket <span class="s2">"/var/run/postgresql/.s.PGSQL.5432"</span>
2024-05-11 14:25:21.012 UTC <span class="o">[</span>29] LOG:  database system was interrupted<span class="p">;</span> last known up at 2024-05-11 14:21:16 UTC
2024-05-11 14:25:21.017 UTC <span class="o">[</span>29] LOG:  entering standby mode
2024-05-11 14:25:21.026 UTC <span class="o">[</span>29] LOG:  redo starts at 0/5000028
2024-05-11 14:25:21.026 UTC <span class="o">[</span>29] LOG:  consistent recovery state reached at 0/5000100
2024-05-11 14:25:21.026 UTC <span class="o">[</span>1] LOG:  database system is ready to accept read-only connections
2024-05-11 14:25:21.034 UTC <span class="o">[</span>30] LOG:  started streaming WAL from primary at 0/6000000 on timeline 1</code></pre></figure>

<ul>
  <li>Test replication and failover</li>
</ul>

<p>First, let us test the replication, by login to postgres-1, create a zack_customers table, then validating from postgres-2</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># bash into postgres-1, create zack_customers table</span>
root@ubt-server:~# docker <span class="nb">exec</span> <span class="nt">-it</span> postgres-1 bash
root@06dd98085df7:/# psql <span class="nt">--username</span><span class="o">=</span>postgresadmin postgresdb
psql <span class="o">(</span>15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">))</span>
Type <span class="s2">"help"</span> <span class="k">for </span>help.

<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># CREATE TABLE zack_customers (zackname text, z_customer_id serial, date_created timestamp);</span>
CREATE TABLE
<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># \dt</span>
                List of relations
 Schema |      Name      | Type  |     Owner     
<span class="nt">--------</span>+----------------+-------+---------------
 public | zack_customers | table | postgresadmin
<span class="o">(</span>1 row<span class="o">)</span>

<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># \q</span>
root@06dd98085df7:/# <span class="nb">exit
exit</span>

<span class="c"># bash into postgres-2, validate zack_customers table</span>
root@ubt-server:~/postgres-2/pgdata# docker <span class="nb">exec</span> <span class="nt">-it</span> postgres-2 bash
root@b333ff290624:/# psql <span class="nt">--username</span><span class="o">=</span>postgresadmin postgresdb
psql <span class="o">(</span>15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">))</span>
Type <span class="s2">"help"</span> <span class="k">for </span>help.

<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># \dt</span>
                List of relations
 Schema |      Name      | Type  |     Owner     
<span class="nt">--------</span>+----------------+-------+---------------
 public | zack_customers | table | postgresadmin
<span class="o">(</span>1 row<span class="o">)</span>

<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># \q</span>
root@b333ff290624:/# <span class="nb">exit
exit</span></code></pre></figure>

<p>now we simulate failover by using loadbalancer tool “pgctl”, to shut down the primary instance, then promote the standby read-only instance into a read-write instance</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># shut down the primary instance</span>
root@ubt-server:~# docker <span class="nb">rm</span> <span class="nt">-f</span> postgres-1
postgres-1
<span class="c"># exec standby try to create a table zack_customers_2, get error as it's read-only</span>
root@ubt-server:~# docker <span class="nb">exec</span> <span class="nt">-it</span> postgres-2 bash
root@b333ff290624:/# psql <span class="nt">--username</span><span class="o">=</span>postgresadmin postgresdb
psql <span class="o">(</span>15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">))</span>
Type <span class="s2">"help"</span> <span class="k">for </span>help.

<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># CREATE TABLE zack_customers_2 (zackname text, z_customer_id serial, date_created timestamp);</span>
ERROR:  cannot execute CREATE TABLE <span class="k">in </span>a read-only transaction

postgresdb-# <span class="se">\q</span>

<span class="c"># promote postgres-2 from standby to primary</span>
root@b333ff290624:/# runuser <span class="nt">-u</span> postgres <span class="nt">--</span> pg_ctl promote
waiting <span class="k">for </span>server to promote.... <span class="k">done
</span>server promoted

<span class="c"># exec to create table zack_customers_2, this time works</span>
root@b333ff290624:/# psql <span class="nt">--username</span><span class="o">=</span>postgresadmin postgresdb
psql <span class="o">(</span>15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">))</span>
Type <span class="s2">"help"</span> <span class="k">for </span>help.

<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># CREATE TABLE zack_customers_2 (zackname text, z_customer_id serial, date_created timestamp);</span>
CREATE TABLE
<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># \dt</span>
                 List of relations
 Schema |       Name       | Type  |     Owner     
<span class="nt">--------</span>+------------------+-------+---------------
 public | zack_customers   | table | postgresadmin
 public | zack_customers_2 | table | postgresadmin
<span class="o">(</span>2 rows<span class="o">)</span>

<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># \q</span>
root@b333ff290624:/# <span class="nb">exit
exit
</span>root@ubt-server:~# </code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we are able to run a PostgreSQL primary and standby instances to test replication and failover, in the next blog I will discover how to depoly a single PostgreSQL on kubernetes.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[The Replication]]></summary></entry><entry><title type="html">PostgreSQL: Get Started</title><link href="http://localhost:4000/jekyll/cat2/2024/05/06/PS1.html" rel="alternate" type="text/html" title="PostgreSQL: Get Started" /><published>2024-05-06T10:15:29+10:00</published><updated>2024-05-06T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/06/PS1</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/06/PS1.html"><![CDATA[<p><b> The Scenario</b></p>

<p>PostgreSQL is a very popular open-source relational database management systems (RDBMS), for its Extensibility and Feature-Rich, suitable for mission-critical applications, not to mention its active PostgreSQL community.</p>

<p>In the upcoming posts, I will start a series of PostgreSQL study to :</p>

<ul>
  <li>
    <p>explore PostgreSQL main features, installation, basic administration tasks</p>
  </li>
  <li>
    <p>deploy PostgreSQL cluster onto K8S with PostgreSQL Operater, validate backup and rolling upgrade</p>
  </li>
  <li>
    <p>create a simple Flash microservice application to connect PostgreSQL cluster and validate failover</p>
  </li>
  <li>
    <p>integrate the whole deployment into CICD pipeline for automation</p>
  </li>
  <li>
    <p>create AWS RDS PostgreSQL, with S3 Block storage as replica</p>
  </li>
</ul>

<p>By the end of the series we should be able to have a comprehensive understanding of PostgreSQL from a DevOps perspective</p>

<p><b>PostgreSQL Basic</b></p>

<p>To begin, we will</p>

<ul>
  <li>install PostgreSQL as a docker container on a local Ubuntu machine to get it up and running,</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># ubuntu install docker</span>
root@ubt-server:~# curl <span class="nt">-fsSL</span> https://get.docker.com <span class="nt">-o</span> get-docker.sh
root@ubt-server:~# sh get-docker.sh
root@ubt-server:~# docker <span class="nt">--version</span>
root@ubt-server:~# systemctl <span class="nb">enable </span>docker

<span class="c"># install PostgreSQL 15.0</span>
root@ubt-server:~#  docker run <span class="nt">--name</span> zack-postgres <span class="nt">-e</span> <span class="nv">POSTGRES_PASSWORD</span><span class="o">=</span>password <span class="nt">-d</span> postgres:15.0
root@ubt-server:~# docker ps
CONTAINER ID   IMAGE           COMMAND                  CREATED         STATUS         PORTS      NAMES
b4fc638dfde3   postgres:15.0   <span class="s2">"docker-entrypoint.s…"</span>   7 seconds ago   Up 6 seconds   5432/tcp   zack-postgres</code></pre></figure>

<ul>
  <li>Run a simple PostgreSQL database with docker compose</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create docker-compose.yaml and run postgres and adminer from dockercompose</span>
root@ubt-server:~# vim docker-compose.yaml

version: <span class="s1">'3.1'</span>
services:
  db:
    image: postgres:15.0
    restart: always
    environment:
      POSTGRES_PASSWORD: password
    ports:
    - 5000:5432
  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080

<span class="c"># run docker compose</span>
root@ubt-server:~# docker compose up</code></pre></figure>

<ul>
  <li>Validate from adminer web console locahost:8080 with password set in the environment variables</li>
</ul>

<p><img src="/assets/ps1-1.png" alt="image tooltip here" />
<img src="/assets/ps1-2.png" alt="image tooltip here" /></p>

<ul>
  <li>Persist data to mount the PostgreSQL container volume, validate data table after start/stop container 
PostgreSQL stores its data by default under /var/lib/postgresql/data, here we create a /pgdata folder on local machine to mount PostgreSQL default volume</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create local Persist data directory /pgdata</span>
root@ubt-server:~# <span class="nb">mkdir </span>pgdata
<span class="c"># run PostgreSQL to mount local Persist data and Bind a different port</span>
root@ubt-server:~# docker run <span class="nt">-d</span> <span class="nt">-it</span> <span class="nt">--rm</span> <span class="nt">--name</span> zack-postgres2 <span class="nt">-e</span> <span class="nv">POSTGRES_PASSWORD</span><span class="o">=</span>password <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/pgdata:/var/lib/postgresql/data <span class="nt">-p</span> 5000:5432 postgres:15.0

PostgreSQL Database directory appears to contain a database<span class="p">;</span> Skipping initialization

2024-05-08 00:58:47.540 UTC <span class="o">[</span>1] LOG:  starting PostgreSQL 15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">)</span> on x86_64-pc-linux-gnu, compiled by gcc <span class="o">(</span>Debian 10.2.1-6<span class="o">)</span> 10.2.1 20210110, 64-bit
2024-05-08 00:58:47.541 UTC <span class="o">[</span>1] LOG:  listening on IPv4 address <span class="s2">"0.0.0.0"</span>, port 5432
2024-05-08 00:58:47.541 UTC <span class="o">[</span>1] LOG:  listening on IPv6 address <span class="s2">"::"</span>, port 5432
2024-05-08 00:58:47.542 UTC <span class="o">[</span>1] LOG:  listening on Unix socket <span class="s2">"/var/run/postgresql/.s.PGSQL.5432"</span>
2024-05-08 00:58:47.545 UTC <span class="o">[</span>28] LOG:  database system was shut down at 2024-05-08 00:57:45 UTC
2024-05-08 00:58:47.547 UTC <span class="o">[</span>1] LOG:  database system is ready to accept connections</code></pre></figure>

<ul>
  <li>Connect to DB container and validate</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># enter the container</span>
root@ubt-server:~# docker <span class="nb">exec</span> <span class="nt">-it</span> zack-postgres2 bash
<span class="c"># login to postgres</span>
root@d7386c566872:/# psql <span class="nt">-h</span> localhost <span class="nt">-U</span> postgres
psql <span class="o">(</span>15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">))</span>
Type <span class="s2">"help"</span> <span class="k">for </span>help.
<span class="c"># create a table</span>
<span class="nv">postgres</span><span class="o">=</span><span class="c"># CREATE TABLE customers (firstname text,lastname text, customer_id serial);</span>
CREATE TABLE
<span class="c"># add record</span>
<span class="nv">postgres</span><span class="o">=</span><span class="c"># INSERT INTO customers (firstname, lastname) VALUES ( 'Bob', 'Smith');</span>
INSERT 0 1
<span class="c"># show table</span>
<span class="nv">postgres</span><span class="o">=</span><span class="c"># \dt</span>
           List of relations
 Schema |   Name    | Type  |  Owner   
<span class="nt">--------</span>+-----------+-------+----------
 public | customers | table | postgres
<span class="o">(</span>1 row<span class="o">)</span>
<span class="c"># get records</span>
<span class="nv">postgres</span><span class="o">=</span><span class="c"># SELECT * FROM customers;</span>
 firstname | lastname | customer_id 
<span class="nt">-----------</span>+----------+-------------
 Bob       | Smith    |           1
<span class="o">(</span>1 row<span class="o">)</span>
<span class="c"># quit </span>
<span class="nv">postgres</span><span class="o">=</span><span class="c"># \q</span>
<span class="c"># exit db container</span>
root@d7386c566872:/# <span class="nb">exit
exit</span></code></pre></figure>

<ul>
  <li>add persist data in docker-compose and run PostgreSQL from compose</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># add presist data folder in compose yaml</span>
root@ubt-server:~# vim docker-compose.yaml

version: <span class="s1">'3.1'</span>
services:
  db:
    image: postgres:15.0
    restart: always
    environment:
      POSTGRES_PASSWORD: admin123
    ports:
    - 5000:5432
    volumes:
    - ./pgdata:/var/lib/postgresql/data
  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080

root@ubt-server:~# docker compose up</code></pre></figure>

<ul>
  <li>Validate the previous table and record from adminer console</li>
</ul>

<p>Table and record still there because of the persistent data mount
<img src="/assets/ps1-3.png" alt="image tooltip here" /></p>

<p><b>PostgreSQL Configuration</b></p>

<p>Before jumping into replication, it is more important to explore the PostgreSQL configuration files to have a better understanding of its important config, take the default conf files out of a running database and learn it and make own configuration, then mount these conf files into container, so tell PostgreSQL to use my own configuration files to perform my prefered way.</p>

<p>To achieve this, we need the db user “postgres” has ID of 999 with access to custom conf files.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~/pgdata# <span class="nb">chown </span>999:999 config/postgresql.conf
root@ubt-server:~/pgdata# <span class="nb">chown </span>999:999 config/pg_hba.conf
root@ubt-server:~/pgdata# <span class="nb">chown </span>999:999 config/pg_ident.conf

root@ubt-server:~/pgdata# ll <span class="k">*</span>.conf
<span class="nt">-rw-------</span> 1 lxd docker  4821 May  8 00:30 pg_hba.conf
<span class="nt">-rw-------</span> 1 lxd docker  1636 May  8 00:30 pg_ident.conf
<span class="nt">-rw-------</span> 1 lxd docker    88 May  8 00:30 postgresql.auto.conf
<span class="nt">-rw-------</span> 1 lxd docker 29525 May  8 00:30 postgresql.conf

root@ubt-server:~# <span class="nb">mkdir </span>config
root@ubt-server:~# <span class="nb">cd </span>config/
root@ubt-server:~# <span class="nb">cp</span> <span class="k">*</span>.conf /config

root@ubt-server:~/pgdata# <span class="nb">chown </span>999:999 config/postgresql.conf
root@ubt-server:~/pgdata# <span class="nb">chown </span>999:999 config/pg_hba.conf
root@ubt-server:~/pgdata# <span class="nb">chown </span>999:999 config/pg_ident.conf</code></pre></figure>

<p>The official PostgreSQL documentation explains those configuration files as below:</p>

<ul>
  <li>pg_hba.conf:</li>
</ul>

<p>This file stands for “PostgreSQL Host-Based Authentication.” It controls client authentication based on the host and user information. It specifies which hosts are allowed to connect to the PostgreSQL server, which databases and users they can access, and what authentication methods they must use. It’s a crucial security measure for controlling access to PostgreSQL server.</p>

<ul>
  <li>pg_ident.conf:</li>
</ul>

<p>This file, “PostgreSQL Identification Mapping,” allows to define mappings between external (e.g., operating system) and internal (PostgreSQL) user names.</p>

<ul>
  <li>postgresql.conf:
Main configuration file for PostgreSQL which contains global settings to tailor its behavior to specific requirements and environment.</li>
</ul>

<p>Create custom config file 
<img src="/assets/ps1-4.png" alt="image tooltip here" /></p>

<p>now we can adjust the command by adding environment variables to run PostgreSQL from docker and docker-compose using our custom conf files</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~# vim docker-compose.yaml
version: <span class="s1">'3.1'</span>
services:
  db:
    container_name: postgres
    image: postgres:15.0
<span class="c"># important: passing argument to postgres container tell where conf file located # to match the custom conf file we created before when DB initiate       </span>
    <span class="nb">command</span>: <span class="s2">"postgres -c config_file=/config/postgresql.conf"</span>
    environment:
      POSTGRES_USER: <span class="s2">"postgresadmin"</span>
      POSTGRES_PASSWORD: <span class="s2">"admin123"</span>
      POSTGRES_DB: <span class="s2">"postgresdb"</span>
      PGDATA: <span class="s2">"/data"</span>
    volumes:
    - ./pgdata:/data
    - ./config:/config/
    ports:
    - 5000:5432
  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080

root@ubt-server:~# docker run <span class="nt">-it</span> <span class="nt">--rm</span> <span class="nt">--name</span> postgres <span class="nt">-e</span> <span class="nv">POSTGRES_USER</span><span class="o">=</span>postgresadmin <span class="nt">-e</span> <span class="nv">POSTGRES_PASSWORD</span><span class="o">=</span>admin123 <span class="nt">-e</span> <span class="nv">POSTGRES_DB</span><span class="o">=</span>postgresdb <span class="nt">-e</span> <span class="nv">PGDATA</span><span class="o">=</span><span class="s2">"/data"</span> <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/pgdata:/data <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/config:/config <span class="nt">-p</span> 5000:5432 postgres:15.0 <span class="nt">-c</span> <span class="s1">'config_file=/config/postgresql.conf'</span>

PostgreSQL Database directory appears to contain a database<span class="p">;</span> Skipping initialization

2024-05-10 10:40:44.685 UTC <span class="o">[</span>1] LOG:  starting PostgreSQL 15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">)</span> on x86_64-pc-linux-gnu, compiled by gcc <span class="o">(</span>Debian 10.2.1-6<span class="o">)</span> 10.2.1 20210110, 64-bit
2024-05-10 10:40:44.685 UTC <span class="o">[</span>1] LOG:  listening on IPv4 address <span class="s2">"0.0.0.0"</span>, port 5432
2024-05-10 10:40:44.685 UTC <span class="o">[</span>1] LOG:  listening on IPv6 address <span class="s2">"::"</span>, port 5432
2024-05-10 10:40:44.686 UTC <span class="o">[</span>1] LOG:  listening on Unix socket <span class="s2">"/var/run/postgresql/.s.PGSQL.5432"</span>
2024-05-10 10:40:44.688 UTC <span class="o">[</span>28] LOG:  database system was shut down at 2024-05-10 10:30:42 UTC
2024-05-10 10:40:44.690 UTC <span class="o">[</span>1] LOG:  database system is ready to accept connections

root@ubt-server:~# docker compose up <span class="nt">-d</span>
WARN[0000] /root/docker-compose.yaml: <span class="sb">`</span>version<span class="sb">`</span> is obsolete 
<span class="o">[</span>+] Running 2/2
 ✔ Container postgres        Started                                                                                            0.4s 
 ✔ Container root-adminer-1  Started</code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we can run a PostgreSQL container from docker and docker-compose with Persist data and custom configuration mount into the container, in the next blog we will discover primary and standby replication, WAL (write ahead log) options.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[The Scenario]]></summary></entry></feed>