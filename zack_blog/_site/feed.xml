<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-05-09T21:29:57+10:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Zack’s Blog</title><subtitle>## AWS   ## Jenkins  ## Microservices ## Automation ## K8S   ## CICD     ## Gitops  [京ICP备2024056683号-1]</subtitle><entry><title type="html">PostgreSQL Series: Basic</title><link href="http://localhost:4000/jekyll/cat2/2024/05/06/PS1.html" rel="alternate" type="text/html" title="PostgreSQL Series: Basic" /><published>2024-05-06T10:15:29+10:00</published><updated>2024-05-06T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/06/PS1</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/06/PS1.html"><![CDATA[<p><b> The Scenario</b></p>

<p>PostgreSQL is a very popular open-source relational database management systems (RDBMS), for its Extensibility and Feature-Rich, suitable for mission-critical applications, not to mention its active PostgreSQL community.</p>

<p>In the upcoming posts, I will start a series of PostgreSQL study to :</p>

<ul>
  <li>
    <p>explore PostgreSQL main features, installation, basic administration tasks</p>
  </li>
  <li>
    <p>deploy PostgreSQL cluster onto K8S with PostgreSQL Operater, validate backup and rolling upgrade</p>
  </li>
  <li>
    <p>create a simple Flash microservice application to connect PostgreSQL cluster and validate failover</p>
  </li>
  <li>
    <p>integrate the whole deployment into CICD pipeline for automation</p>
  </li>
  <li>
    <p>create AWS RDS PostgreSQL, with S3 Block storage as replica</p>
  </li>
</ul>

<p>By the end of the series we should be able to have a comprehensive understanding of PostgreSQL from a DevOps perspective</p>

<p><b>PostgreSQL Basic</b></p>

<p>To begin, we will</p>

<ul>
  <li>install PostgreSQL as a docker container on a local Ubuntu machine to get it up and running,</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># ubuntu install docker</span>
root@ubt-server:~# curl <span class="nt">-fsSL</span> https://get.docker.com <span class="nt">-o</span> get-docker.sh
root@ubt-server:~# sh get-docker.sh
root@ubt-server:~# docker <span class="nt">--version</span>
root@ubt-server:~# systemctl <span class="nb">enable </span>docker

<span class="c"># install PostgreSQL 15.0</span>
root@ubt-server:~#  docker run <span class="nt">--name</span> zack-postgres <span class="nt">-e</span> <span class="nv">POSTGRES_PASSWORD</span><span class="o">=</span>password <span class="nt">-d</span> postgres:15.0
root@ubt-server:~# docker ps
CONTAINER ID   IMAGE           COMMAND                  CREATED         STATUS         PORTS      NAMES
b4fc638dfde3   postgres:15.0   <span class="s2">"docker-entrypoint.s…"</span>   7 seconds ago   Up 6 seconds   5432/tcp   zack-postgres</code></pre></figure>

<ul>
  <li>Run a simple PostgreSQL database with docker compose</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create docker-compose.yaml and run postgres and adminer from dockercompose</span>
root@ubt-server:~# vim docker-compose.yaml

version: <span class="s1">'3.1'</span>
services:
  db:
    image: postgres:15.0
    restart: always
    environment:
      POSTGRES_PASSWORD: password
    ports:
    - 5000:5432
  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080

<span class="c"># run docker compose</span>
root@ubt-server:~# docker compose up</code></pre></figure>

<ul>
  <li>Validate from adminer web console locahost:8080 with password set in the environment variables</li>
</ul>

<p><img src="/assets/ps1-1.png" alt="image tooltip here" />
<img src="/assets/ps1-2.png" alt="image tooltip here" /></p>

<ul>
  <li>Persist data to mount the PostgreSQL container volume, validate data table after start/stop container 
PostgreSQL stores its data by default under /var/lib/postgresql/data, here we create a /pgdata folder on local machine to mount PostgreSQL default volume</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create local Persist data directory /pgdata</span>
root@ubt-server:~# <span class="nb">mkdir </span>pgdata
<span class="c"># run PostgreSQL to mount local Persist data and Bind a different port</span>
root@ubt-server:~# docker run <span class="nt">-d</span> <span class="nt">-it</span> <span class="nt">--rm</span> <span class="nt">--name</span> zack-postgres2 <span class="nt">-e</span> <span class="nv">POSTGRES_PASSWORD</span><span class="o">=</span>password <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/pgdata:/var/lib/postgresql/data <span class="nt">-p</span> 5000:5432 postgres:15.0

PostgreSQL Database directory appears to contain a database<span class="p">;</span> Skipping initialization

2024-05-08 00:58:47.540 UTC <span class="o">[</span>1] LOG:  starting PostgreSQL 15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">)</span> on x86_64-pc-linux-gnu, compiled by gcc <span class="o">(</span>Debian 10.2.1-6<span class="o">)</span> 10.2.1 20210110, 64-bit
2024-05-08 00:58:47.541 UTC <span class="o">[</span>1] LOG:  listening on IPv4 address <span class="s2">"0.0.0.0"</span>, port 5432
2024-05-08 00:58:47.541 UTC <span class="o">[</span>1] LOG:  listening on IPv6 address <span class="s2">"::"</span>, port 5432
2024-05-08 00:58:47.542 UTC <span class="o">[</span>1] LOG:  listening on Unix socket <span class="s2">"/var/run/postgresql/.s.PGSQL.5432"</span>
2024-05-08 00:58:47.545 UTC <span class="o">[</span>28] LOG:  database system was shut down at 2024-05-08 00:57:45 UTC
2024-05-08 00:58:47.547 UTC <span class="o">[</span>1] LOG:  database system is ready to accept connections</code></pre></figure>

<ul>
  <li>Connect to DB container and validate</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># enter the container</span>
root@ubt-server:~# docker <span class="nb">exec</span> <span class="nt">-it</span> zack-postgres2 bash
<span class="c"># login to postgres</span>
root@d7386c566872:/# psql <span class="nt">-h</span> localhost <span class="nt">-U</span> postgres
psql <span class="o">(</span>15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">))</span>
Type <span class="s2">"help"</span> <span class="k">for </span>help.
<span class="c"># create a table</span>
<span class="nv">postgres</span><span class="o">=</span><span class="c"># CREATE TABLE customers (firstname text,lastname text, customer_id serial);</span>
CREATE TABLE
<span class="c"># add record</span>
<span class="nv">postgres</span><span class="o">=</span><span class="c"># INSERT INTO customers (firstname, lastname) VALUES ( 'Bob', 'Smith');</span>
INSERT 0 1
<span class="c"># show table</span>
<span class="nv">postgres</span><span class="o">=</span><span class="c"># \dt</span>
           List of relations
 Schema |   Name    | Type  |  Owner   
<span class="nt">--------</span>+-----------+-------+----------
 public | customers | table | postgres
<span class="o">(</span>1 row<span class="o">)</span>
<span class="c"># get records</span>
<span class="nv">postgres</span><span class="o">=</span><span class="c"># SELECT * FROM customers;</span>
 firstname | lastname | customer_id 
<span class="nt">-----------</span>+----------+-------------
 Bob       | Smith    |           1
<span class="o">(</span>1 row<span class="o">)</span>
<span class="c"># quit </span>
<span class="nv">postgres</span><span class="o">=</span><span class="c"># \q</span>
<span class="c"># exit db container</span>
root@d7386c566872:/# <span class="nb">exit
exit</span></code></pre></figure>

<ul>
  <li>add persist data in docker-compose and run PostgreSQL from compose</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># add presist data folder in compose yaml</span>
root@ubt-server:~# vim docker-compose.yaml

version: <span class="s1">'3.1'</span>
services:
  db:
    image: postgres:15.0
    restart: always
    environment:
      POSTGRES_PASSWORD: admin123
    ports:
    - 5000:5432
    volumes:
    - ./pgdata:/var/lib/postgresql/data
  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080

root@ubt-server:~# docker compose up</code></pre></figure>

<ul>
  <li>Validate the previous table and record from adminer console</li>
</ul>

<p>Table and record still there because of the persistent data mount
<img src="/assets/ps1-3.png" alt="image tooltip here" /></p>

<p><b> Conclusion</b></p>

<p>Now we can run a Basic PostgreSQL container from docker and docker compose with Persist data, in next blog we will discover ton of configuration options and explore environment variables for PostgreSQL config file.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[The Scenario]]></summary></entry><entry><title type="html">RedHat Idm on AWS PoC</title><link href="http://localhost:4000/jekyll/cat2/2024/05/01/AWS-AD-idm.html" rel="alternate" type="text/html" title="RedHat Idm on AWS PoC" /><published>2024-05-01T10:15:29+10:00</published><updated>2024-05-01T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/01/AWS-AD-idm</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/01/AWS-AD-idm.html"><![CDATA[<p><b> RedHat idm on AWS design </b></p>

<p>The proof of concept aims to achieve that, where a Red Hat IDM (Identity Management) server hosted in AWS with its own domain (zack.linux.com) is integrated with an existing AWS Managed AD (zack.existing.com) for authentication of Linux EC2 instances across multiple AWS accounts. The AWS managed AD also synced with Azure AD.</p>

<p>In the future picture, all users will be created and maintained centrally from Azure AD, then placed into an OU named “idm” if they require AWS Linux EC2 instances access. Then Red Hat IDM would be able to add or sync those users and configure RBAC and HBAC accordingly.</p>

<p><b> Set up Red Hat IDM Server and AWS managed AD:</b></p>

<ul>
  <li>
    <p>Launch a Red Hat idm EC2 instance</p>
  </li>
  <li>
    <p>Follow previous post for idm installation and configuration.</p>
  </li>
  <li>
    <p>create AWS managed directory service</p>
  </li>
</ul>

<p><img src="/assets/awsad1.png" alt="image tooltip here" /></p>

<ul>
  <li>create ADDC management windows ec2 instance, create test user1 from domain zack.existing.com</li>
</ul>

<p><b> Configure DNS:</b></p>

<ul>
  <li>
    <p>Set up DNS resolution to ensure that the IDM server and the existing AWS Managed AD can resolve each other’s domain names.</p>
  </li>
  <li>
    <p>Create DNS records to point idm.poc.com to the IDM server’s IP address and configure DNS forwarding or conditional forwarding to resolve queries for the zackad.awsmanaged.com domain to AWS Managed AD DNS servers.</p>
  </li>
</ul>

<p><b> Establish Trust Relationship:</b></p>

<ul>
  <li>
    <p>Set up a two-way trust relationship between the Red Hat IDM domain (idm.poc.com) and the AWS Managed AD domain (zackad.awsmanaged.com).</p>
  </li>
  <li>
    <p>Follow the AWS documentation for establishing trust relationships with AWS Managed Microsoft AD.</p>
  </li>
  <li>
    <p>Configure trust settings on both sides to allow authentication and authorization between the domains.</p>
  </li>
</ul>

<p><b> Configure SSSD on existing Linux EC2 Instances:</b></p>

<ul>
  <li>
    <p>Install and configure the System Security Services Daemon (SSSD) on your Linux EC2 instances.</p>
  </li>
  <li>
    <p>Configure SSSD to authenticate users against the Red Hat IDM server.</p>
  </li>
  <li>
    <p>Ensure that SSSD is configured to use the trust relationship established between the IDM domain and the AWS Managed AD domain for authentication.</p>
  </li>
</ul>

<p><b> Role-Based and Host-Based Access Control:</b></p>

<ul>
  <li>
    <p>Define roles and access policies within the Red Hat IDM server for role-based access control.</p>
  </li>
  <li>
    <p>Utilize the trust relationship between the IDM domain and the AWS Managed AD domain to grant access based on user groups and permissions stored in both directories.</p>
  </li>
  <li>
    <p>Implement host-based access control using SSSD configuration on the Linux EC2 instances, leveraging group memberships and permissions from both IDM and AWS Managed AD.</p>
  </li>
  <li>
    <p>design pre-define roles in idm (developer, admin, vendor and etc)</p>
  </li>
  <li>
    <p>design pre-define host groups in idm (aws accounts vs Lab, Dev, Staging and Prd)</p>
  </li>
</ul>

<p><b> Establish automation for future host and user management</b></p>

<ul>
  <li>
    <p>use ansible or other scripted method to add or remove host and user</p>
  </li>
  <li>
    <p>enable ansible dynamic inventory to list and descover new or existing AWS linux ec2 instances</p>
  </li>
</ul>

<p>More info can be found via <a href="https://freeipa.readthedocs.io/en/latest/workshop.html">Freeipa workshop</a>, <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_idm_users_groups_hosts_and_access_control_rules/index">Red Hat product documentation</a>, <a href="https://chamathb.wordpress.com/2019/06/21/setting-up-rhel-idm-with-integrated-dns-on-aws/">Redhat Idm on AWS with DNS forwarder</a>, <a href="https://www.reddit.com/r/redhat/comments/6ixtoe/idmfreeipa_dns_forwarding/">idmfreeipa DNS forwarder configurations on AWS</a>, and <a href="https://redhat.com/en/blog/automating-red-hat-identity-management-installation">Automating Red Hat Identity Management installation with Ansible</a>.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[RedHat idm on AWS design]]></summary></entry><entry><title type="html"> Automate massive AWS EC2 tagging</title><link href="http://localhost:4000/jekyll/cat2/2024/05/01/AWS-tagging.html" rel="alternate" type="text/html" title=" Automate massive AWS EC2 tagging" /><published>2024-05-01T10:15:29+10:00</published><updated>2024-05-01T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/01/AWS-tagging</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/01/AWS-tagging.html"><![CDATA[<p><b> Backgroud </b></p>

<p>I was tasked to enforce mandatory tagging for ec2 instances, as there are a lot of machines and a lot of tags need to be attached to each machine, here I need a scripted way to get the job done.</p>

<p><b> How to achieve </b></p>

<ul>
  <li>Prepare a list of ec2 instances with default name tag only,</li>
</ul>

<p><img src="/assets/awstag2.png" alt="image tooltip here" /></p>

<ul>
  <li>Open cloud shell or ssh to a linux box where AWSCli installed and configured to a AWS account, export the instances with ID, export to a csv file</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~# aws ec2 describe-instances <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[*].Instances[*].[InstanceId]'</span> <span class="o">&gt;</span> zztag.csv</code></pre></figure>

<ul>
  <li>Then add the header row for “instance ID” “tagA”  ”valueA”    ”tagB”  ”valueB”</li>
</ul>

<p><img src="/assets/awstag1.png" alt="image tooltip here" /></p>

<p><b> Create shell script to read the CSV file line by line, and add tags for each instance </b></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~# vim zacktag.sh

<span class="c">#!/bin/bash</span>

<span class="c"># Read the CSV file line by line</span>
<span class="k">while </span><span class="nv">IFS</span><span class="o">=</span>, <span class="nb">read</span> <span class="nt">-r</span> instance_id tagA valueA tagB valueB <span class="o">||</span> <span class="o">[</span> <span class="nt">-n</span> <span class="s2">"</span><span class="nv">$instance_id</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">do</span>
    <span class="c"># Add tagA</span>
    aws ec2 create-tags <span class="nt">--resources</span> <span class="s2">"</span><span class="nv">$instance_id</span><span class="s2">"</span> <span class="nt">--tags</span> <span class="nv">Key</span><span class="o">=</span><span class="s2">"</span><span class="nv">$tagA</span><span class="s2">"</span>,Value<span class="o">=</span><span class="s2">"</span><span class="nv">$valueA</span><span class="s2">"</span>
    <span class="c"># Add tagB</span>
    aws ec2 create-tags <span class="nt">--resources</span> <span class="s2">"</span><span class="nv">$instance_id</span><span class="s2">"</span> <span class="nt">--tags</span> <span class="nv">Key</span><span class="o">=</span><span class="s2">"</span><span class="nv">$tagB</span><span class="s2">"</span>,Value<span class="o">=</span><span class="s2">"</span><span class="nv">$valueB</span><span class="s2">"</span>
<span class="k">done</span> &lt; zztag.csv

root@ubt-server:~# <span class="nb">chmod</span> +x zacktag.sh <span class="o">&amp;&amp;</span> sh zacktag.sh</code></pre></figure>

<ul>
  <li>Validate now ec2 instances with all tags attached</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~# aws ec2 describe-tags <span class="nt">--filters</span> <span class="s2">"Name=resource-id,Values=i-0980018fc6f4f722c"</span>
<span class="o">{</span>
    <span class="s2">"Tags"</span>: <span class="o">[</span>
        <span class="o">{</span>
            <span class="s2">"Key"</span>: <span class="s2">"Name"</span>,
            <span class="s2">"ResourceId"</span>: <span class="s2">"i-0980018fc6f4f722c"</span>,
            <span class="s2">"ResourceType"</span>: <span class="s2">"instance"</span>,
            <span class="s2">"Value"</span>: <span class="s2">"testing-for-tagging"</span>
        <span class="o">}</span>,
        <span class="o">{</span>
            <span class="s2">"Key"</span>: <span class="s2">"zz1"</span>,
            <span class="s2">"ResourceId"</span>: <span class="s2">"i-0980018fc6f4f722c"</span>,
            <span class="s2">"ResourceType"</span>: <span class="s2">"instance"</span>,
            <span class="s2">"Value"</span>: <span class="s2">"aa5"</span>
        <span class="o">}</span>,
        <span class="o">{</span>
            <span class="s2">"Key"</span>: <span class="s2">"zz2"</span>,
            <span class="s2">"ResourceId"</span>: <span class="s2">"i-0980018fc6f4f722c"</span>,
            <span class="s2">"ResourceType"</span>: <span class="s2">"instance"</span>,
            <span class="s2">"Value"</span>: <span class="s2">"bb4"</span>
        <span class="o">}</span>
    <span class="o">]</span>
<span class="o">}</span>
root@ubt-server:~# aws ec2 describe-tags <span class="nt">--filters</span> <span class="s2">"Name=resource-id,Values=i-076226daa5aaf7cf2"</span>
<span class="o">{</span>
    <span class="s2">"Tags"</span>: <span class="o">[</span>
        <span class="o">{</span>
            <span class="s2">"Key"</span>: <span class="s2">"Name"</span>,
            <span class="s2">"ResourceId"</span>: <span class="s2">"i-076226daa5aaf7cf2"</span>,
            <span class="s2">"ResourceType"</span>: <span class="s2">"instance"</span>,
            <span class="s2">"Value"</span>: <span class="s2">"zack-blog"</span>
        <span class="o">}</span>,
        <span class="o">{</span>
            <span class="s2">"Key"</span>: <span class="s2">"zz1"</span>,
            <span class="s2">"ResourceId"</span>: <span class="s2">"i-076226daa5aaf7cf2"</span>,
            <span class="s2">"ResourceType"</span>: <span class="s2">"instance"</span>,
            <span class="s2">"Value"</span>: <span class="s2">"aa1"</span>
        <span class="o">}</span>,
        <span class="o">{</span>
            <span class="s2">"Key"</span>: <span class="s2">"zz2"</span>,
            <span class="s2">"ResourceId"</span>: <span class="s2">"i-076226daa5aaf7cf2"</span>,
            <span class="s2">"ResourceType"</span>: <span class="s2">"instance"</span>,
            <span class="s2">"Value"</span>: <span class="s2">"aa2"</span>
        <span class="o">}</span>
    <span class="o">]</span>
<span class="o">}</span></code></pre></figure>

<p><img src="/assets/awstag3.png" alt="image tooltip here" /></p>

<ul>
  <li>create cronjob to update tagging monthly</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create a monthly script run job</span>
root@ubt-server:~# crontab <span class="nt">-e</span>
no crontab <span class="k">for </span>root - using an empty one

Select an editor.  To change later, run <span class="s1">'select-editor'</span><span class="nb">.</span>
  1. /bin/nano        &lt;<span class="nt">----</span> easiest
  2. /usr/bin/vim.basic
  3. /usr/bin/vim.tiny
  4. /bin/ed

Choose 1-4 <span class="o">[</span>1]: 2
crontab: installing new crontab

<span class="c"># List the scheduled cronjob</span>
root@ubt-server:~# crontab <span class="nt">-l</span>
<span class="k">*</span> <span class="k">*</span> <span class="k">*</span> <span class="k">*</span> <span class="k">*</span> ~/zacktag.sh</code></pre></figure>

<p><b> Conclusion </b></p>

<p>Now we have a scripted way to achieve adding different tags for multiple ec2 instances via AWS CLI and shell script, together with cronjob, we can only update the csv file which regularly updates instances and tags we want to attach, upload the csv file, every month all ec2 and their tags will be updated accordingly.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Backgroud]]></summary></entry><entry><title type="html">Two options for AWS Serverless web hosting</title><link href="http://localhost:4000/jekyll/cat2/2024/04/30/serverless.html" rel="alternate" type="text/html" title="Two options for AWS Serverless web hosting" /><published>2024-04-30T10:15:29+10:00</published><updated>2024-04-30T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/04/30/serverless</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/04/30/serverless.html"><![CDATA[<p><b> Time to move “zackweb” from existing docker and containerization on EC2 and K8S to AWS Serverless with S3 </b></p>

<p>In this article, I will see how to host “zackweb” as a static web application using bellow AWS serverless options:</p>

<ul>
  <li>
    <p>S3 static webhosting</p>
  </li>
  <li>
    <p>AWS CDK + CloudFront</p>
  </li>
</ul>

<p><b> Prerequisite </b></p>

<ul>
  <li>Add one more step in existing Github Action workflow to copy the static web content to newly created S3 bucket</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># edit github action workflow</span>
aws s3 <span class="nb">cp</span> ~/zack-gitops-project/zack_blog/_site/<span class="k">*</span> s3://zackweb-serverless/ <span class="nt">--recursive</span>

<span class="c"># validate content in s3 bucket</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span>aws s3 <span class="nb">ls </span>s3://zackweb-serverless <span class="nt">--summarize</span>
                           PRE aboutme/
                           PRE assets/
                           PRE certificate/
                           PRE gitrepo/
                           PRE jekyll/
                           PRE pro/
                           PRE skillroadmap/
2024-04-30 14:55:05       4455 404.html
2024-04-30 14:55:05        504 Dockerfile
2024-04-30 14:55:06      80555 feed.xml
2024-04-30 14:55:06       7760 index.html
2024-04-30 14:55:06          0 nginx.conf

Total Objects: 5
   Total Size: 93274</code></pre></figure>

<p><b> Option 1: S3 static webhosting </b></p>

<p>Go AWS console, under S3 bucket “zackweb-serverless” properties, enable static website hosting, update the bucket website endpoint address to Godaddy DNS record.</p>

<p><img src="/assets/serverless2.png" alt="image tooltip here" /></p>

<p><b> Option 2: using AWS CDK + CDN </b></p>

<p>With AWS CDK and CDN, the “zackweb” can be straightforward distributed from an S3 bucket accessible to the public by using CloudFront.</p>

<p><img src="/assets/serverless3.png" alt="image tooltip here" /></p>

<p>the steps will be:</p>

<ol>
  <li>
    <p>Enable AWS CDK on EC2 bastion host.</p>
  </li>
  <li>
    <p>S3 bucker ready and copy static web content into it (done above with modification of existing github action workflow)</p>
  </li>
  <li>
    <p>Establish a CloudFront distribution to host a static To-Do web application.</p>
  </li>
  <li>
    <p>Deploy the AWS CDK solution to host the To-do application.</p>
  </li>
</ol>

<ul>
  <li>install AWS CDK on bastion EC2 host</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># AWS CDK requires nodejs newer version</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span><span class="nb">sudo </span>apt-get <span class="nb">install </span>nodejs <span class="nt">-y</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span><span class="nb">sudo </span>npm cache clean <span class="nt">-f</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span><span class="nb">sudo </span>npm <span class="nb">install</span> <span class="nt">-g</span> n
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span><span class="nb">sudo </span>n stable
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span>nodejs <span class="nt">--version</span>
v12.22.9

<span class="c"># install aws-cdk cli</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span>npm <span class="nb">install</span> <span class="nt">-g</span> aws-cdk
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span>cdk <span class="nt">--version</span>
2.139.1 <span class="o">(</span>build b88f959<span class="o">)</span>

<span class="c"># check aws credential and bootstrap CDK</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span>aws sts get-caller-identity
<span class="o">{</span>
    <span class="s2">"UserId"</span>: <span class="s2">"AIDxxxxxxxxx7ZV"</span>,
    <span class="s2">"Account"</span>: <span class="s2">"8xxxxxx342"</span>,
    <span class="s2">"Arn"</span>: <span class="s2">"arn:aws:iam::8xxxxx342:user/zackcdk"</span>
<span class="o">}</span>

<span class="c"># bootstrap CDK</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span><span class="nb">sudo </span>cdk bootstrap aws://8xxxxxxx2/ap-southeast-2

<span class="c"># init app</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span> mkdir cdk
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span><span class="nb">cd </span>cdk
ubuntu@ip-172-31-26-78:~/cdk# cdk init app <span class="nt">--language</span><span class="o">=</span>typescript
Initializing a new git repository...
Executing npm install...
✅ All <span class="k">done</span><span class="o">!</span>

<span class="c"># create CDK code</span>

ubuntu@ip-172-31-26-78:~/cdk/lib# vim cdk-stack.ts

import <span class="k">*</span> as cdk from <span class="s1">'@aws-cdk/core'</span><span class="p">;</span>
import <span class="k">*</span> as cloudfront from <span class="s1">'@aws-cdk/aws-cloudfront'</span><span class="p">;</span>
import <span class="k">*</span> as origins from <span class="s1">'@aws-cdk/aws-cloudfront-origins'</span><span class="p">;</span>

<span class="nb">export </span>class ZackWebStack extends cdk.Stack <span class="o">{</span>
  constructor<span class="o">(</span>scope: cdk.Construct, <span class="nb">id</span>: string, props?: cdk.StackProps<span class="o">)</span> <span class="o">{</span>
    super<span class="o">(</span>scope, <span class="nb">id</span>, props<span class="o">)</span><span class="p">;</span>

    // existing S3 bucket
    const existingBucketName <span class="o">=</span> <span class="s1">'zackweb-serverless'</span><span class="p">;</span>

    // Create a CloudFront distribution
    const distribution <span class="o">=</span> new cloudfront.Distribution<span class="o">(</span>this, <span class="s1">'MyDistribution'</span>, <span class="o">{</span>
      defaultBehavior: <span class="o">{</span>
        origin: new origins.S3OriginFromBucketName<span class="o">(</span>existingBucketName<span class="o">)</span>
      <span class="o">}</span>,
      defaultRootObject: <span class="s1">'index.html'</span> // default root object
    <span class="o">})</span><span class="p">;</span>

    // Output the CloudFront distribution domain name
    new cdk.CfnOutput<span class="o">(</span>this, <span class="s1">'CloudFrontDomainName'</span>, <span class="o">{</span>
      value: distribution.distributionDomainName
    <span class="o">})</span><span class="p">;</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c"># install required module</span>

ubuntu@ip-172-31-26-78:~/cdk/lib# npm <span class="nb">install</span> @aws-cdk/core
ubuntu@ip-172-31-26-78:~/cdk/lib# npm <span class="nb">install</span> @aws-cdk/aws-cloudfront
ubuntu@ip-172-31-26-78:~/cdk/lib# npm <span class="nb">install</span> @aws-cdk/aws-cloudfront-origins

<span class="c"># Deploy stack</span>
ubuntu@ip-172-31-26-78:~/cdk/lib# <span class="nb">cd</span> ..
ubuntu@ip-172-31-26-78:~/cdk/# cdk deploy</code></pre></figure>

<ul>
  <li>The “zackweb” is now hosted on the AWS with serverless deployment !</li>
</ul>

<p><img src="/assets/serverless4.png" alt="image tooltip here" /></p>

<p><b> Conclusion</b></p>

<p>Now we move the blog onto AWS with serverless website hosting, using both S3 static webhosting and AWS CDK plus Cloudfront.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Time to move “zackweb” from existing docker and containerization on EC2 and K8S to AWS Serverless with S3]]></summary></entry><entry><title type="html">Ubuntu 18.04 to 22.04 in-place upgrade</title><link href="http://localhost:4000/jekyll/cat2/2024/04/28/ubt-upg.html" rel="alternate" type="text/html" title="Ubuntu 18.04 to 22.04 in-place upgrade" /><published>2024-04-28T10:15:29+10:00</published><updated>2024-04-28T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/04/28/ubt-upg</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/04/28/ubt-upg.html"><![CDATA[<p><b> Ubuntu releases EOL roadmap </b></p>

<p>Every single Ubuntu LTS comes with 5 years of standard support. During those five years, bug fixes and security patches will be provided. Ubuntu 18.04 ‘Bionic Beaver’ is reaching End of Standard Support this May. so today we are going to run in-place upgrade for ubuntu 18.04 LTS to 22.04 LTS.</p>

<p><img src="/assets/ubt-upg1.png" alt="image tooltip here" /></p>

<p><b> Pre-upgrade checklist </b></p>

<ul>
  <li>validate current OS version and running service (nginx)</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># current OS version</span>
root@ubuntu-test:~# <span class="nb">cat</span> /etc/os-release 
<span class="nv">NAME</span><span class="o">=</span><span class="s2">"Ubuntu"</span>
<span class="nv">VERSION</span><span class="o">=</span><span class="s2">"18.04.6 LTS (Bionic Beaver)"</span>
<span class="nv">ID</span><span class="o">=</span>ubuntu
<span class="nv">ID_LIKE</span><span class="o">=</span>debian
<span class="nv">PRETTY_NAME</span><span class="o">=</span><span class="s2">"Ubuntu 18.04.6 LTS"</span>
<span class="nv">VERSION_ID</span><span class="o">=</span><span class="s2">"18.04"</span>
<span class="nv">HOME_URL</span><span class="o">=</span><span class="s2">"https://www.ubuntu.com/"</span>
<span class="nv">SUPPORT_URL</span><span class="o">=</span><span class="s2">"https://help.ubuntu.com/"</span>
<span class="nv">BUG_REPORT_URL</span><span class="o">=</span><span class="s2">"https://bugs.launchpad.net/ubuntu/"</span>
<span class="nv">PRIVACY_POLICY_URL</span><span class="o">=</span><span class="s2">"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"</span>
<span class="nv">VERSION_CODENAME</span><span class="o">=</span>bionic
<span class="nv">UBUNTU_CODENAME</span><span class="o">=</span>bionic

<span class="c"># nginx service status</span>

root@ubuntu-test:~# root@ubuntu-test:~# <span class="nb">echo</span> <span class="s2">"ubuntu-inplace-upgrade zack-testing-nginx-service!!"</span>  <span class="o">&gt;&gt;</span> /var/www/html/index.html
root@ubuntu-test:~# systemctl restart nginx
root@ubuntu-test:~# curl localhost
ubuntu-inplace-upgrade zack-testing-nginx-service!!</code></pre></figure>

<ul>
  <li>Fully update the system</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># update system</span>
root@ubuntu-test:~# <span class="nb">sudo </span>apt update
Hit:1 http://au.archive.ubuntu.com/ubuntu bionic InRelease
Hit:2 http://au.archive.ubuntu.com/ubuntu bionic-updates InRelease
Hit:3 http://au.archive.ubuntu.com/ubuntu bionic-backports InRelease
Hit:4 http://au.archive.ubuntu.com/ubuntu bionic-security InRelease
Reading package lists... Done                      
Building dependency tree       
Reading state information... Done
All packages are up to date.
root@ubuntu-test:~# <span class="nb">sudo </span>apt upgrade <span class="nt">-y</span>
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Calculating upgrade... Done
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.

<span class="c"># reboot system before upgrade</span>
root@ubuntu-test:~# <span class="nb">sudo </span><span class="k">do</span><span class="nt">-release-upgrade</span>
Checking <span class="k">for </span>a new Ubuntu release
You have not rebooted after updating a package which requires a reboot. Please reboot before upgrading.
root@ubuntu-test:~# reboot
Connection closing...Socket close.</code></pre></figure>

<ul>
  <li>take full system backup</li>
</ul>

<p>here I took a VM snapshot before upgrade</p>

<p><b> 18.04 to 22.04 upgrade</b></p>

<p>There is no direct upgrade path from 18.04 LTS to Ubuntu 22.04 LTS, so we go Ubuntu 20.04 LTS first and then to Ubuntu 22.04 LTS.</p>

<ul>
  <li>first upgrade to 20.04</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># run upgrade</span>

root@ubuntu-test:~# <span class="nb">sudo </span><span class="k">do</span><span class="nt">-release-upgrade</span>

This session appears to be running under ssh. It is not recommended 
to perform a upgrade over ssh currently because <span class="k">in case</span> of failure it 
is harder to recover. 

If you <span class="k">continue</span>, an additional ssh daemon will be started at port 
<span class="s1">'1022'</span><span class="nb">.</span> 
Do you want to <span class="k">continue</span>? 

Continue <span class="o">[</span>yN] y

Starting additional sshd 

Calculating the changes
  MarkInstall libfwupdplugin1:amd64 &lt; none -&gt; 1.5.11-0ubuntu1~20.04.2 @un uN Ib <span class="o">&gt;</span> <span class="nv">FU</span><span class="o">=</span>1
  Installing libxmlb1 as Depends of libfwupdplugin1
    MarkInstall libxmlb1:amd64 &lt; none -&gt; 0.1.15-2ubuntu1~20.04.1 @un uN <span class="o">&gt;</span> <span class="nv">FU</span><span class="o">=</span>0

Do you want to start the upgrade? 

Continue <span class="o">[</span>yN]  Details <span class="o">[</span>d]y</code></pre></figure>

<ul>
  <li>allow service restart during upgrade</li>
</ul>

<p><img src="/assets/ubt-upg2.png" alt="image tooltip here" /></p>

<ul>
  <li>reboot after upgrade</li>
</ul>

<p>The installation and removing of packages may take some time, then reboot is required after ungrade completion</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">Purging configuration files <span class="k">for </span>ebtables <span class="o">(</span>2.0.11-3build1<span class="o">)</span> ...
Purging configuration files <span class="k">for </span>python3.6-minimal <span class="o">(</span>3.6.9-1~18.04ubuntu1.12<span class="o">)</span> ...
Purging configuration files <span class="k">for </span>mlocate <span class="o">(</span>0.26-3ubuntu3<span class="o">)</span> ...
Processing triggers <span class="k">for </span>dbus <span class="o">(</span>1.12.16-2ubuntu2.3<span class="o">)</span> ...
Processing triggers <span class="k">for </span>systemd <span class="o">(</span>245.4-4ubuntu3.23<span class="o">)</span> ...

System upgrade is complete.

Restart required 

To finish the upgrade, a restart is required. 
If you <span class="k">select</span> <span class="s1">'y'</span> the system will be restarted. 

Continue <span class="o">[</span>yN] y</code></pre></figure>

<ul>
  <li>validate OS and service</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># validate nginx service</span>
root@ubuntu-test:~# curl localhost
ubuntu-inplace-upgrade  zack-testing-nginx-service!!
<span class="c"># validate OS version</span>
root@ubuntu-test:~# <span class="nb">cat</span> /etc/os-release 
<span class="nv">NAME</span><span class="o">=</span><span class="s2">"Ubuntu"</span>
<span class="nv">VERSION</span><span class="o">=</span><span class="s2">"20.04.6 LTS (Focal Fossa)"</span>
<span class="nv">ID</span><span class="o">=</span>ubuntu
<span class="nv">ID_LIKE</span><span class="o">=</span>debian
<span class="nv">PRETTY_NAME</span><span class="o">=</span><span class="s2">"Ubuntu 20.04.6 LTS"</span>
<span class="nv">VERSION_ID</span><span class="o">=</span><span class="s2">"20.04"</span>
<span class="nv">HOME_URL</span><span class="o">=</span><span class="s2">"https://www.ubuntu.com/"</span>
<span class="nv">SUPPORT_URL</span><span class="o">=</span><span class="s2">"https://help.ubuntu.com/"</span>
<span class="nv">BUG_REPORT_URL</span><span class="o">=</span><span class="s2">"https://bugs.launchpad.net/ubuntu/"</span>
<span class="nv">PRIVACY_POLICY_URL</span><span class="o">=</span><span class="s2">"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"</span>
<span class="nv">VERSION_CODENAME</span><span class="o">=</span>focal
<span class="nv">UBUNTU_CODENAME</span><span class="o">=</span>focal</code></pre></figure>

<ul>
  <li>then upgrade to 22.04</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubuntu-test:~# <span class="nb">sudo </span>apt update
Hit:1 http://au.archive.ubuntu.com/ubuntu focal InRelease
Hit:2 http://au.archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:3 http://au.archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:4 http://au.archive.ubuntu.com/ubuntu focal-security InRelease
Reading package lists... Done
Building dependency tree       
Reading state information... Done
All packages are up to date.

root@ubuntu-test:~# <span class="nb">sudo </span>apt upgrade
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Calculating upgrade... Done
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.

root@ubuntu-test:~# <span class="nb">sudo </span><span class="k">do</span><span class="nt">-release-upgrade</span>

<span class="c"># validate after upgrade </span>
root@ubuntu-test:~# curl localhost
ubuntu-inplace-upgrade  zack-testing-nginx-service!!

root@ubuntu-test:~# lsb_release <span class="nt">-a</span>
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 22.04.4 LTS
Release:	22.04
Codename:	jammy</code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we complete the in-place ubuntu OS release upgrade from 18.04 to 22.04. The whole upgrade took about 1 hour to finish, with several comfirmation required during upgrade process. The service nginx was running after each upgrade.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Ubuntu releases EOL roadmap]]></summary></entry><entry><title type="html">RedHat Identity management (IdM)</title><link href="http://localhost:4000/jekyll/cat2/2024/04/04/Redhat-idm.html" rel="alternate" type="text/html" title="RedHat Identity management (IdM)" /><published>2024-04-04T11:15:29+11:00</published><updated>2024-04-04T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/04/04/Redhat-idm</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/04/04/Redhat-idm.html"><![CDATA[<p><b> About Linux Identity management </b></p>

<p>In Linux, user and permission management is crucial for maintaining system security and controlling access to files, directories, and resources.</p>

<p>Redhat provides such enterprise-level solution called redhat Identity management (IdM), also with its opensource free version called freeIPA, to offer centralized authentication, authorization, and identity management services such as Single Sign-On (SSO), Role-Based Access Control (RBAC), Identity Federation, Integration with Microsoft AD and AAD, also can cross-cloud access with AWS SSO as external identity provider.</p>

<p>when a company facing challenge to manage its Linux environments across local and public cloud, RedHat Identity management can be the solution to achieve:</p>

<ul>
  <li>
    <p>With Local AD and Azure AD (AAD) Integration</p>
  </li>
  <li>
    <p>With AWS SSO Integration as externel identity provider</p>
  </li>
  <li>
    <p>LDAP – an LDAP directory (389 Directory Server) is embedded</p>
  </li>
  <li>
    <p>Kerberos – KDC (MIT Kerberos) for Kerberos key management and single signon</p>
  </li>
  <li>
    <p>DNS – BIND (bind-dyndb-ldap) for domain name services</p>
  </li>
  <li>
    <p>PKI – Red Hat Certificate System (Dogtag certificate system) provides public key infrastructure for certificate management</p>
  </li>
  <li>
    <p>NTP – Network Time Protocol service for time sync</p>
  </li>
  <li>
    <p>A web-based management front-end running on Apache</p>
  </li>
</ul>

<p><b> A Typical AD User Authentication Flow across AWS and IdM would be: </b></p>

<ul>
  <li>
    <p>When a user attempts to access AWS resources, they first authenticate through Azure AD using their Azure AD credentials.</p>
  </li>
  <li>
    <p>Azure AD authenticates the user and issues a security token.</p>
  </li>
  <li>
    <p>The user accesses AWS SSO, which is configured to use Red Hat IDM as an identity provider.</p>
  </li>
  <li>
    <p>AWS SSO redirects the user to the Red Hat IDM authentication page.</p>
  </li>
  <li>
    <p>The user enters their Red Hat IDM credentials and authenticates against the IdM server.</p>
  </li>
  <li>
    <p>Red Hat IDM validates the user’s credentials and issues a SAML assertion to AWS SSO.</p>
  </li>
  <li>
    <p>AWS SSO validates the SAML assertion and grants the user access to the requested AWS resources.</p>
  </li>
</ul>

<p><b> FreeIPA: the opensource version of RedHat IdM </b></p>

<p>Here I am going to install and configure a local lab IdM portal using the opensource version of RedHat IdM called “freeIPA”, with 3 linux boxes to validate the user permission and client hosts (both CentOS and Ubuntu) enrollment, requirement and design as bellow:</p>

<ul>
  <li>
    <p>freeIPA Server: freeipa-server.zackz.oonline 11.0.1.150 (CentOS 7.9)</p>
  </li>
  <li>
    <p>freeIPA Client1: freeipa-client1.zackz.oonline 11.0.1.151 (CentOS 7.9)</p>
  </li>
  <li>
    <p>freeIPA Client2: freeipa-client2.zackz.oonline 11.0.1.72 (Ubuntu 22.04)</p>
  </li>
</ul>

<p><b> Local testlab FreeIPA server installation </b></p>

<p>On freeIPA Server freeipa-server.zackz.oonline 11.0.1.150 (CentOS 7.9)</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># set hostname with domain</span>
hostnamectl set-hostname freeipa-server.zackz.oonline

<span class="c"># add 3 hosts to /etc/hosts</span>
<span class="nb">echo </span>11.0.1.150 freeipa-server.zackz.oonline  ipa <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.151 freeipa-client1.zackz.oonline ipa <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.72  freeipa-client2.zackz.oonline ipa <span class="o">&gt;&gt;</span> /etc/hosts

<span class="c"># install ipa-server</span>
yum <span class="nb">install </span>ipa-server bind-dyndb-ldap ipa-server-dns

<span class="c"># Configure ipa-server and DNS, here set ipa console and domain admin passwd</span>
ipa-server-install <span class="nt">--setup-dns</span>

<span class="c"># configure firewall rules and services</span>
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-service</span><span class="o">={</span>http,https,ldap,ldaps,kerberos,dns,kpasswd,ntp<span class="o">}</span>
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-service</span> freeipa-ldap
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-service</span> freeipa-ldaps
firewall-cmd <span class="nt">--reload</span>

<span class="c"># check ipastatus</span>
<span class="o">[</span>root@freeipa ~]# ipactl status
Directory Service: RUNNING
krb5kdc Service: RUNNING
kadmin Service: RUNNING
httpd Service: RUNNING
ipa-custodia Service: RUNNING
ntpd Service: RUNNING
pki-tomcatd Service: RUNNING
ipa-otpd Service: RUNNING
ipa: INFO: The ipactl <span class="nb">command </span>was successful


<span class="c"># Obtain a Kerberos ticket for the Kerberos admin user and Verify the ticket</span>
kinit admin
klist

Ticket cache: KEYRING:persistent:0:0
Default principal: admin@ZACKZ.OONLINE

Valid starting     Expires            Service principal
04/04/24 22:17:29  05/04/24 22:02:43  HTTP/freeipa-server.zackz.oonline@ZACKZ.OONLINE

<span class="c"># check content of /etc/resolv.conf</span>
<span class="nb">cat</span> /etc/resolv.conf
search zackz.oonline
nameserver 127.0.0.1

<span class="c"># Configure FreeIPA for User Authentication</span>

yum <span class="nb">install</span> <span class="nt">-y</span> vsftpd
systemctl <span class="nb">enable </span>vsftpd <span class="o">&amp;&amp;</span> systemctl start vsftpd
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-service</span><span class="o">=</span>ftp
firewall-cmd <span class="nt">--reload</span>


<span class="c"># copy CA certificate of the IPA server to the FTP site</span>
<span class="nb">cp</span> /root/cacert.p12 /var/ftp/pub

<span class="c"># Configure default login shell to Bash and Create Users</span>
ipa config-mod <span class="nt">--defaultshell</span><span class="o">=</span>/bin/bash
ipa user-add alice <span class="nt">--first</span><span class="o">=</span>alice <span class="nt">--last</span><span class="o">=</span>abernathy <span class="nt">--password</span>
ipa user-add vince <span class="nt">--first</span><span class="o">=</span>vincent <span class="nt">--last</span><span class="o">=</span>valentine <span class="nt">--password</span>

<span class="c"># add client hosts </span>
ipa host-add <span class="nt">--ip-address</span> 11.0.1.151 freeipa-client1.zackz.oonline
ipa host-add <span class="nt">--ip-address</span> 11.0.1.72 freeipa-client2.zackz.oonline

<span class="c"># create NFS service entry in the IdM domain</span>
ipa service-add nfs/freeipa-client1.zackz.oonline
ipa service-add nfs/freeipa-client2.zackz.oonline

<span class="c"># add entry to the keytab file /etc/krb5.keytab</span>
kadmin.local
Authenticating as principal admin/admin@RHCE.LOCAL with password.
kadmin.local:  ktadd nfs/freeipa-client1.zackz.oonline
kadmin.local:  ktadd nfs/freeipa-client2.zackz.oonline
kadmin.local:  quit

<span class="c"># verify keytab file</span>
<span class="o">[</span>root@freeipa-server pub]# klist <span class="nt">-k</span>
Keytab name: FILE:/etc/krb5.keytab
KVNO Principal
<span class="nt">----</span> <span class="nt">--------------------------------------------------------------------------</span>
   2 host/freeipa-server.zackz.oonline@ZACKZ.OONLINE
   2 host/freeipa-server.zackz.oonline@ZACKZ.OONLINE
   2 host/freeipa-server.zackz.oonline@ZACKZ.OONLINE
   2 host/freeipa-server.zackz.oonline@ZACKZ.OONLINE
   2 host/freeipa-server.zackz.oonline@ZACKZ.OONLINE
   2 host/freeipa-server.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client2.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client2.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client2.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client2.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client2.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client2.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client1.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client1.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client1.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client1.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client1.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client1.zackz.oonline@ZACKZ.OONLINE


<span class="c"># Generate keys to copy over to NFS systems, chmod to Make the keytab file accessible to FTP clients</span>
ipa-getkeytab <span class="nt">-s</span> freeipa-server.zackz.oonline <span class="nt">-p</span> nfs/freeipa-client2.zackz.oonline <span class="nt">-k</span> /var/ftp/pub/freeipa-client2.keytab
ipa-getkeytab <span class="nt">-s</span> freeipa-server.zackz.oonline <span class="nt">-p</span> nfs/freeipa-client1.zackz.oonline <span class="nt">-k</span> /var/ftp/pub/freeipa-client1.keytab
<span class="nb">chmod </span>644 /var/ftp/pub/<span class="k">*</span>.keytab

<span class="c"># Configure DNS</span>
ipa dnszone-mod <span class="nt">--allow-transfer</span><span class="o">=</span>11.0.1.0/24 zackz.oonline
ipa dnsrecord-add zackz.oonline vhost1 <span class="nt">--ttl</span><span class="o">=</span>3600 <span class="nt">--a-ip-address</span><span class="o">=</span>11.0.1.151
ipa dnsrecord-add zackz.oonline dynamic1 <span class="nt">--ttl</span><span class="o">=</span>3600 <span class="nt">--a-ip-address</span><span class="o">=</span>11.0.1.151
ipa dnsrecord-add zackz.oonline @ <span class="nt">--mx-rec</span><span class="o">=</span><span class="s2">"0 freeipa-server.zackz.oonline."</span></code></pre></figure>

<p><b> Console login </b></p>

<p>login freeIPA console with serverIP or DNS name</p>

<p><img src="/assets/linuxidm1.png" alt="image tooltip here" /></p>

<p>list the 2 users “alice” and “vincent” previously created</p>

<p><img src="/assets/linuxidm2.png" alt="image tooltip here" /></p>

<p><b> Add and enroll client hosts into IdM </b></p>

<p>To add client hosts into freeIPA can be done via command or console, but to enroll the host can only be done via each client host by install “freeipa-client” and condigure domain.</p>

<ul>
  <li>For CentOS client (11.0.1.151 freeipa-client1.zackz.oonline) enrollment:</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># set client hostname with domain</span>
hostnamectl set-hostname freeipa-client1.zackz.oonline

<span class="c"># add 3 hosts to /etc/hosts</span>
<span class="nb">echo </span>11.0.1.150 freeipa-server.zackz.oonline  ipa <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.151 freeipa-client1.zackz.oonline ipa <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.72  freeipa-client2.zackz.oonline ipa <span class="o">&gt;&gt;</span> /etc/hosts


<span class="c"># install freeipa-client and manually configure domain</span>
<span class="o">[</span>root@freeipa-client1 ~]# ipa-client-install <span class="nt">--mkhomedir</span>
WARNING: ntpd <span class="nb">time</span>&amp;date synchronization service will not be configured as
conflicting service <span class="o">(</span>chronyd<span class="o">)</span> is enabled
Use <span class="nt">--force-ntpd</span> option to disable it and force configuration of ntpd

DNS discovery failed to determine your DNS domain
Provide the domain name of your IPA server <span class="o">(</span>ex: example.com<span class="o">)</span>: zackz.oonline
Provide your IPA server name <span class="o">(</span>ex: ipa.example.com<span class="o">)</span>: freeipa-server.zackz.oonline
The failure to use DNS to find your IPA server indicates that your resolv.conf file is not properly configured.
Autodiscovery of servers <span class="k">for </span>failover cannot work with this configuration.
If you proceed with the installation, services will be configured to always access the discovered server <span class="k">for </span>all operations and will not fail over to other servers <span class="k">in case</span> of failure.
Proceed with fixed values and no DNS discovery? <span class="o">[</span>no]: <span class="nb">yes
</span>Client <span class="nb">hostname</span>: freeipa-client1.zackz.oonline
Realm: ZACKZ.OONLINE
DNS Domain: zackz.oonline
IPA Server: freeipa-server.zackz.oonline
BaseDN: <span class="nv">dc</span><span class="o">=</span>zackz,dc<span class="o">=</span>oonline

Continue to configure the system with these values? <span class="o">[</span>no]: <span class="nb">yes
</span>Skipping synchronizing <span class="nb">time </span>with NTP server.
User authorized to enroll computers: admin
Password <span class="k">for </span>admin@ZACKZ.OONLINE: 
Successfully retrieved CA cert
    Subject:     <span class="nv">CN</span><span class="o">=</span>Certificate Authority,O<span class="o">=</span>ZACKZ.OONLINE
    Issuer:      <span class="nv">CN</span><span class="o">=</span>Certificate Authority,O<span class="o">=</span>ZACKZ.OONLINE
    Valid From:  2024-04-04 10:45:20
    Valid Until: 2044-04-04 11:45:20

Enrolled <span class="k">in </span>IPA realm ZACKZ.OONLINE
Created /etc/ipa/default.conf
New SSSD config will be created
Configured sudoers <span class="k">in</span> /etc/nsswitch.conf
Configured /etc/sssd/sssd.conf
trying https://freeipa-server.zackz.oonline/ipa/json
<span class="o">[</span>try 1]: Forwarding <span class="s1">'schema'</span> to json server <span class="s1">'https://freeipa-server.zackz.oonline/ipa/json'</span>
trying https://freeipa-server.zackz.oonline/ipa/session/json
<span class="o">[</span>try 1]: Forwarding <span class="s1">'ping'</span> to json server <span class="s1">'https://freeipa-server.zackz.oonline/ipa/session/json'</span>
<span class="o">[</span>try 1]: Forwarding <span class="s1">'ca_is_enabled'</span> to json server <span class="s1">'https://freeipa-server.zackz.oonline/ipa/session/json'</span>
Systemwide CA database updated.
Hostname <span class="o">(</span>freeipa-client1.zackz.oonline<span class="p">)</span> does not have A/AAAA record.
Failed to update DNS records.
Missing reverse record<span class="o">(</span>s<span class="o">)</span> <span class="k">for </span>address<span class="o">(</span>es<span class="o">)</span>: 11.0.1.151.
Adding SSH public key from /etc/ssh/ssh_host_rsa_key.pub
Adding SSH public key from /etc/ssh/ssh_host_ecdsa_key.pub
Adding SSH public key from /etc/ssh/ssh_host_ed25519_key.pub
<span class="o">[</span>try 1]: Forwarding <span class="s1">'host_mod'</span> to json server <span class="s1">'https://freeipa-server.zackz.oonline/ipa/session/json'</span>
Could not update DNS SSHFP records.
SSSD enabled
Configured /etc/openldap/ldap.conf
Unable to find <span class="s1">'admin'</span> user with <span class="s1">'getent passwd admin@zackz.oonline'</span><span class="o">!</span>
Unable to reliably detect configuration. Check NSS setup manually.
Configured /etc/ssh/ssh_config
Configured /etc/ssh/sshd_config
Configuring zackz.oonline as NIS domain.
Configured /etc/krb5.conf <span class="k">for </span>IPA realm ZACKZ.OONLINE
Client configuration complete.
The ipa-client-install <span class="nb">command </span>was successful</code></pre></figure>

<ul>
  <li>for Ubuntu client host (11.0.1.72 freeipa-client2.zackz.oonline) enrollment:</li>
</ul>

<p>First back to freeIPA server, add DNS record for Ubuntu</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># add DNS record for Ubuntu client on freeIPA server</span>
ipa dnsrecord-add hwdomain.io ubuntu-node.hwdomain.io <span class="nt">--a-rec</span> 192.168.10.50</code></pre></figure>

<p>Then login Ubuntu host to install freeipa-client and enroll:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># set hostname </span>
hostnamectl set-hostname freeipa-client2.zackz.oonline

<span class="c"># add 3 hosts to /etc/hosts</span>
<span class="nb">echo </span>11.0.1.150 freeipa-server.zackz.oonline  ipa <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.151 freeipa-client1.zackz.oonline ipa <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.72  freeipa-client2.zackz.oonline ipa <span class="o">&gt;&gt;</span> /etc/hosts

<span class="c"># install ipa client</span>
apt update <span class="o">&amp;&amp;</span> apt <span class="nb">install </span>freeipa-client oddjob-mkhomedir

<span class="c"># add ubuntu client to freeIPA server</span>
ipa-client-install <span class="nt">--hostname</span><span class="o">=</span><span class="sb">`</span><span class="nb">hostname</span> <span class="nt">-f</span><span class="sb">`</span> <span class="nt">--mkhomedir</span> /
<span class="nt">--server</span><span class="o">=</span>freeipa-server.zackz.oonline /
<span class="nt">--domain</span> zackz.online /
<span class="nt">--realm</span> ZACKZ.OONLINE

<span class="c"># change PAM profile to enable "Create a home directory on login" </span>
pam-auth-update</code></pre></figure>

<p>now back to console, the 2 client hosts had been added and enrolled into freeIPA server
<img src="/assets/linuxidm3.png" alt="image tooltip here" /></p>

<ul>
  <li>verify to use user “alice” and its passwd to ssh login to ubuntu client from freeIPA server</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@freeipa-server ~]# ssh alice@freeipa-client2.zackz.oonline
Password: 
Welcome to Ubuntu 22.04.4 LTS <span class="o">(</span>GNU/Linux 5.15.0-101-generic x86_64<span class="o">)</span>

 <span class="k">*</span> Documentation:  https://help.ubuntu.com
 <span class="k">*</span> Management:     https://landscape.canonical.com
 <span class="k">*</span> Support:        https://ubuntu.com/pro

  System information as of Thu Apr  4 01:02:00 PM UTC 2024

  System load:  0.0               Processes:              234
  Usage of /:   8.0% of 93.93GB   Users logged <span class="k">in</span>:        1
  Memory usage: 11%               IPv4 address <span class="k">for </span>ens33: 11.0.1.72
  Swap usage:   0%

 <span class="k">*</span> Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar <span class="k">for </span>easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

Expanded Security Maintenance <span class="k">for </span>Applications is not enabled.

18 updates can be applied immediately.
To see these additional updates run: apt list <span class="nt">--upgradable</span>

Enable ESM Apps to receive additional future security updates.
See https://ubuntu.com/esm or run: <span class="nb">sudo </span>pro status


Last login: Thu Apr  4 11:47:37 2024 from 11.0.1.150
alice@freeipa-client2:~<span class="nv">$ </span><span class="nb">id
</span><span class="nv">uid</span><span class="o">=</span>1239800001<span class="o">(</span>alice<span class="o">)</span> <span class="nv">gid</span><span class="o">=</span>1239800001<span class="o">(</span>alice<span class="o">)</span> <span class="nb">groups</span><span class="o">=</span>1239800001<span class="o">(</span>alice<span class="o">)</span>
alice@freeipa-client2:~<span class="nv">$ </span></code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we install Redhat IdM server and be able to enroll client hosts, looking at the user details, IdM using Kerberos for authentication, together with user group, policy, HBAC and Sudo roles, provides a flexible and robust authentication framework that supports multiple authentication mechanisms, enabling organizations to authenticate users securely across their Linux and Unix environments.</p>

<p><img src="/assets/linuxidm5.png" alt="image tooltip here" /></p>

<p>More info can be found via <a href="https://freeipa.readthedocs.io/en/latest/workshop.html">Freeipa workshop</a>, <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_idm_users_groups_hosts_and_access_control_rules/index">Red Hat product documentation</a>, <a href="https://chamathb.wordpress.com/2019/06/21/setting-up-rhel-idm-with-integrated-dns-on-aws/">Redhat Idm on AWS with DNS forwarder</a>, <a href="https://www.reddit.com/r/redhat/comments/6ixtoe/idmfreeipa_dns_forwarding/">idmfreeipa DNS forwarder configurations on AWS</a>, and <a href="https://redhat.com/en/blog/automating-red-hat-identity-management-installation">Automating Red Hat Identity Management installation with Ansible</a>.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Linux Identity management]]></summary></entry><entry><title type="html">Move To Rancher</title><link href="http://localhost:4000/jekyll/cat2/2024/03/26/Rancher.html" rel="alternate" type="text/html" title="Move To Rancher" /><published>2024-03-26T11:15:29+11:00</published><updated>2024-03-26T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/03/26/Rancher</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/03/26/Rancher.html"><![CDATA[<p><b> My k8s provisioning journey </b></p>

<p>So far many different ways I have used to deploy k8s cluster, each with its own pros and cons.</p>

<ul>
  <li>
    <p>home lab build k8s components (etcd, keepalived, apiserver, scheduler, coreDNS, calico)</p>
  </li>
  <li>
    <p>home lab k8s cluster with kubeadm</p>
  </li>
  <li>
    <p>k8s on AWS using kops and eksctl</p>
  </li>
  <li>
    <p>AWS self-managed k8s cluster directly on ec2 by ansible</p>
  </li>
</ul>

<p><b>Rancher Support matrix</b></p>

<p><img src="/assets/rancher1.png" alt="image tooltip here" /></p>

<p><b> Local docker Installation</b></p>

<p>To enable Rancher on homelab env, we need a Linux box to run Rancher as docker.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Create Persisting rancher data directory to map within the Rancher Docker container</span>
ubuntu@ubt-server:/<span class="nv">$ </span><span class="nb">mkdir</span> <span class="nt">-p</span> /path/to/rancher-data

ubuntu@ubt-server:/<span class="nv">$ </span><span class="nb">sudo </span>docker run <span class="nt">-d</span> <span class="nt">--restart</span><span class="o">=</span>unless-stopped  <span class="se">\</span>
 <span class="nt">-p</span> 80:80 <span class="nt">-p</span> 443:443  <span class="se">\</span>
 <span class="nt">-v</span> /path/to/rancher-data:/var/lib/rancher <span class="se">\</span>
 <span class="nt">--privileged</span>   rancher/rancher:latest
d26e32094657b598f61233d0d86e448ab4bfd980763928ca6f298ae0d3774a56

ubuntu@ubt-server:/<span class="nv">$ </span><span class="nb">sudo </span>docker ps
CONTAINER ID   IMAGE                    COMMAND           CREATED         STATUS         PORTS                                                                      NAMES
d26e32094657   rancher/rancher:latest   <span class="s2">"entrypoint.sh"</span>   7 seconds ago   Up 6 seconds   0.0.0.0:80-&gt;80/tcp, :::80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, :::443-&gt;443/tcp   flamboyant_bassi

ubuntu@ubt-server:/<span class="nv">$ </span><span class="nb">sudo </span>docker logs  d26e32094657  2&gt;&amp;1 | <span class="nb">grep</span> <span class="s2">"Bootstrap Password:"</span>
2024/03/29 04:07:34 <span class="o">[</span>INFO] Bootstrap Password: zn7nd25rfmkm7kztkfmnk8m84gtlw76gd96sgxz8j2rdm6pnkpqgt9</code></pre></figure>

<p><b> Rancher Web Portal login</b></p>

<p>via https://localhost/dashboard/home</p>

<p><img src="/assets/rancher2.png" alt="image tooltip here" /></p>

<p><b> Import existing k8s culster vs create new k8s from rancher console</b></p>

<ul>
  <li>Under “cluster management”, it supports importing k8s from cloud providers to local k8s, unfutinately I previous k8s cluster is v1.28 which is too high to be imported and managed by this rancher.</li>
</ul>

<p><img src="/assets/rancher3.png" alt="image tooltip here" /></p>

<ul>
  <li>Hence I will use Rancher to create a new one here First prepare 3 local Linux VM boxes, come back to Rancher console under cluster management, give name of the new cluster, then run command to initiate control plane.</li>
</ul>

<p><img src="/assets/rancher4.png" alt="image tooltip here" /></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">ubuntu@rancher-master01:~<span class="nv">$ </span>curl <span class="nt">--insecure</span> <span class="nt">-fL</span> https://11.0.1.220/system-agent-install.sh | <span class="nb">sudo  </span>sh <span class="nt">-s</span> - <span class="nt">--server</span> https://11.0.1.220 <span class="nt">--label</span> <span class="s1">'cattle.io/os=linux'</span> <span class="nt">--token</span> kx92bf7gxdfx2nfnl6rvw4hlmcwdxcb2rt442vgsvgb7tz29rmd4c6 <span class="nt">--ca-checksum</span> 31478d0c1db90313258de7fa258cc60de1a3e67dfb2b285cb682463644474780 <span class="nt">--etcd</span> <span class="nt">--controlplane</span>
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 30845    0 30845    0     0  2037k      0 <span class="nt">--</span>:--:-- <span class="nt">--</span>:--:-- <span class="nt">--</span>:--:-- 2151k
<span class="o">[</span>INFO]  Label: cattle.io/os<span class="o">=</span>linux
<span class="o">[</span>INFO]  Role requested: etcd
<span class="o">[</span>INFO]  Role requested: controlplane
<span class="o">[</span>INFO]  Using default agent configuration directory /etc/rancher/agent
<span class="o">[</span>INFO]  Using default agent var directory /var/lib/rancher/agent
<span class="o">[</span>INFO]  Determined CA is necessary to connect to Rancher
<span class="o">[</span>INFO]  Successfully downloaded CA certificate
<span class="o">[</span>INFO]  Value from https://11.0.1.220/cacerts is an x509 certificate
<span class="o">[</span>INFO]  Successfully tested Rancher connection
<span class="o">[</span>INFO]  Downloading rancher-system-agent binary from https://11.0.1.220/assets/rancher-system-agent-amd64
<span class="o">[</span>INFO]  Successfully downloaded the rancher-system-agent binary.
<span class="o">[</span>INFO]  Downloading rancher-system-agent-uninstall.sh script from https://11.0.1.220/assets/system-agent-uninstall.sh
<span class="o">[</span>INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.
<span class="o">[</span>INFO]  Generating Cattle ID
<span class="o">[</span>INFO]  Successfully downloaded Rancher connection information
<span class="o">[</span>INFO]  systemd: Creating service file
<span class="o">[</span>INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env
<span class="o">[</span>INFO]  Enabling rancher-system-agent.service
Created symlink /etc/systemd/system/multi-user.target.wants/rancher-system-agent.service → /etc/systemd/system/rancher-system-agent.service.
<span class="o">[</span>INFO]  Starting/restarting rancher-system-agent.service</code></pre></figure>

<ul>
  <li>Updating new machine as a K8S rancher node as control plane.</li>
</ul>

<p><img src="/assets/rancher5.png" alt="image tooltip here" /></p>

<ul>
  <li>Then join the 2 worker nodes</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">ubuntu@racher-worker01:~<span class="nv">$ </span>curl <span class="nt">--insecure</span> <span class="nt">-fL</span> https://11.0.1.220/system-agent-install.sh | <span class="nb">sudo  </span>sh <span class="nt">-s</span> - <span class="nt">--server</span> https://11.0.1.220 <span class="nt">--label</span> <span class="s1">'cattle.io/os=linux'</span> <span class="nt">--token</span> hdsvptc74zvzz62hw9gtt6p7m6nl5k4fs6vk92zqm4f6tvj4tf8m54 <span class="nt">--ca-checksum</span> 31478d0c1db90313258de7fa258cc60de1a3e67dfb2b285cb682463644474780 <span class="nt">--worker</span>
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 30845    0 30845    0     0  5455k      0 <span class="nt">--</span>:--:-- <span class="nt">--</span>:--:-- <span class="nt">--</span>:--:-- 6024k
<span class="o">[</span>INFO]  Label: cattle.io/os<span class="o">=</span>linux
<span class="o">[</span>INFO]  Role requested: worker
<span class="o">[</span>INFO]  Using default agent configuration directory /etc/rancher/agent
<span class="o">[</span>INFO]  Using default agent var directory /var/lib/rancher/agent
<span class="o">[</span>INFO]  Determined CA is necessary to connect to Rancher
<span class="o">[</span>INFO]  Successfully downloaded CA certificate
<span class="o">[</span>INFO]  Value from https://11.0.1.220/cacerts is an x509 certificate
<span class="o">[</span>INFO]  Successfully tested Rancher connection
<span class="o">[</span>INFO]  Downloading rancher-system-agent binary from https://11.0.1.220/assets/rancher-system-agent-amd64
<span class="o">[</span>INFO]  Successfully downloaded the rancher-system-agent binary.
<span class="o">[</span>INFO]  Downloading rancher-system-agent-uninstall.sh script from https://11.0.1.220/assets/system-agent-uninstall.sh
<span class="o">[</span>INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.
<span class="o">[</span>INFO]  Generating Cattle ID
<span class="o">[</span>INFO]  Successfully downloaded Rancher connection information
<span class="o">[</span>INFO]  systemd: Creating service file
<span class="o">[</span>INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env
<span class="o">[</span>INFO]  Enabling rancher-system-agent.service
Created symlink /etc/systemd/system/multi-user.target.wants/rancher-system-agent.service → /etc/systemd/system/rancher-system-agent.service.
<span class="o">[</span>INFO]  Starting/restarting rancher-system-agent.service</code></pre></figure>

<p><img src="/assets/rancher6.png" alt="image tooltip here" /></p>

<ul>
  <li>Create zackweb and joesite as deployment from Rancher console</li>
</ul>

<p><img src="/assets/rancher7.png" alt="image tooltip here" />
<img src="/assets/rancher8.png" alt="image tooltip here" /></p>

<p><b> Conclusion</b></p>

<p>now we can use Rancher to deploy a local k8s cluster based on 3 Linux machines without any trouble just a few commands. then we will be able to create deployment and service in rancher console instead of “kubectl” all the time, it also provides app market for most popular helm charts ready to be installed just by one click like Istio and Prometheus. Only downside is, Rancher itself requires resources to run which may impact the performance and resources on each node, also it brings complexity in upgrade for both Rancher and k8s. overall I love the concept and tools that Rancher provides to manage k8s cluster. I will explore more in the next blog.</p>

<p><img src="/assets/rancher9.png" alt="image tooltip here" /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[My k8s provisioning journey]]></summary></entry><entry><title type="html">K8S with External Cloud Controller Manager</title><link href="http://localhost:4000/jekyll/cat2/2024/02/26/External-cloudprovider.html" rel="alternate" type="text/html" title="K8S with External Cloud Controller Manager" /><published>2024-02-26T11:15:29+11:00</published><updated>2024-02-26T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/02/26/External-cloudprovider</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/02/26/External-cloudprovider.html"><![CDATA[<p><b> Scenario and Challange </b></p>

<p>In last post I was able to automate and create a self-owned K8S cluster on AWS EC2 instances by using Ansible and Terraform.</p>

<p>In this Scenario when deploying k8s pods and services in this K8S cluster, it will not create AWS loadbalancer even mentioned type = LoadBalancer, but this can be eaily achieved when in a AWS managed EKS cluster.</p>

<p>By searching online, Kubernetes actually provides such solution called <b>“cloud-provider-aws”</b>, which provides interface between a self-owned AWS Kubernetes cluster and AWS APIs. This allows EC2 instances running Kubernetes node to be able to provision AWS NLB or ELB resources during service deployment by mentioning “LoadBalancer”..</p>

<p><b></b></p>

<p>To enable the Kubernetes External Cloud Controller Manager, a <b>AWS Cloud Controller manager</b> need to be deployed into cluster, by doing so, it will create and a AWS loadbalancers (NLB), then self-owned K8S cluster can expose services externally by creating ELB.</p>

<p><b> Steps</b></p>

<p>There are a few steps need to be done, docs can be followed by bellow link:</p>

<p>https://github.com/kubernetes/cloud-provider-aws/blob/master/docs/getting_started.md</p>

<ul>
  <li>Change EC2 hostname from ip to FQDN</li>
</ul>

<p>To be able to communicate with AWS API, the k8s node should be changed to FQDN rather than default EC2 IP address</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">curl http://169.254.169.254/latest/meta-data/local-hostname
hostnamectl set-hostname</code></pre></figure>

<ul>
  <li>Create and assign IAM roles to k8s nodes</li>
</ul>

<p>IAM role needed for EC2 running K8S nodes to have proper permission to interact with AWS APIs and create and maintain AWS service</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Roles for master and worker nodes can be found bellow link</span>
https://github.com/kubernetes/cloud-provider-aws/blob/master/docs/prerequisites.md </code></pre></figure>

<ul>
  <li>Tag ec2 instances as owned</li>
</ul>

<p>The K8S nodes need to be tagged as owned</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">tag kubernetes.io/cluster/your_cluster_id<span class="o">=</span>owned</code></pre></figure>

<ul>
  <li>AWS Cloud Controller manager need to be deployed into K8S, this will create a NetworkLoadbalancer in AWS</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl apply <span class="nt">-k</span> <span class="s1">'github.com/kubernetes/cloud-provider-aws/examples/existing-cluster/base/?ref=master</span></code></pre></figure>

<ul>
  <li>Add the –cloud-provider=external to the kube-controller-manager config, kube apiserver config and kubelet’s config</li>
</ul>

<p><b> Conclusion</b></p>

<p>now when create a k8s deployment and service, it will automatically create AWS loadbalancer to route traffic from external into K8S internal pods</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Scenario and Challange]]></summary></entry><entry><title type="html">Automate K8S with Terraform and Ansible</title><link href="http://localhost:4000/jekyll/cat2/2024/02/24/EC2-K8S.html" rel="alternate" type="text/html" title="Automate K8S with Terraform and Ansible" /><published>2024-02-24T11:15:29+11:00</published><updated>2024-02-24T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/02/24/EC2-K8S</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/02/24/EC2-K8S.html"><![CDATA[<p><b> K8S on EC2  vs EKS </b></p>

<p>While I was in an interview last week, the company heavily using Ansible to provision and configure their k8s cluster on AWS EC2, aim to have the most control and flexibility in case they need to move to another cloud provider.</p>

<p>Here I will use both Terraform and Ansible to automate K8S cluster creation and configuration on a few EC2 instances.</p>

<p><b> The tools and workflow</b></p>

<ul>
  <li>Major tools will include Ansible, terraform, together with a shell script.</li>
</ul>

<ol>
  <li>
    <p>Use terraform to create S3 as backend state file, create security groups and 4 ec2 instances ( 1 bastion, 1 master, 2 workers)</p>
  </li>
  <li>
    <p>As Instances are created in AWS environment dynamically, Ansible ec2-dynamic-inventory approach is best way to manage them.</p>
  </li>
  <li>
    <p>Using terraform “connection”, to ssh to the bastion host after creation, use provisoners “file” to upload shell script to bootstrap bastion host as a ansbile master, configure ec2-dynamic-inventory to fetch the other 3 instances for K8S.</p>
  </li>
  <li>
    <p>Continue with terraform “file” and “remote-exec” to upload playbooks and with “inline” to execute playbooks to init k8s master node and join the worker node.</p>
  </li>
  <li>
    <p>Finally another playbook to configure bastion to run kubectl</p>
  </li>
</ol>

<p><b> Create SGs and EC2s, with Bastion bootstrap via terraform </b></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># main.tf</span>

resource <span class="s2">"aws_instance"</span> <span class="s2">"bolg"</span> <span class="o">{</span>
  ami             <span class="o">=</span> var.ami_id <span class="c"># Replace with your AMI ID</span>
  instance_type   <span class="o">=</span> var.instance_type_free
  security_groups <span class="o">=</span> <span class="o">[</span>aws_security_group.blog_sg.name]
  <span class="c">#  vpc_security_group_ids = ["sg-089842a753c9309bb"]</span>
  key_name <span class="o">=</span> var.key_pair
  user_data <span class="o">=</span> <span class="o">&lt;&lt;-</span><span class="no">EOF</span><span class="sh">
    #!/bin/bash
    sudo apt update
    sudo apt install software-properties-common
    sudo add-apt-repository --yes --update ppa:ansible/ansible
    sudo apt install ansible -y
</span><span class="no">  EOF
</span>  tags <span class="o">=</span> <span class="o">{</span>
    Name <span class="o">=</span> <span class="s2">"blog"</span>
  <span class="o">}</span>
  lifecycle <span class="o">{</span>
    prevent_destroy <span class="o">=</span> <span class="nb">true</span>
  <span class="o">}</span>
<span class="o">}</span>
resource <span class="s2">"null_resource"</span> <span class="s2">"upload_playbook"</span> <span class="o">{</span>
  triggers <span class="o">=</span> <span class="o">{</span>
    <span class="c"># Add a dummy trigger to force a refresh</span>
    timestamp <span class="o">=</span> <span class="s2">"</span><span class="k">${</span><span class="nv">timestamp</span><span class="p">()</span><span class="k">}</span><span class="s2">"</span>
  <span class="o">}</span>
  depends_on <span class="o">=</span> <span class="o">[</span>null_resource.wait_for_bastion]
  connection <span class="o">{</span>
    <span class="nb">type</span>        <span class="o">=</span> <span class="s2">"ssh"</span>
    user        <span class="o">=</span> <span class="s2">"ubuntu"</span>
    private_key <span class="o">=</span> file<span class="o">(</span><span class="s2">"terraform-new-key1.pem"</span><span class="o">)</span>
    host        <span class="o">=</span> aws_instance.testbastion.public_ip
  <span class="o">}</span>
  provisioner <span class="s2">"file"</span> <span class="o">{</span>
    <span class="nb">source</span>      <span class="o">=</span> <span class="s2">"pb1.yaml"</span>
    destination <span class="o">=</span> <span class="s2">"/home/ubuntu/pb1.yaml"</span>
  <span class="o">}</span>
  provisioner <span class="s2">"file"</span> <span class="o">{</span>
    <span class="nb">source</span>      <span class="o">=</span> <span class="s2">"aws_ec2.yaml"</span>
    destination <span class="o">=</span> <span class="s2">"/home/ubuntu/aws_ec2.yaml"</span>
  <span class="o">}</span>
  provisioner <span class="s2">"file"</span> <span class="o">{</span>
    <span class="nb">source</span>      <span class="o">=</span> <span class="s2">"ansible.sh"</span>
    destination <span class="o">=</span> <span class="s2">"/home/ubuntu/ansible.sh"</span>
  <span class="o">}</span>
  provisioner <span class="s2">"remote-exec"</span> <span class="o">{</span>
    inline <span class="o">=</span> <span class="o">[</span>
      <span class="s2">"cd /home/ubuntu/"</span>,
      <span class="s2">"sudo chmod 600 terraform-new-key1.pem"</span>,
      <span class="s2">"sudo chmod +x pb1.yaml"</span>,
      <span class="s2">"sudo ansible-playbook pb1.yaml"</span>,
      <span class="s2">"sudo chmod +x ansible.sh"</span>,
      <span class="s2">"sudo ./ansible.sh"</span>,
      <span class="s2">"sudo ansible-inventory -i aws_ec2.yaml --list"</span>,
      <span class="s2">"sudo ansible-inventory --graph"</span>
    <span class="o">]</span>
  <span class="o">}</span>
<span class="o">}</span>
 </code></pre></figure>

<p><b> SSH to bastion host and run playbooks to init and join k8s nodes </b></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># k8s-deploy.yaml</span>

<span class="c"># Playbook hosts can be defined by EC2 tags</span>
<span class="nt">---</span>
- name: EC2-K8S cluster setup
  hosts: all
  gather_facts: <span class="nb">false
  </span>tasks:
    - name: <span class="nb">test </span>EC2 dynamic hosts connections
      include_playbook: 2-test-connection.yaml

    - name: Pre-task <span class="k">for </span>all k8s hosts
      include_playbook: 3-k8s-nodes-preparation.yaml

    - name: init Master node
      include_playbook: 4-master-init.yaml

    - name: Join worker nodes
      include_playbook: 5-work-join.yaml

    - name: configure bastion to run kubectl
      include_playbook: 8-local-kubeconf-admin.yaml</code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we are able to spin up a 3-nodes K8S cluster on EC2 instances in about 20 minutes.</p>

<p>For lab purpose, running K8S on EC2 instances with default VPC can be cheaper and flexible option compare to EKS, as AWS managed EKS require additional VPC and unable to change node instance type unless delete the existing node group.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[K8S on EC2 vs EKS]]></summary></entry><entry><title type="html">Ansible Advance - Roles</title><link href="http://localhost:4000/jekyll/cat2/2024/02/21/Ansible-role.html" rel="alternate" type="text/html" title="Ansible Advance - Roles" /><published>2024-02-21T11:15:29+11:00</published><updated>2024-02-21T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/02/21/Ansible-role</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/02/21/Ansible-role.html"><![CDATA[<p><b>About Ansible Roles </b></p>

<p>Here we play with Ansible Roles, as Ansible is powerful configuration automation tool by running playpook, in addition organizing multiple Ansible contents and playbooks into roles provides a structural and more manageable way to achieve complex tasks for multiple targets or groups.</p>

<p>===================================================================</p>

<p>Ansible Roles can be created by:</p>

<ul>
  <li>Method1: Create Ansible roles by ansible-galaxy</li>
</ul>

<p>Quickly creating a well-defined role directory structure skeleton, we can leverage the command ansible-galaxy init <role_name></role_name></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@localhost ansible]# ansible-galaxy init zack-role
 Role zack-role was created successfully
<span class="o">[</span>root@localhost ansible]# tree zack-role
zack-role
├── defaults
│   └── main.yml
├── files
├── handlers
│   └── main.yml
├── meta
│   └── main.yml
├── README.md
├── tasks
│   └── main.yml
├── templates
├── tests
│   ├── inventory
│   └── test.yml
└── vars
    └── main.yml</code></pre></figure>

<p><b>defaults</b>: Contain default variables for the role.</p>

<p><b>files</b>: Contain static files that you want to copy to the target hosts.</p>

<p><b>handlers</b>: Contain handlers, which are tasks triggered by other tasks.</p>

<p><b>meta</b>: Contain metadata for the role, such as dependencies.</p>

<p><b>tasks</b>: Contains a main.yaml file, which includes tasks specific to the “server1” role.</p>

<p><b>templates</b>: Contain Jinja2 templates.</p>

<p><b>vars</b>: Contain variables specific to the “server1” role</p>

<p>===================================================================</p>

<ul>
  <li>Method2: Manually create roles (server1 &amp; server2) by create on-demand folder structure, Setting up groups in ansible inventory file：</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">vim /etc/ansible/hosts
<span class="c"># add group server1 as web server, server2 as backend</span>
<span class="o">[</span>defaults]
11.0.1.132
11.0.1.131

<span class="o">[</span>server1] <span class="c"># web</span>
11.0.1.131

<span class="o">[</span>server2]  <span class="c"># backend</span>
11.0.1.132

<span class="c"># create own roles structure server1 (web) and server2 (backend)</span>
<span class="c"># create zack.html under role server1/files</span>

<span class="o">[</span>root@localhost ansible]# tree roles
roles
├── base
│   └── tasks
│       └── main.yaml
├── server1
│   ├── defaults
│   ├── files
│   │   └── zack.html
│   ├── handlers
│   ├── meta
│   ├── tasks
│   │   └── main.yaml
│   ├── templates
│   └── vars
└── server2
    ├── defaults
    ├── files
    ├── handlers
    ├── meta
    ├── tasks
    │   └── main.yaml
    ├── templates
    └── vars</code></pre></figure>

<ul>
  <li>define tasks for role server1 and role server2</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">cat</span> /etc/ansible/roles/server1/tasks/main.yaml
<span class="c"># web server tasks include: install httpd, enable service, open 80 port, replace index.html, restart httpd service</span>
- name: Install HTTPD package
  yum:
    name: httpd
    state: present

- name: Enable HTTPD service
  service:
    name: httpd
    enabled: <span class="nb">yes
    </span>state: started

- name: Open port 80 <span class="k">in </span>firewall
  firewalld:
    service: http
    permanent: <span class="nb">yes
    </span>state: enabled

- name: replace index.html
  copy:
    src: zack.html
    dest: /var/www/html/index.html
    owner: root
    group: root
    mode: 0644
  register: httpd_updated

- name: restart httpd service
  service:
    name: httpd
    state: restarted
  when: httpd_updated.changed


<span class="nb">cat</span> /etc/ansible/roles/server2/tasks/main.yaml

<span class="c"># backend server2 tasks include install epel repo, install iftop and lrzsz</span>

- name: Install epel-release
  yum:
    name:
      - epel-release
    state: latest
- name: Check Yum Repository List
  yum:
    list: repo

- name: Install usage packages
  yum:
    name:
      - iftop
      - lrzsz
    state: latest</code></pre></figure>

<ul>
  <li>define role main playbook zack-role.yaml</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># update repo for all hosts, for server1 and server 2, execute each role server1 and server2</span>

<span class="nt">---</span>
- become: <span class="nb">true
  </span>hosts: all
  pre_tasks:
  - name: update repository index
    tags: always
    yum:
      update_cache: <span class="nb">yes
    </span>changed_when: <span class="nb">false</span>

- import_playbook: /etc/ansible/add-more-user.yaml

- hosts: server1
  become: <span class="nb">true
  </span>roles:
    - server1


- hosts: server2
  become: <span class="nb">true
  </span>roles:
    - server2</code></pre></figure>

<ul>
  <li>run playbook for zack-role.yaml</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@localhost ansible]# ansible-playbook zack-role.yaml

PLAY <span class="o">[</span>all] <span class="k">**************************************************************************************************************************</span>

TASK <span class="o">[</span>Gathering Facts] <span class="k">**************************************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.132]
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>update repository index] <span class="k">******************************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]
ok: <span class="o">[</span>11.0.1.132]


PLAY <span class="o">[</span>server1] <span class="k">**********************************************************************************************************************</span>

TASK <span class="o">[</span>Gathering Facts] <span class="k">**************************************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>server1 : Install HTTPD package] <span class="k">**********************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>server1 : Enable HTTPD service] <span class="k">***********************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>server1 : Open port 80 <span class="k">in </span>firewall] <span class="k">*******************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>server1 : replace index.html] <span class="k">*************************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>server1 : restart httpd service] <span class="k">**********************************************************************************************</span>
skipping: <span class="o">[</span>11.0.1.131]

PLAY <span class="o">[</span>server2] <span class="k">**********************************************************************************************************************</span>

TASK <span class="o">[</span>Gathering Facts] <span class="k">**************************************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.132]

TASK <span class="o">[</span>server2 : Install epel-release] <span class="k">***********************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.132]

TASK <span class="o">[</span>server2 : Check Yum Repository List] <span class="k">******************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.132]

TASK <span class="o">[</span>server2 : Install usage packages] <span class="k">*********************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.132]

PLAY RECAP <span class="k">**************************************************************************************************************************</span>
11.0.1.131                 : <span class="nv">ok</span><span class="o">=</span>11   <span class="nv">changed</span><span class="o">=</span>0    <span class="nv">unreachable</span><span class="o">=</span>0    <span class="nv">failed</span><span class="o">=</span>0    <span class="nv">skipped</span><span class="o">=</span>1    <span class="nv">rescued</span><span class="o">=</span>0    <span class="nv">ignored</span><span class="o">=</span>0   
11.0.1.132                 : <span class="nv">ok</span><span class="o">=</span>10   <span class="nv">changed</span><span class="o">=</span>0    <span class="nv">unreachable</span><span class="o">=</span>0    <span class="nv">failed</span><span class="o">=</span>0    <span class="nv">skipped</span><span class="o">=</span>0    <span class="nv">rescued</span><span class="o">=</span>0    <span class="nv">ignored</span><span class="o">=</span>0   </code></pre></figure>

<ul>
  <li>Bingo! validate httpd on server1, and iftop &amp; lrzsz installed on server2 by leveraging with ansible roles</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@localhost ansible]# curl 11.0.1.131
This is zack <span class="nb">test </span>Ansible role web <span class="o">!!!!!!!!!!!</span>   
<span class="o">!!!!!!!!!</span></code></pre></figure>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Ansible Roles]]></summary></entry></feed>