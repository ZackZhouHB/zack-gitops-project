<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-03-29T16:54:25+11:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Zack’s Blog</title><subtitle>## AWS   ## Jenkins  ## Microservices ## Automation ## K8S   ## CICD     ## Gitops  [京ICP备2024056683号-1]</subtitle><entry><title type="html">Move to Rancher</title><link href="http://localhost:4000/jekyll/cat2/2024/03/29/Rancher.html" rel="alternate" type="text/html" title="Move to Rancher" /><published>2024-03-29T11:15:29+11:00</published><updated>2024-03-29T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/03/29/Rancher</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/03/29/Rancher.html"><![CDATA[<p><b> My k8s provisioning journey </b></p>

<p>So far many different ways I have used to deploy k8s cluster, each with its own pros and cons.</p>

<ul>
  <li>
    <p>home lab install individual k8s components (etcd, keepalived, apiserver, scheduler, coreDNS, calico)</p>
  </li>
  <li>
    <p>home lab using kubeadm</p>
  </li>
  <li>
    <p>AWS cloud provisioning using kops and eksctl</p>
  </li>
  <li>
    <p>provision self-managed k8s cluster directly on ec2 by ansible</p>
  </li>
</ul>

<p><b> Rancher Support matrix </b></p>

<p>Finally it is time to move to Rancher as its power and simplicity in managing multiple k8s cluster. Based on latest version 2.8.2, it supports to manage k8s v1.27.</p>

<p><img src="/assets/rancher1.png" alt="image tooltip here" /></p>

<p><b> Rancher local installation </b></p>

<p>To enable Rancher on homelab env, we need a Linux box to run Rancher as docker.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Create Persisting rancher data directory to map within the Rancher Docker container</span>
ubuntu@ubt-server:/<span class="nv">$ </span><span class="nb">mkdir</span> <span class="nt">-p</span> /path/to/rancher-data

ubuntu@ubt-server:/<span class="nv">$ </span><span class="nb">sudo </span>docker run <span class="nt">-d</span> <span class="nt">--restart</span><span class="o">=</span>unless-stopped  <span class="se">\</span>
 <span class="nt">-p</span> 80:80 <span class="nt">-p</span> 443:443  <span class="se">\</span>
 <span class="nt">-v</span> /path/to/rancher-data:/var/lib/rancher <span class="se">\</span>
 <span class="nt">--privileged</span>   rancher/rancher:latest
d26e32094657b598f61233d0d86e448ab4bfd980763928ca6f298ae0d3774a56

ubuntu@ubt-server:/<span class="nv">$ </span><span class="nb">sudo </span>docker ps
CONTAINER ID   IMAGE                    COMMAND           CREATED         STATUS         PORTS                                                                      NAMES
d26e32094657   rancher/rancher:latest   <span class="s2">"entrypoint.sh"</span>   7 seconds ago   Up 6 seconds   0.0.0.0:80-&gt;80/tcp, :::80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, :::443-&gt;443/tcp   flamboyant_bassi

ubuntu@ubt-server:/<span class="nv">$ </span><span class="nb">sudo </span>docker logs  d26e32094657  2&gt;&amp;1 | <span class="nb">grep</span> <span class="s2">"Bootstrap Password:"</span>
2024/03/29 04:07:34 <span class="o">[</span>INFO] Bootstrap Password: zn7nd25rfmkm7kztkfmnk8m84gtlw76gd96sgxz8j2rdm6pnkpqgt9</code></pre></figure>

<p><b> Console login </b></p>

<p>Rancher login page via https://localhost/dashboard/home</p>

<p><img src="/assets/rancher2.png" alt="image tooltip here" /></p>

<p><b> K8s cluster import and create on Rancher console </b></p>

<ul>
  <li>under cluster management, it supports importing k8s from cloud providers to local k8s, unfutinately I previous k8s cluster is v1.28 which is too high to be imported and managed by this rancher.</li>
</ul>

<p><img src="/assets/rancher3.png" alt="image tooltip here" /></p>

<ul>
  <li>Hence I will use Rancher to create a new one here
First prepare 3 local Linux VM boxes, come back to Rancher console under cluster management, give name of the new cluster, then run command to initiate control plane.</li>
</ul>

<p><img src="/assets/rancher4.png" alt="image tooltip here" /></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">ubuntu@rancher-master01:~<span class="nv">$ </span>curl <span class="nt">--insecure</span> <span class="nt">-fL</span> https://11.0.1.220/system-agent-install.sh | <span class="nb">sudo  </span>sh <span class="nt">-s</span> - <span class="nt">--server</span> https://11.0.1.220 <span class="nt">--label</span> <span class="s1">'cattle.io/os=linux'</span> <span class="nt">--token</span> kx92bf7gxdfx2nfnl6rvw4hlmcwdxcb2rt442vgsvgb7tz29rmd4c6 <span class="nt">--ca-checksum</span> 31478d0c1db90313258de7fa258cc60de1a3e67dfb2b285cb682463644474780 <span class="nt">--etcd</span> <span class="nt">--controlplane</span>
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 30845    0 30845    0     0  2037k      0 <span class="nt">--</span>:--:-- <span class="nt">--</span>:--:-- <span class="nt">--</span>:--:-- 2151k
<span class="o">[</span>INFO]  Label: cattle.io/os<span class="o">=</span>linux
<span class="o">[</span>INFO]  Role requested: etcd
<span class="o">[</span>INFO]  Role requested: controlplane
<span class="o">[</span>INFO]  Using default agent configuration directory /etc/rancher/agent
<span class="o">[</span>INFO]  Using default agent var directory /var/lib/rancher/agent
<span class="o">[</span>INFO]  Determined CA is necessary to connect to Rancher
<span class="o">[</span>INFO]  Successfully downloaded CA certificate
<span class="o">[</span>INFO]  Value from https://11.0.1.220/cacerts is an x509 certificate
<span class="o">[</span>INFO]  Successfully tested Rancher connection
<span class="o">[</span>INFO]  Downloading rancher-system-agent binary from https://11.0.1.220/assets/rancher-system-agent-amd64
<span class="o">[</span>INFO]  Successfully downloaded the rancher-system-agent binary.
<span class="o">[</span>INFO]  Downloading rancher-system-agent-uninstall.sh script from https://11.0.1.220/assets/system-agent-uninstall.sh
<span class="o">[</span>INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.
<span class="o">[</span>INFO]  Generating Cattle ID
<span class="o">[</span>INFO]  Successfully downloaded Rancher connection information
<span class="o">[</span>INFO]  systemd: Creating service file
<span class="o">[</span>INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env
<span class="o">[</span>INFO]  Enabling rancher-system-agent.service
Created symlink /etc/systemd/system/multi-user.target.wants/rancher-system-agent.service → /etc/systemd/system/rancher-system-agent.service.
<span class="o">[</span>INFO]  Starting/restarting rancher-system-agent.service</code></pre></figure>

<ul>
  <li>Cluster updated a new machine as a node being installing and configuring k8s control plane components.</li>
</ul>

<p><img src="/assets/rancher5.png" alt="image tooltip here" /></p>

<ul>
  <li>Then join the 2 worker nodes</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">ubuntu@racher-worker01:~<span class="nv">$ </span>curl <span class="nt">--insecure</span> <span class="nt">-fL</span> https://11.0.1.220/system-agent-install.sh | <span class="nb">sudo  </span>sh <span class="nt">-s</span> - <span class="nt">--server</span> https://11.0.1.220 <span class="nt">--label</span> <span class="s1">'cattle.io/os=linux'</span> <span class="nt">--token</span> hdsvptc74zvzz62hw9gtt6p7m6nl5k4fs6vk92zqm4f6tvj4tf8m54 <span class="nt">--ca-checksum</span> 31478d0c1db90313258de7fa258cc60de1a3e67dfb2b285cb682463644474780 <span class="nt">--worker</span>
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 30845    0 30845    0     0  5455k      0 <span class="nt">--</span>:--:-- <span class="nt">--</span>:--:-- <span class="nt">--</span>:--:-- 6024k
<span class="o">[</span>INFO]  Label: cattle.io/os<span class="o">=</span>linux
<span class="o">[</span>INFO]  Role requested: worker
<span class="o">[</span>INFO]  Using default agent configuration directory /etc/rancher/agent
<span class="o">[</span>INFO]  Using default agent var directory /var/lib/rancher/agent
<span class="o">[</span>INFO]  Determined CA is necessary to connect to Rancher
<span class="o">[</span>INFO]  Successfully downloaded CA certificate
<span class="o">[</span>INFO]  Value from https://11.0.1.220/cacerts is an x509 certificate
<span class="o">[</span>INFO]  Successfully tested Rancher connection
<span class="o">[</span>INFO]  Downloading rancher-system-agent binary from https://11.0.1.220/assets/rancher-system-agent-amd64
<span class="o">[</span>INFO]  Successfully downloaded the rancher-system-agent binary.
<span class="o">[</span>INFO]  Downloading rancher-system-agent-uninstall.sh script from https://11.0.1.220/assets/system-agent-uninstall.sh
<span class="o">[</span>INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.
<span class="o">[</span>INFO]  Generating Cattle ID
<span class="o">[</span>INFO]  Successfully downloaded Rancher connection information
<span class="o">[</span>INFO]  systemd: Creating service file
<span class="o">[</span>INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env
<span class="o">[</span>INFO]  Enabling rancher-system-agent.service
Created symlink /etc/systemd/system/multi-user.target.wants/rancher-system-agent.service → /etc/systemd/system/rancher-system-agent.service.
<span class="o">[</span>INFO]  Starting/restarting rancher-system-agent.service</code></pre></figure>

<p><img src="/assets/rancher6.png" alt="image tooltip here" /></p>

<ul>
  <li>Create zackweb and joesite as deployment from Rancher console</li>
</ul>

<p><img src="/assets/rancher7.png" alt="image tooltip here" />
<img src="/assets/rancher8.png" alt="image tooltip here" /></p>

<p><b> Conclusion</b></p>

<p>now we can use Rancher to deploy a local k8s cluster based on 3 Linux machines without any trouble just a few commands. then we will be able to create deployment and service in rancher console instead of “kubectl” all the time, it also provides app market for most popular helm charts ready to be installed just by one click like Istio and Prometheus.</p>

<p>Only downside is, Rancher itself requires resources to run which may impact the performance and resources on each node, also it brings complexity in upgrade for both Rancher and k8s.</p>

<p>Overall I love the concept and tools that Rancher provides to manage k8s cluster. I will explore more in the next blog.</p>

<p><img src="/assets/rancher9.png" alt="image tooltip here" /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[My k8s provisioning journey]]></summary></entry><entry><title type="html">K8S with External Cloud Controller Manager</title><link href="http://localhost:4000/jekyll/cat2/2024/02/26/External-cloudprovider.html" rel="alternate" type="text/html" title="K8S with External Cloud Controller Manager" /><published>2024-02-26T11:15:29+11:00</published><updated>2024-02-26T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/02/26/External-cloudprovider</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/02/26/External-cloudprovider.html"><![CDATA[<p><b> Scenario and Challange </b></p>

<p>In last post I was able to automate and create a self-owned K8S cluster on AWS EC2 instances by using Ansible and Terraform.</p>

<p>In this Scenario when deploying k8s pods and services in this K8S cluster, it will not create AWS loadbalancer even mentioned type = LoadBalancer, but this can be eaily achieved when in a AWS managed EKS cluster.</p>

<p>By searching online, Kubernetes actually provides such solution called <b>“cloud-provider-aws”</b>, which provides interface between a self-owned AWS Kubernetes cluster and AWS APIs. This allows EC2 instances running Kubernetes node to be able to provision AWS NLB or ELB resources during service deployment by mentioning “LoadBalancer”..</p>

<p><b></b></p>

<p>To enable the Kubernetes External Cloud Controller Manager, a <b>AWS Cloud Controller manager</b> need to be deployed into cluster, by doing so, it will create and a AWS loadbalancers (NLB), then self-owned K8S cluster can expose services externally by creating ELB.</p>

<p><b> Steps</b></p>

<p>There are a few steps need to be done, docs can be followed by bellow link:</p>

<p>https://github.com/kubernetes/cloud-provider-aws/blob/master/docs/getting_started.md</p>

<ul>
  <li>Change EC2 hostname from ip to FQDN</li>
</ul>

<p>To be able to communicate with AWS API, the k8s node should be changed to FQDN rather than default EC2 IP address</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">curl http://169.254.169.254/latest/meta-data/local-hostname
hostnamectl set-hostname</code></pre></figure>

<ul>
  <li>Create and assign IAM roles to k8s nodes</li>
</ul>

<p>IAM role needed for EC2 running K8S nodes to have proper permission to interact with AWS APIs and create and maintain AWS service</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Roles for master and worker nodes can be found bellow link</span>
https://github.com/kubernetes/cloud-provider-aws/blob/master/docs/prerequisites.md </code></pre></figure>

<ul>
  <li>Tag ec2 instances as owned</li>
</ul>

<p>The K8S nodes need to be tagged as owned</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">tag kubernetes.io/cluster/your_cluster_id<span class="o">=</span>owned</code></pre></figure>

<ul>
  <li>AWS Cloud Controller manager need to be deployed into K8S, this will create a NetworkLoadbalancer in AWS</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl apply <span class="nt">-k</span> <span class="s1">'github.com/kubernetes/cloud-provider-aws/examples/existing-cluster/base/?ref=master</span></code></pre></figure>

<ul>
  <li>Add the –cloud-provider=external to the kube-controller-manager config, kube apiserver config and kubelet’s config</li>
</ul>

<p><b> Conclusion</b></p>

<p>now when create a k8s deployment and service, it will automatically create AWS loadbalancer to route traffic from external into K8S internal pods</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Scenario and Challange]]></summary></entry><entry><title type="html">Automate K8S with Terraform and Ansible</title><link href="http://localhost:4000/jekyll/cat2/2024/02/24/EC2-K8S.html" rel="alternate" type="text/html" title="Automate K8S with Terraform and Ansible" /><published>2024-02-24T11:15:29+11:00</published><updated>2024-02-24T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/02/24/EC2-K8S</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/02/24/EC2-K8S.html"><![CDATA[<p><b> K8S on EC2  vs EKS </b></p>

<p>While I was in an interview last week, the company heavily using Ansible to provision and configure their k8s cluster on AWS EC2, aim to have the most control and flexibility in case they need to move to another cloud provider.</p>

<p>Here I will use both Terraform and Ansible to automate K8S cluster creation and configuration on a few EC2 instances.</p>

<p><b> The tools and workflow</b></p>

<ul>
  <li>Major tools will include Ansible, terraform, together with a shell script.</li>
</ul>

<ol>
  <li>
    <p>Use terraform to create S3 as backend state file, create security groups and 4 ec2 instances ( 1 bastion, 1 master, 2 workers)</p>
  </li>
  <li>
    <p>As Instances are created in AWS environment dynamically, Ansible ec2-dynamic-inventory approach is best way to manage them.</p>
  </li>
  <li>
    <p>Using terraform “connection”, to ssh to the bastion host after creation, use provisoners “file” to upload shell script to bootstrap bastion host as a ansbile master, configure ec2-dynamic-inventory to fetch the other 3 instances for K8S.</p>
  </li>
  <li>
    <p>Continue with terraform “file” and “remote-exec” to upload playbooks and with “inline” to execute playbooks to init k8s master node and join the worker node.</p>
  </li>
  <li>
    <p>Finally another playbook to configure bastion to run kubectl</p>
  </li>
</ol>

<p><b> Create SGs and EC2s, with Bastion bootstrap via terraform </b></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># main.tf</span>

resource <span class="s2">"aws_instance"</span> <span class="s2">"bolg"</span> <span class="o">{</span>
  ami             <span class="o">=</span> var.ami_id <span class="c"># Replace with your AMI ID</span>
  instance_type   <span class="o">=</span> var.instance_type_free
  security_groups <span class="o">=</span> <span class="o">[</span>aws_security_group.blog_sg.name]
  <span class="c">#  vpc_security_group_ids = ["sg-089842a753c9309bb"]</span>
  key_name <span class="o">=</span> var.key_pair
  user_data <span class="o">=</span> <span class="o">&lt;&lt;-</span><span class="no">EOF</span><span class="sh">
    #!/bin/bash
    sudo apt update
    sudo apt install software-properties-common
    sudo add-apt-repository --yes --update ppa:ansible/ansible
    sudo apt install ansible -y
</span><span class="no">  EOF
</span>  tags <span class="o">=</span> <span class="o">{</span>
    Name <span class="o">=</span> <span class="s2">"blog"</span>
  <span class="o">}</span>
  lifecycle <span class="o">{</span>
    prevent_destroy <span class="o">=</span> <span class="nb">true</span>
  <span class="o">}</span>
<span class="o">}</span>
resource <span class="s2">"null_resource"</span> <span class="s2">"upload_playbook"</span> <span class="o">{</span>
  triggers <span class="o">=</span> <span class="o">{</span>
    <span class="c"># Add a dummy trigger to force a refresh</span>
    timestamp <span class="o">=</span> <span class="s2">"</span><span class="k">${</span><span class="nv">timestamp</span><span class="p">()</span><span class="k">}</span><span class="s2">"</span>
  <span class="o">}</span>
  depends_on <span class="o">=</span> <span class="o">[</span>null_resource.wait_for_bastion]
  connection <span class="o">{</span>
    <span class="nb">type</span>        <span class="o">=</span> <span class="s2">"ssh"</span>
    user        <span class="o">=</span> <span class="s2">"ubuntu"</span>
    private_key <span class="o">=</span> file<span class="o">(</span><span class="s2">"terraform-new-key1.pem"</span><span class="o">)</span>
    host        <span class="o">=</span> aws_instance.testbastion.public_ip
  <span class="o">}</span>
  provisioner <span class="s2">"file"</span> <span class="o">{</span>
    <span class="nb">source</span>      <span class="o">=</span> <span class="s2">"pb1.yaml"</span>
    destination <span class="o">=</span> <span class="s2">"/home/ubuntu/pb1.yaml"</span>
  <span class="o">}</span>
  provisioner <span class="s2">"file"</span> <span class="o">{</span>
    <span class="nb">source</span>      <span class="o">=</span> <span class="s2">"aws_ec2.yaml"</span>
    destination <span class="o">=</span> <span class="s2">"/home/ubuntu/aws_ec2.yaml"</span>
  <span class="o">}</span>
  provisioner <span class="s2">"file"</span> <span class="o">{</span>
    <span class="nb">source</span>      <span class="o">=</span> <span class="s2">"ansible.sh"</span>
    destination <span class="o">=</span> <span class="s2">"/home/ubuntu/ansible.sh"</span>
  <span class="o">}</span>
  provisioner <span class="s2">"remote-exec"</span> <span class="o">{</span>
    inline <span class="o">=</span> <span class="o">[</span>
      <span class="s2">"cd /home/ubuntu/"</span>,
      <span class="s2">"sudo chmod 600 terraform-new-key1.pem"</span>,
      <span class="s2">"sudo chmod +x pb1.yaml"</span>,
      <span class="s2">"sudo ansible-playbook pb1.yaml"</span>,
      <span class="s2">"sudo chmod +x ansible.sh"</span>,
      <span class="s2">"sudo ./ansible.sh"</span>,
      <span class="s2">"sudo ansible-inventory -i aws_ec2.yaml --list"</span>,
      <span class="s2">"sudo ansible-inventory --graph"</span>
    <span class="o">]</span>
  <span class="o">}</span>
<span class="o">}</span>
 </code></pre></figure>

<p><b> SSH to bastion host and run playbooks to init and join k8s nodes </b></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># k8s-deploy.yaml</span>

<span class="c"># Playbook hosts can be defined by EC2 tags</span>
<span class="nt">---</span>
- name: EC2-K8S cluster setup
  hosts: all
  gather_facts: <span class="nb">false
  </span>tasks:
    - name: <span class="nb">test </span>EC2 dynamic hosts connections
      include_playbook: 2-test-connection.yaml

    - name: Pre-task <span class="k">for </span>all k8s hosts
      include_playbook: 3-k8s-nodes-preparation.yaml

    - name: init Master node
      include_playbook: 4-master-init.yaml

    - name: Join worker nodes
      include_playbook: 5-work-join.yaml

    - name: configure bastion to run kubectl
      include_playbook: 8-local-kubeconf-admin.yaml</code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we are able to spin up a 3-nodes K8S cluster on EC2 instances in about 20 minutes.</p>

<p>For lab purpose, running K8S on EC2 instances with default VPC can be cheaper and flexible option compare to EKS, as AWS managed EKS require additional VPC and unable to change node instance type unless delete the existing node group.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[K8S on EC2 vs EKS]]></summary></entry><entry><title type="html">Ansible Advance - Roles</title><link href="http://localhost:4000/jekyll/cat2/2024/02/21/Ansible-role.html" rel="alternate" type="text/html" title="Ansible Advance - Roles" /><published>2024-02-21T11:15:29+11:00</published><updated>2024-02-21T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/02/21/Ansible-role</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/02/21/Ansible-role.html"><![CDATA[<p><b>About Ansible Roles </b></p>

<p>Here we play with Ansible Roles, as Ansible is powerful configuration automation tool by running playpook, in addition organizing multiple Ansible contents and playbooks into roles provides a structural and more manageable way to achieve complex tasks for multiple targets or groups.</p>

<p>===================================================================</p>

<p>Ansible Roles can be created by:</p>

<ul>
  <li>Method1: Create Ansible roles by ansible-galaxy</li>
</ul>

<p>Quickly creating a well-defined role directory structure skeleton, we can leverage the command ansible-galaxy init <role_name></role_name></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@localhost ansible]# ansible-galaxy init zack-role
 Role zack-role was created successfully
<span class="o">[</span>root@localhost ansible]# tree zack-role
zack-role
├── defaults
│   └── main.yml
├── files
├── handlers
│   └── main.yml
├── meta
│   └── main.yml
├── README.md
├── tasks
│   └── main.yml
├── templates
├── tests
│   ├── inventory
│   └── test.yml
└── vars
    └── main.yml</code></pre></figure>

<p><b>defaults</b>: Contain default variables for the role.</p>

<p><b>files</b>: Contain static files that you want to copy to the target hosts.</p>

<p><b>handlers</b>: Contain handlers, which are tasks triggered by other tasks.</p>

<p><b>meta</b>: Contain metadata for the role, such as dependencies.</p>

<p><b>tasks</b>: Contains a main.yaml file, which includes tasks specific to the “server1” role.</p>

<p><b>templates</b>: Contain Jinja2 templates.</p>

<p><b>vars</b>: Contain variables specific to the “server1” role</p>

<p>===================================================================</p>

<ul>
  <li>Method2: Manually create roles (server1 &amp; server2) by create on-demand folder structure, Setting up groups in ansible inventory file：</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">vim /etc/ansible/hosts
<span class="c"># add group server1 as web server, server2 as backend</span>
<span class="o">[</span>defaults]
11.0.1.132
11.0.1.131

<span class="o">[</span>server1] <span class="c"># web</span>
11.0.1.131

<span class="o">[</span>server2]  <span class="c"># backend</span>
11.0.1.132

<span class="c"># create own roles structure server1 (web) and server2 (backend)</span>
<span class="c"># create zack.html under role server1/files</span>

<span class="o">[</span>root@localhost ansible]# tree roles
roles
├── base
│   └── tasks
│       └── main.yaml
├── server1
│   ├── defaults
│   ├── files
│   │   └── zack.html
│   ├── handlers
│   ├── meta
│   ├── tasks
│   │   └── main.yaml
│   ├── templates
│   └── vars
└── server2
    ├── defaults
    ├── files
    ├── handlers
    ├── meta
    ├── tasks
    │   └── main.yaml
    ├── templates
    └── vars</code></pre></figure>

<ul>
  <li>define tasks for role server1 and role server2</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">cat</span> /etc/ansible/roles/server1/tasks/main.yaml
<span class="c"># web server tasks include: install httpd, enable service, open 80 port, replace index.html, restart httpd service</span>
- name: Install HTTPD package
  yum:
    name: httpd
    state: present

- name: Enable HTTPD service
  service:
    name: httpd
    enabled: <span class="nb">yes
    </span>state: started

- name: Open port 80 <span class="k">in </span>firewall
  firewalld:
    service: http
    permanent: <span class="nb">yes
    </span>state: enabled

- name: replace index.html
  copy:
    src: zack.html
    dest: /var/www/html/index.html
    owner: root
    group: root
    mode: 0644
  register: httpd_updated

- name: restart httpd service
  service:
    name: httpd
    state: restarted
  when: httpd_updated.changed


<span class="nb">cat</span> /etc/ansible/roles/server2/tasks/main.yaml

<span class="c"># backend server2 tasks include install epel repo, install iftop and lrzsz</span>

- name: Install epel-release
  yum:
    name:
      - epel-release
    state: latest
- name: Check Yum Repository List
  yum:
    list: repo

- name: Install usage packages
  yum:
    name:
      - iftop
      - lrzsz
    state: latest</code></pre></figure>

<ul>
  <li>define role main playbook zack-role.yaml</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># update repo for all hosts, for server1 and server 2, execute each role server1 and server2</span>

<span class="nt">---</span>
- become: <span class="nb">true
  </span>hosts: all
  pre_tasks:
  - name: update repository index
    tags: always
    yum:
      update_cache: <span class="nb">yes
    </span>changed_when: <span class="nb">false</span>

- import_playbook: /etc/ansible/add-more-user.yaml

- hosts: server1
  become: <span class="nb">true
  </span>roles:
    - server1


- hosts: server2
  become: <span class="nb">true
  </span>roles:
    - server2</code></pre></figure>

<ul>
  <li>run playbook for zack-role.yaml</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@localhost ansible]# ansible-playbook zack-role.yaml

PLAY <span class="o">[</span>all] <span class="k">**************************************************************************************************************************</span>

TASK <span class="o">[</span>Gathering Facts] <span class="k">**************************************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.132]
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>update repository index] <span class="k">******************************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]
ok: <span class="o">[</span>11.0.1.132]


PLAY <span class="o">[</span>server1] <span class="k">**********************************************************************************************************************</span>

TASK <span class="o">[</span>Gathering Facts] <span class="k">**************************************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>server1 : Install HTTPD package] <span class="k">**********************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>server1 : Enable HTTPD service] <span class="k">***********************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>server1 : Open port 80 <span class="k">in </span>firewall] <span class="k">*******************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>server1 : replace index.html] <span class="k">*************************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>server1 : restart httpd service] <span class="k">**********************************************************************************************</span>
skipping: <span class="o">[</span>11.0.1.131]

PLAY <span class="o">[</span>server2] <span class="k">**********************************************************************************************************************</span>

TASK <span class="o">[</span>Gathering Facts] <span class="k">**************************************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.132]

TASK <span class="o">[</span>server2 : Install epel-release] <span class="k">***********************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.132]

TASK <span class="o">[</span>server2 : Check Yum Repository List] <span class="k">******************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.132]

TASK <span class="o">[</span>server2 : Install usage packages] <span class="k">*********************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.132]

PLAY RECAP <span class="k">**************************************************************************************************************************</span>
11.0.1.131                 : <span class="nv">ok</span><span class="o">=</span>11   <span class="nv">changed</span><span class="o">=</span>0    <span class="nv">unreachable</span><span class="o">=</span>0    <span class="nv">failed</span><span class="o">=</span>0    <span class="nv">skipped</span><span class="o">=</span>1    <span class="nv">rescued</span><span class="o">=</span>0    <span class="nv">ignored</span><span class="o">=</span>0   
11.0.1.132                 : <span class="nv">ok</span><span class="o">=</span>10   <span class="nv">changed</span><span class="o">=</span>0    <span class="nv">unreachable</span><span class="o">=</span>0    <span class="nv">failed</span><span class="o">=</span>0    <span class="nv">skipped</span><span class="o">=</span>0    <span class="nv">rescued</span><span class="o">=</span>0    <span class="nv">ignored</span><span class="o">=</span>0   </code></pre></figure>

<ul>
  <li>Bingo! validate httpd on server1, and iftop &amp; lrzsz installed on server2 by leveraging with ansible roles</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@localhost ansible]# curl 11.0.1.131
This is zack <span class="nb">test </span>Ansible role web <span class="o">!!!!!!!!!!!</span>   
<span class="o">!!!!!!!!!</span></code></pre></figure>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Ansible Roles]]></summary></entry><entry><title type="html">Enable Free SSL certificate for blog</title><link href="http://localhost:4000/jekyll/cat2/2024/02/07/ssl-cert.html" rel="alternate" type="text/html" title="Enable Free SSL certificate for blog" /><published>2024-02-07T11:15:29+11:00</published><updated>2024-02-07T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/02/07/ssl-cert</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/02/07/ssl-cert.html"><![CDATA[<p><b>HTTPS with SSL cert </b></p>

<ul>
  <li>
    <p>Generate free SSL certificate from https://zerossl.com/</p>
  </li>
  <li>
    <p>Validate the Certificate with Private Key via https://www.sslshopper.com/certificate-key-matcher.html</p>
  </li>
  <li>
    <p>Upload ‘certificate.crt’ and ‘private.key’ to web server /etc/nginx/ssl/</p>
  </li>
  <li>
    <p>Setting up NGINX HTTPS Server by include the ssl parameter to the listen directive in the server block under ‘http’ in ‘nginx.conf’:</p>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">http <span class="o">{</span>

    server <span class="o">{</span>
        listen 443 ssl<span class="p">;</span>
        server_name zackdevops.online<span class="p">;</span>

        ssl_certificate /etc/nginx/ssl/certificate.crt<span class="p">;</span>
        ssl_certificate_key /etc/nginx/ssl/private.key<span class="p">;</span>

    <span class="o">}</span>
...
<span class="o">}</span></code></pre></figure>

<ul>
  <li>fix 2 errors</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">2024/02/07 10:29:33 <span class="o">[</span>emerg] 73175#73175: <span class="s2">"server"</span> directive is not allowed here <span class="k">in</span> /etc/nginx/nginx.conf:11

2024/02/07 10:30:25 <span class="o">[</span>error] 73207#73207: <span class="k">*</span>1 directory index of <span class="s2">"/usr/share/nginx/html/"</span> is forbidden,client: 163.53.144.82, server: zackdevops.online, request: <span class="s2">"GET / HTTP/1.1"</span>, host: <span class="s2">"zackdevops.online"</span>
2024/02/07 10:30:36 <span class="o">[</span>error] 73207#73207: <span class="k">*</span>1 directory index of <span class="s2">"/usr/share/nginx/html/"</span> is forbidden, client: 163.53.144.82, server: zackdevops.online, request: <span class="s2">"GET / HTTP/1.1"</span>, host: <span class="s2">"zackdevops.online"</span></code></pre></figure>

<p>Bingo! https://zackdevops.online connection is secure!</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[HTTPS with SSL cert]]></summary></entry><entry><title type="html">Ansible for AWS Dynamic Inventory</title><link href="http://localhost:4000/jekyll/cat2/2024/01/22/ansible-aws.html" rel="alternate" type="text/html" title="Ansible for AWS Dynamic Inventory" /><published>2024-01-22T11:15:29+11:00</published><updated>2024-01-22T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/01/22/ansible-aws</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/01/22/ansible-aws.html"><![CDATA[<p><b>About Ansible AWS Dynamic Inventory </b></p>

<p>When using Ansible with AWS, maintaining the inventory file will be a hectic task as AWS has frequently changed IPs, autoscaling instances, and much more.</p>

<p>Here we will install and apply ansible plugin for AWS dynamic inventory which makes an API call to AWS to get the instance information in the run time. It givesthe ec2 instance details dynamically to manage the AWS infrastructure</p>

<p>It supports most of the public and private cloud platforms not limited to just AWS.</p>

<ul>
  <li>The Dynamic Inventory Topology:</li>
</ul>

<p><img src="/assets/ansible-inventory.png" alt="image tooltip here" /></p>

<ul>
  <li>Setup Ansible AWS Dynamic Inventory</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Ensure python3 &amp; pip3 installed in Ansible server</span>
python3 <span class="nt">--version</span>
<span class="nb">sudo </span>apt-get <span class="nb">install </span>python3 <span class="nt">-y</span>
<span class="nb">sudo </span>apt-get <span class="nb">install </span>python3-pip <span class="nt">-y</span>

<span class="c"># Install the boto3 library for ansible boot core to make API calls to AWS to retrieve ec2 instance details</span>
<span class="nb">sudo </span>pip3 <span class="nb">install </span>boto3

fix error
ERROR! The ec2 dynamic inventory plugin requires boto3 and botocore.


<span class="c"># Create an inventory directory under /opt and cd into the directory</span>
<span class="nb">sudo mkdir</span> <span class="nt">-p</span> /opt/ansible/inventory
<span class="nb">cd</span> /opt/ansible/inventory
<span class="nb">sudo </span>vi aws_ec2.yaml

<span class="nt">---</span>
plugin: aws_ec2

aws_access_key: &lt;xxx-AWS-ACCESS-KEY-HERE&gt;
aws_secret_key: &lt;xx-AWS-SECRET-KEY-HERE&gt;

regions:
  - us-west-2

keyed_groups:  <span class="c"># key filter for listing AWS ec2 groups</span>
  - key: tags
    prefix: tag
  - prefix: instance_type
    key: instance_type
  - key: placement.region
    prefix: aws_region

<span class="c"># edit ansible config file to enable AWS plugin and set inventory as above yaml</span>
<span class="nb">sudo </span>vi /etc/ansible/ansible.cfg

<span class="o">[</span>inventory]
enable_plugins <span class="o">=</span> aws_ec2

inventory      <span class="o">=</span> /opt/ansible/inventory/aws_ec2.yaml</code></pre></figure>

<p><img src="/assets/ec2-inventory.png" alt="image tooltip here" /></p>

<ul>
  <li>Test if Ansible is able to ping all the machines returned by the dynamic inventory</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">ansible-inventory <span class="nt">-i</span> /opt/ansible/inventory/aws_ec2.yaml <span class="nt">--list</span>
ansible all <span class="nt">-m</span> ping</code></pre></figure>

<ul>
  <li>Execute Ansible Commands With ec2 Dynamic Inventory</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">ansible-inventory <span class="nt">--graph</span></code></pre></figure>

<p>List all instances grouped under tags, zones, and regions with dynamic group names like</p>

<p>aws_region_ap_southeast_2</p>

<p>instance_type_t2_micro</p>

<p>tag_Name</p>

<p><img src="/assets/list-aws-ec2.png" alt="image tooltip here" /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Ansible AWS Dynamic Inventory]]></summary></entry><entry><title type="html">Istio for Traffic Routing</title><link href="http://localhost:4000/jekyll/cat2/2024/01/15/istio.html" rel="alternate" type="text/html" title="Istio for Traffic Routing" /><published>2024-01-15T11:15:29+11:00</published><updated>2024-01-15T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/01/15/istio</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/01/15/istio.html"><![CDATA[<p><b>Helm install istio </b></p>

<p>Here we use helm to install istio (istio-base, istiod, istia gateway), then deploy a sample online book store microservice “bookinfo”, practise istio tasks include Traffic Management, Observability, Security.</p>

<p>Bookinfo Topology:</p>

<p><img src="/assets/bookinfo.png" alt="image tooltip here" /></p>

<ul>
  <li>Helm install istio (istiod, istio-ingress)</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl create namespace istio-system
helm pull istio/base
helm <span class="nb">install </span>istio-base <span class="nb">.</span> <span class="nt">-n</span> istio-system <span class="nt">--set</span> <span class="nv">defaultRevision</span><span class="o">=</span>default

helm pull istio/istiod
helm <span class="nb">install </span>istiod <span class="nb">.</span> <span class="nt">-n</span> istio-system 

kubectl create namespace istio-ingress
helm pull istio/gateway
helm <span class="nb">install </span>istio-ingress <span class="nb">.</span> <span class="nt">-n</span> istio-ingress

helm <span class="nb">ls</span> <span class="nt">-n</span> istio-system
NAME      	NAMESPACE   	REVISION	UPDATED                                	STATUS  	CHART        	APP VERSION
istio-base	istio-system	1       	2023-12-17 08:14:06.943276388 +0800 CST	deployed	base-1.20.1  	1.20.1     
istiod    	istio-system	1       	2023-12-17 08:15:40.370551503 +0800 CST	deployed	istiod-1.20.1	1.20.1 

helm <span class="nb">ls</span> <span class="nt">-n</span> istio-ingress
NAME         	NAMESPACE    	REVISION	UPDATED                                	STATUS  	CHART         	APP VERSION
istio-ingress	istio-ingress	1       	2023-12-17 08:25:07.111999373 +0800 CST	deployed	gateway-1.20.1	1.20.1</code></pre></figure>

<ul>
  <li>Deploy bookinfo microservice and istio ingressgateway and virtualservice</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl label namespace istio-system istio-injection<span class="o">=</span>enabled

kubectl apply <span class="nt">-f</span> https://github.com/istio/istio/blob/master/samples/bookinfo/platform/kube/bookinfo.yaml <span class="nt">-oyaml</span> <span class="o">&gt;</span> bookinfo.yaml
kubectl apply <span class="nt">-f</span> bookinfo.yaml

kubectl get po
NAME                                                     READY   STATUS    RESTARTS       AGE
details-v1-698d88b-wmfcb                                 2/2     Running   0              21m
ratings-v1-6484c4d9bb-cb6gx                              2/2     Running   0              21m
reviews-v1-5b5d6494f4-jrsvc                              2/2     Running   0              21m
reviews-v2-5b667bcbf8-jgfzj                              2/2     Running   0              21m
reviews-v3-5b9bd44f4-tmmfz                               2/2     Running   0              21m

kubectl apply <span class="nt">-f</span> https://github.com/istio/istio/blob/master/samples/bookinfo/networking/bookinfo-gateway.yaml <span class="nt">-oyaml</span> <span class="o">&gt;</span> bookinfo-gateway.yaml
kubectl apply <span class="nt">-f</span> bookinfo-gateway.yaml</code></pre></figure>

<ul>
  <li>Deploy Kiali, jaeger, grafana, prometheus</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">wget https://raw.githubusercontent.com/istio/istio/release-1.20/samples/addons/prometheus.yaml
wget https://raw.githubusercontent.com/istio/istio/release-1.20/samples/addons/jaeger.yaml
wget https://raw.githubusercontent.com/istio/istio/release-1.20/samples/addons/grafana.yaml
kubectl create <span class="nt">-f</span> prometheus.yaml <span class="nt">-f</span> jaeger.yaml <span class="nt">-f</span> grafana.yaml</code></pre></figure>

<ul>
  <li>
    <p>visit http://book.istio:31000/productpage, with review (v1, v2, v3)
<img src="/assets/kiali.png" alt="image tooltip here" /></p>
  </li>
  <li>
    <p>define destination rules and virtual service for reviews</p>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">wget https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/networking/destination-rule-all.yaml
wget https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/networking/virtual-service-reviews-90-10.yaml

kubectl create <span class="nt">-f</span> destination-rule-all.yaml <span class="nt">-f</span> virtual-service-reviews-90-10.yaml   <span class="c"># route v1 10% and v3 90%</span>
kubectl scale deployment reviews-v2 <span class="nt">-n</span> istio-system <span class="nt">--replicas</span><span class="o">=</span>0 <span class="c"># scale down v2 to 0</span>

kubectl get dr <span class="nt">-A</span>
NAMESPACE      NAME          HOST          AGE
istio-system   details       details       6m56s
istio-system   productpage   productpage   6m56s
istio-system   ratings       ratings       6m56s
istio-system   reviews       reviews       6m56s
kubectl get vs <span class="nt">-A</span>
NAMESPACE      NAME       GATEWAYS               HOSTS            AGE
istio-system   bookinfo   <span class="o">[</span><span class="s2">"bookinfo-gateway"</span><span class="o">]</span>   <span class="o">[</span><span class="s2">"book.istio"</span><span class="o">]</span>   19h
istio-system   reviews                           <span class="o">[</span><span class="s2">"reviews"</span><span class="o">]</span>      5m26s

Spec:
  Hosts:
    reviews
  Http:
    Route:
      Destination:
        Host:    reviews
        Subset:  v1
      Weight:    10
      Destination:
        Host:    reviews
        Subset:  v3
      Weight:    90</code></pre></figure>

<p>refresh bookinfo webpage, test Traffic route weight as bellow</p>

<p>90% traffic for reviews v3  vs  10% traffic for review v1</p>

<p><img src="/assets/1090.png" alt="image tooltip here" /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Helm install istio]]></summary></entry><entry><title type="html">Redis cluster with Helm</title><link href="http://localhost:4000/jekyll/cat2/2023/12/09/redis.html" rel="alternate" type="text/html" title="Redis cluster with Helm" /><published>2023-12-09T11:15:29+11:00</published><updated>2023-12-09T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2023/12/09/redis</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2023/12/09/redis.html"><![CDATA[<p><b>Helm install Redis cluster</b></p>

<p>Here we use helm to install Redis cluster, then validate statefuleset storage and cluster avalibility</p>

<p>Typical Redis cluster (3 master + 3 slave for slots) Topology:</p>

<p><img src="/assets/redis-slot.png" alt="image tooltip here" /></p>

<ul>
  <li>Helm install bitnami/redis-cluster</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">helm repo add bitnami https://charts.bitnami.com/bitnami
helm pull bitnami/redis-cluster
kubectl create ns redis
helm <span class="nb">install </span>zz-redis bitnami/redis-cluster <span class="nt">-n</span> redis</code></pre></figure>

<ul>
  <li>redis-cluster status</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl get po | <span class="nb">grep </span>zz-redis
zz-redis-redis-cluster-0                                 1/1     Running   3 <span class="o">(</span>33m ago<span class="o">)</span>     36m
zz-redis-redis-cluster-1                                 1/1     Running   1 <span class="o">(</span>32m ago<span class="o">)</span>     36m
zz-redis-redis-cluster-2                                 1/1     Running   1 <span class="o">(</span>33m ago<span class="o">)</span>     36m
zz-redis-redis-cluster-3                                 1/1     Running   1 <span class="o">(</span>33m ago<span class="o">)</span>     36m
zz-redis-redis-cluster-4                                 1/1     Running   1 <span class="o">(</span>33m ago<span class="o">)</span>     36m
zz-redis-redis-cluster-5                                 1/1     Running   1 <span class="o">(</span>33m ago<span class="o">)</span>     36m

kubectl get svc
zz-redis-redis-cluster                    ClusterIP   10.96.68.34     &lt;none&gt;        6379/TCP                        37m
zz-redis-redis-cluster-headless           ClusterIP   None            &lt;none&gt;        6379/TCP,16379/TCP              37m

kubectl get pvc
NAME                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
redis-data-zz-redis-redis-cluster-0   Bound    pvc-81f69c15-2f59-4704-9fec-c3ab217ebca5   1Gi        RWO            rook-ceph-block   37m
redis-data-zz-redis-redis-cluster-1   Bound    pvc-871dbb68-a78b-48a8-8feb-8726eb8a795e   1Gi        RWO            rook-ceph-block   37m
redis-data-zz-redis-redis-cluster-2   Bound    pvc-afd8c82e-c314-426a-a3cd-3c8d10b42bb1   1Gi        RWO            rook-ceph-block   37m
redis-data-zz-redis-redis-cluster-3   Bound    pvc-e8fd5d47-dd60-4358-8cf5-a17d6574bbe2   1Gi        RWO            rook-ceph-block   37m
redis-data-zz-redis-redis-cluster-4   Bound    pvc-04d4f148-b7c1-407e-b9a8-b2fa911405f0   1Gi        RWO            rook-ceph-block   37m
redis-data-zz-redis-redis-cluster-5   Bound    pvc-99e8b2cc-c5d5-4636-a19b-042922eca3cc   1Gi        RWO            rook-ceph-block   37m</code></pre></figure>

<ul>
  <li>Validate cluster by set key</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl <span class="nb">exec</span> <span class="nt">-it</span> zz-redis-redis-cluster-0 <span class="nt">--</span> sh
redis-cli
info replication
cluster info
cluster nodes
054ff137e9530c0e4d8afd1d00162d01952580de 172.16.122.179:6379@16379 slave 2ebcfdf7e74aa8755250384f7efa466f4d18e9d4 0 1702703474000 3 connected
7cfeb1d88dc3838e600a32c1e233b1c5f05006f1 172.16.58.197:6379@16379 master - 0 1702703473000 2 connected 5461-10922
1e56bad62197062fad465bbcc6b625bea8364db2 172.16.58.224:6379@16379 slave 7cfeb1d88dc3838e600a32c1e233b1c5f05006f1 0 1702703474954 2 connected
7e539b2209581c8375f7fb0aa9eedf5b98754b05 172.16.85.252:6379@16379 slave 889515187dbbb525fd73dc840d5bcad78305645d 0 1702703473947 1 connected
889515187dbbb525fd73dc840d5bcad78305645d 172.16.195.10:6379@16379 myself,master - 0 1702703469000 1 connected 0-5460
2ebcfdf7e74aa8755250384f7efa466f4d18e9d4 172.16.85.229:6379@16379 master - 0 1702703473000 3 connected 10923-16383


127.0.0.1:6379&gt; <span class="nb">set </span>dad zack
OK
127.0.0.1:6379&gt; get dad
<span class="s2">"zack"</span>

kubectl <span class="nb">exec</span> <span class="nt">-it</span> zz-redis-redis-cluster-4 <span class="nt">--</span> sh
redis-cli
127.0.0.1:6379&gt; KEYS <span class="k">*</span>
1<span class="o">)</span> <span class="s2">"dad"</span></code></pre></figure>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Helm install Redis cluster]]></summary></entry><entry><title type="html">Rook-Ceph for dynamic Persistent Volume</title><link href="http://localhost:4000/jekyll/cat2/2023/11/09/ceph.html" rel="alternate" type="text/html" title="Rook-Ceph for dynamic Persistent Volume" /><published>2023-11-09T11:15:29+11:00</published><updated>2023-11-09T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2023/11/09/ceph</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2023/11/09/ceph.html"><![CDATA[<p><b>Helm install Rook-Ceph for persistent storage</b></p>

<ul>
  <li>
    <p>Configure local VM block storage to add 50Gb sdb to all k8s master and work nodes</p>
  </li>
  <li>
    <p>install rook-ceph cluster</p>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">git clone <span class="nt">--single-branch</span> <span class="nt">--branch</span> master https://github.com/rook/rook.git
<span class="nb">cd </span>rook/deploy/examples
kubectl create <span class="nt">-f</span> crds.yaml <span class="nt">-f</span> common.yaml <span class="nt">-f</span> operator.yaml
kubectl create <span class="nt">-f</span> cluster.yaml</code></pre></figure>

<ul>
  <li>Ceph toolbox to check cluster status</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl create <span class="nt">-f</span> toolbox.yaml
kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> deploy/rook-ceph-tools <span class="nt">--</span> bash
ceph status
ceph osd status
ceph <span class="nb">df
</span>rados <span class="nb">df</span></code></pre></figure>

<ul>
  <li>Ceph Dashboard svc for https login</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl create <span class="nt">-f</span> dashboard-external-https.yaml
kubectl <span class="nt">-n</span> rook-ceph get secret rook-ceph-dashboard-password <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{['data']['password']}"</span> | <span class="nb">base64</span> <span class="nt">--decode</span> <span class="o">&amp;&amp;</span> <span class="nb">echo</span></code></pre></figure>

<ul>
  <li>Create storage pool and storageclass</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl create <span class="nt">-f</span> pool.yaml
<span class="nb">cd </span>csi/rbd
kubectl create <span class="nt">-f</span> storageclass.yaml</code></pre></figure>

<ul>
  <li>set “rook-ceph-block” as default sc</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl patch storageclass rook-ceph-block <span class="nt">-p</span> <span class="s1">'{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'</span>

kubectl get sc
NAME                        PROVISIONER                  RECLAIMPOLICY  VOLUMEBINDINGMODE  ALLOWVOLUMEEXPANSION   AGE
rook-ceph-block <span class="o">(</span>default<span class="o">)</span>   rook-ceph.rbd.csi.ceph.com   Delete          Immediate          <span class="nb">true                  </span>8d</code></pre></figure>

<p>check pool and OSDs in ceph webui</p>

<p><img src="/assets/ceph.png" alt="image tooltip here" /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Helm install Rook-Ceph for persistent storage]]></summary></entry><entry><title type="html">Apache Kafka cluster with testing topics</title><link href="http://localhost:4000/jekyll/cat2/2023/11/09/kafka.html" rel="alternate" type="text/html" title="Apache Kafka cluster with testing topics" /><published>2023-11-09T11:15:29+11:00</published><updated>2023-11-09T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2023/11/09/kafka</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2023/11/09/kafka.html"><![CDATA[<p><b>About Kafka </b></p>

<p>Apache Kafka has proven to be an extremely popular event streaming platform, as its scalable distributed architecture, high performance, and  use cases, some key terms and concepts as bellow:</p>

<ul>
  <li>
    <p>Kafka clusters and Kafka brokers</p>
  </li>
  <li>
    <p>Kafka clients and servers</p>
  </li>
  <li>
    <p>Producers, and Consumers, and Consumer groups</p>
  </li>
  <li>
    <p>Kafka topics &amp; Kafka partitions, offsets</p>
  </li>
  <li>
    <p>Kafka topic replication, leaders, and followers</p>
  </li>
  <li>
    <p>ZooKeeper or not</p>
  </li>
</ul>

<p>Typical Kafka Topology:</p>

<p><img src="/assets/kafka.png" alt="image tooltip here" /></p>

<ul>
  <li>Helm install bitnami/kafka</li>
</ul>

<p>Here we use helm to install Kafka, then validate statefuleset storage and cluster avalibility by create topic, producer and consumer</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">helm repo add bitnami https://charts.bitnami.com/bitnami
helm pull bitnami/kafka
kubectl create ns kafka
helm <span class="nb">install </span>zz-kafka <span class="nb">.</span> <span class="nt">-n</span> kafka</code></pre></figure>

<ul>
  <li>Kafka-cluster status</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl get all | <span class="nb">grep </span>zz-kaf
pod/zz-kafka-client                                          1/1     Running   0               6m27s
pod/zz-kafka-controller-0                                    1/1     Running   0               8m52s
pod/zz-kafka-controller-1                                    1/1     Running   0               8m52s
pod/zz-kafka-controller-2                                    1/1     Running   0               8m52s
service/zz-kafka                                  ClusterIP   10.96.58.62     &lt;none&gt;        9092/TCP                        8m52s
service/zz-kafka-controller-headless              ClusterIP   None            &lt;none&gt;        9094/TCP,9092/TCP,9093/TCP      8m52s
statefulset.apps/zz-kafka-controller                                    3/3     8m52s

kubectl get pvc
NAME                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
data-zz-kafka-controller-0   Bound    pvc-f3831a5c-c9cf-46bb-a47d-58ea80f82e28   8Gi        RWO            rook-ceph-block   9m19s
data-zz-kafka-controller-1   Bound    pvc-a45fd063-7979-4c0d-8ae4-93b4b4b0bf7f   8Gi        RWO            rook-ceph-block   9m19s
data-zz-kafka-controller-2   Bound    pvc-ed48b71d-81be-4067-b05f-9e8dc64fab04   8Gi        RWO            rook-ceph-block   9m19s</code></pre></figure>

<ul>
  <li>Validate cluster by set key
create client.properties with SASL authentication details, copy to client</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl run zz-kafka-client <span class="nt">--restart</span><span class="o">=</span><span class="s1">'Never'</span> <span class="nt">--image</span> docker.io/bitnami/kafka:3.6.1-debian-11-r0 <span class="nt">--namespace</span> default <span class="nt">--command</span> <span class="nt">--</span> <span class="nb">sleep </span>infinity
kubectl <span class="nb">cp</span> <span class="nt">--namespace</span> default client.properties zz-kafka-client:/tmp/client.properties</code></pre></figure>

<p>open 2 bash window to access kafka client</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl <span class="nb">exec</span> <span class="nt">--tty</span> <span class="nt">-i</span> zz-kafka-client <span class="nt">--namespace</span> default <span class="nt">--</span> bash

window1:
    PRODUCER:
        kafka-console-producer.sh <span class="se">\</span>
            <span class="nt">--producer</span>.config /tmp/client.properties <span class="se">\</span>
            <span class="nt">--broker-list</span> zz-kafka-controller-0.zz-kafka-controller-headless.default.svc.cluster.local:9092,zz-kafka-controller-1.zz-kafka-controller-headless.default.svc.cluster.local:9092,zz-kafka-controller-2.zz-kafka-controller-headless.default.svc.cluster.local:9092 <span class="se">\</span>
            <span class="nt">--topic</span> <span class="nb">test

</span>window2:
    CONSUMER:
        kafka-console-consumer.sh <span class="se">\</span>
            <span class="nt">--consumer</span>.config /tmp/client.properties <span class="se">\</span>
            <span class="nt">--bootstrap-server</span> zz-kafka.default.svc.cluster.local:9092 <span class="se">\</span>
            <span class="nt">--topic</span> <span class="nb">test</span> <span class="se">\</span>
            <span class="nt">--from-beginning</span></code></pre></figure>

<p>test topic and PRODUCER with CONSUMER</p>

<p><img src="/assets/kafka-validation.png" alt="image tooltip here" /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Kafka]]></summary></entry></feed>