<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-05-19T12:03:53+10:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Zack’s Blog</title><subtitle>## AWS   ## Jenkins  ## Microservices ## Automation ## K8S   ## CICD     ## Gitops  [京ICP备2024056683号-1]</subtitle><entry><title type="html">Python: File Handling for AWS tagging</title><link href="http://localhost:4000/jekyll/cat2/2024/05/17/py2.html" rel="alternate" type="text/html" title="Python: File Handling for AWS tagging" /><published>2024-05-17T10:15:29+10:00</published><updated>2024-05-17T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/17/py2</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/17/py2.html"><![CDATA[<p><b> About Python File Handling</b></p>

<p>In the previous post I developed shell script + awscli to apply <a href="https://zackz.online/jekyll/cat2/2024/05/01/AWS-tagging.html">aws EC2 tages</a>, since the last post we discovered <a href="https://zackz.online/jekyll/cat2/2024/05/16/py1.html">Python Boto3</a> scripts for AWS resource automation and management, I think it is time to improve the EC2 tagging task with Python and boto3, together with file handling to achieve:</p>

<ol>
  <li>
    <p>List and export ec2 information to a CSV file (instanceID, default instance name, Existing tags)</p>
  </li>
  <li>
    <p>define 4 mandatory tags in CSV header (Env, BizOwner, Technology, Project)</p>
  </li>
  <li>
    <p>validate exported tags against the 4 mandatory new tags, if any of the new mandatory tags exists, then keep the tage and value, if any of the new mandatory tags do not exist, leave the value blank</p>
  </li>
  <li>
    <p>Get csv file fill with mandatory tags input from Biz team (manual work)</p>
  </li>
  <li>
    <p>open the updated CSV file, apply the mandatory tags based on the input value</p>
  </li>
  <li>
    <p>create and trigger lambda function with aws config rules to enforce 4 mandatory tags whenever a new instance launch  </p>
  </li>
</ol>

<p><b> List and export ec2 information to a CSV</b></p>

<ul>
  <li>
    <p>First I need to import Python libraries for boto3, csv and os, then call boto3 sessions to retrieve EC2 information.</p>
  </li>
  <li>
    <p>Next step I will use Python “with open” and “for” loops to write each ec2 info to a csv file, also add mandatory tags write in the header fields</p>
  </li>
  <li>
    <p>Once passing csv file to get new tag input, use Python “csv.DictReader” to read from the file, then apply tags by “ec2.create_tags”</p>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">vim ec2_export_tag_csv.py

<span class="c"># Import libiaries</span>
import boto3
import csv
import os

<span class="c"># Define the mandatory tags</span>
MANDATORY_TAGS <span class="o">=</span> <span class="o">[</span><span class="s2">"Env"</span>, <span class="s2">"BizOwner"</span>, <span class="s2">"Technology"</span>, <span class="s2">"Project"</span><span class="o">]</span>

<span class="c"># Initialize boto3 clients</span>
ec2 <span class="o">=</span> boto3.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>
s3 <span class="o">=</span> boto3.client<span class="o">(</span><span class="s1">'s3'</span><span class="o">)</span>

def list_ec2_instances<span class="o">()</span>:
    instances <span class="o">=</span> <span class="o">[]</span>
    response <span class="o">=</span> ec2.describe_instances<span class="o">()</span>
    <span class="k">for </span>reservation <span class="k">in </span>response[<span class="s1">'Reservations'</span><span class="o">]</span>:
        <span class="k">for </span>instance <span class="k">in </span>reservation[<span class="s1">'Instances'</span><span class="o">]</span>:
            instance_id <span class="o">=</span> instance[<span class="s1">'InstanceId'</span><span class="o">]</span>
            default_name <span class="o">=</span> next<span class="o">((</span>tag[<span class="s1">'Value'</span><span class="o">]</span> <span class="k">for </span>tag <span class="k">in </span>instance.get<span class="o">(</span><span class="s1">'Tags'</span>, <span class="o">[])</span> <span class="k">if </span>tag[<span class="s1">'Key'</span><span class="o">]</span> <span class="o">==</span> <span class="s1">'Name'</span><span class="o">)</span>, <span class="s1">'No Name'</span><span class="o">)</span>
            tags <span class="o">=</span> <span class="o">{</span>tag[<span class="s1">'Key'</span><span class="o">]</span>: tag[<span class="s1">'Value'</span><span class="o">]</span> <span class="k">for </span>tag <span class="k">in </span>instance.get<span class="o">(</span><span class="s1">'Tags'</span>, <span class="o">[])}</span>
            instance_info <span class="o">=</span> <span class="o">{</span>
                <span class="s1">'InstanceId'</span>: instance_id,
                <span class="s1">'DefaultName'</span>: default_name,
                <span class="k">**</span><span class="o">{</span>tag: tags.get<span class="o">(</span>tag, <span class="s1">''</span><span class="o">)</span> <span class="k">for </span>tag <span class="k">in </span>MANDATORY_TAGS<span class="o">}</span>
            <span class="o">}</span>
            instances.append<span class="o">(</span>instance_info<span class="o">)</span>
    <span class="k">return </span>instances

<span class="c"># write and loop each ec2 tag info, add MANDATORY_TAGS in fields</span>
def export_to_csv<span class="o">(</span>instances, <span class="nv">filename</span><span class="o">=</span><span class="s1">'ec2_instances.csv'</span><span class="o">)</span>:
    fieldnames <span class="o">=</span> <span class="o">[</span><span class="s1">'InstanceId'</span>, <span class="s1">'DefaultName'</span><span class="o">]</span> + MANDATORY_TAGS
    with open<span class="o">(</span>filename, <span class="s1">'w'</span>, <span class="nv">newline</span><span class="o">=</span><span class="s1">''</span><span class="o">)</span> as csvfile:
        writer <span class="o">=</span> csv.DictWriter<span class="o">(</span>csvfile, <span class="nv">fieldnames</span><span class="o">=</span>fieldnames<span class="o">)</span>
        writer.writeheader<span class="o">()</span>
        <span class="k">for </span>instance <span class="k">in </span>instances:
            writer.writerow<span class="o">(</span>instance<span class="o">)</span>

<span class="c"># Update CSV by adding biz input, then apply all tags </span>
def update_tags_from_csv<span class="o">(</span><span class="nv">filename</span><span class="o">=</span><span class="s1">'ec2_instances_updated.csv'</span><span class="o">)</span>:
    with open<span class="o">(</span>filename, <span class="nv">newline</span><span class="o">=</span><span class="s1">''</span><span class="o">)</span> as csvfile:
        reader <span class="o">=</span> csv.DictReader<span class="o">(</span>csvfile<span class="o">)</span>
        <span class="k">for </span>row <span class="k">in </span>reader:
            instance_id <span class="o">=</span> row[<span class="s1">'InstanceId'</span><span class="o">]</span>
            tags <span class="o">=</span> <span class="o">[{</span><span class="s1">'Key'</span>: tag, <span class="s1">'Value'</span>: row[tag]<span class="o">}</span> <span class="k">for </span>tag <span class="k">in </span>MANDATORY_TAGS <span class="k">if </span>row[tag]]
            <span class="k">if </span>tags:
                ec2.create_tags<span class="o">(</span><span class="nv">Resources</span><span class="o">=[</span>instance_id], <span class="nv">Tags</span><span class="o">=</span>tags<span class="o">)</span>

<span class="c"># skip the lambda function and config rule creation steps</span>
def create_lambda_function<span class="o">()</span>: <span class="c"># skip lambda creation</span>
def create_config_rule<span class="o">()</span>: <span class="c"># skip config rule creation</span>

<span class="c"># print status</span>
def main<span class="o">()</span>:
    instances <span class="o">=</span> list_ec2_instances<span class="o">()</span>
    export_to_csv<span class="o">(</span>instances<span class="o">)</span>
    print<span class="o">(</span><span class="s2">"CSV export complete. Please update the mandatory tags and save the file as 'ec2_instances_updated.csv'."</span><span class="o">)</span>
    input<span class="o">(</span><span class="s2">"Press Enter after updating the CSV file..."</span><span class="o">)</span>
    update_tags_from_csv<span class="o">()</span>
    print<span class="o">(</span><span class="s2">"Tags updated successfully."</span><span class="o">)</span>

<span class="k">if </span>__name__ <span class="o">==</span> <span class="s1">'__main__'</span>:
    main<span class="o">()</span></code></pre></figure>

<p><b> How about managing tags for multiple AWS accounts </b></p>

<p>Considering we have 20+ AWS accounts across the company and with more than 200 EC2 instances that need to apply tagging strategy, here I will</p>

<ol>
  <li>
    <p>use AWScli profile to configure each AWS account creds,</p>
  </li>
  <li>
    <p>Update Python scripts to call each account profile to apply all 20+ AWS accounts in sequence.</p>
  </li>
  <li>
    <p>here I will use my own 2 AWS accounts (ZackBlog and JoeSite) to create AWScli profiles to validate the Python scripts</p>
  </li>
</ol>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Add account creds into ~/.aws/credentials</span>
vim ~/.aws/credentials

<span class="o">[</span>aws_account_zackblog]
aws_access_key_id <span class="o">=</span> xxxx
aws_secret_access_key <span class="o">=</span> yyyy

<span class="o">[</span>aws_account_joesite]
aws_access_key_id <span class="o">=</span> zzzz
aws_secret_access_key <span class="o">=</span> yyyy</code></pre></figure>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># add profiles into ~/.aws/config</span>
vim ~/.aws/config

<span class="o">[</span>profile aws_account_zackblog]
region <span class="o">=</span> ap-southeast-2

<span class="o">[</span>profile aws_account_joesite]
region <span class="o">=</span> ap-southeast-2</code></pre></figure>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">vim muti_ec2_export_tag_csv.py

<span class="c"># Import libraries </span>
import boto3
import csv
import os
from botocore.exceptions import ProfileNotFound

<span class="c"># Define the mandatory tags</span>
MANDATORY_TAGS <span class="o">=</span> <span class="o">[</span><span class="s2">"Env"</span>, <span class="s2">"BizOwner"</span>, <span class="s2">"Technology"</span>, <span class="s2">"Project"</span><span class="o">]</span>

<span class="c"># Define AWS account profiles</span>
AWS_PROFILES <span class="o">=</span> <span class="o">[</span><span class="s2">"aws_account_zackblog"</span>, <span class="s2">"aws_account_joesite"</span><span class="o">]</span>

<span class="c"># Retriete EC2 tags info by calling each AWS account profile boto3 session</span>
def list_ec2_instances<span class="o">(</span>profile_name<span class="o">)</span>:
    session <span class="o">=</span> boto3.Session<span class="o">(</span><span class="nv">profile_name</span><span class="o">=</span>profile_name<span class="o">)</span>
    ec2 <span class="o">=</span> session.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>
    instances <span class="o">=</span> <span class="o">[]</span>
    response <span class="o">=</span> ec2.describe_instances<span class="o">()</span>
    <span class="k">for </span>reservation <span class="k">in </span>response[<span class="s1">'Reservations'</span><span class="o">]</span>:
        <span class="k">for </span>instance <span class="k">in </span>reservation[<span class="s1">'Instances'</span><span class="o">]</span>:
            instance_id <span class="o">=</span> instance[<span class="s1">'InstanceId'</span><span class="o">]</span>
            default_name <span class="o">=</span> next<span class="o">((</span>tag[<span class="s1">'Value'</span><span class="o">]</span> <span class="k">for </span>tag <span class="k">in </span>instance.get<span class="o">(</span><span class="s1">'Tags'</span>, <span class="o">[])</span> <span class="k">if </span>tag[<span class="s1">'Key'</span><span class="o">]</span> <span class="o">==</span> <span class="s1">'Name'</span><span class="o">)</span>, <span class="s1">'No Name'</span><span class="o">)</span>
            tags <span class="o">=</span> <span class="o">{</span>tag[<span class="s1">'Key'</span><span class="o">]</span>: tag[<span class="s1">'Value'</span><span class="o">]</span> <span class="k">for </span>tag <span class="k">in </span>instance.get<span class="o">(</span><span class="s1">'Tags'</span>, <span class="o">[])}</span>
            instance_info <span class="o">=</span> <span class="o">{</span>
                <span class="s1">'InstanceId'</span>: instance_id,
                <span class="s1">'DefaultName'</span>: default_name,
                <span class="k">**</span><span class="o">{</span>tag: tags.get<span class="o">(</span>tag, <span class="s1">''</span><span class="o">)</span> <span class="k">for </span>tag <span class="k">in </span>MANDATORY_TAGS<span class="o">}</span>
            <span class="o">}</span>
            instances.append<span class="o">(</span>instance_info<span class="o">)</span>
    <span class="k">return </span>instances

<span class="c"># Write into CSV for each profile</span>
def export_to_csv<span class="o">(</span>instances, profile_name, <span class="nv">filename_prefix</span><span class="o">=</span><span class="s1">'ec2_instances'</span><span class="o">)</span>:
    filename <span class="o">=</span> f<span class="s2">"{filename_prefix}_{profile_name}.csv"</span>
    fieldnames <span class="o">=</span> <span class="o">[</span><span class="s1">'InstanceId'</span>, <span class="s1">'DefaultName'</span><span class="o">]</span> + MANDATORY_TAGS
    with open<span class="o">(</span>filename, <span class="s1">'w'</span>, <span class="nv">newline</span><span class="o">=</span><span class="s1">''</span><span class="o">)</span> as csvfile:
        writer <span class="o">=</span> csv.DictWriter<span class="o">(</span>csvfile, <span class="nv">fieldnames</span><span class="o">=</span>fieldnames<span class="o">)</span>
        writer.writeheader<span class="o">()</span>
        <span class="k">for </span>instance <span class="k">in </span>instances:
            writer.writerow<span class="o">(</span>instance<span class="o">)</span>

<span class="c"># Update each csv for each profile, then apply tags accordingly</span>
def update_tags_from_csv<span class="o">(</span>profile_name, <span class="nv">filename_prefix</span><span class="o">=</span><span class="s1">'ec2_instances_updated'</span><span class="o">)</span>:
    filename <span class="o">=</span> f<span class="s2">"{filename_prefix}_{profile_name}.csv"</span>
    session <span class="o">=</span> boto3.Session<span class="o">(</span><span class="nv">profile_name</span><span class="o">=</span>profile_name<span class="o">)</span>
    ec2 <span class="o">=</span> session.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>
    with open<span class="o">(</span>filename, <span class="nv">newline</span><span class="o">=</span><span class="s1">''</span><span class="o">)</span> as csvfile:
        reader <span class="o">=</span> csv.DictReader<span class="o">(</span>csvfile<span class="o">)</span>
        <span class="k">for </span>row <span class="k">in </span>reader:
            instance_id <span class="o">=</span> row[<span class="s1">'InstanceId'</span><span class="o">]</span>
            tags <span class="o">=</span> <span class="o">[{</span><span class="s1">'Key'</span>: tag, <span class="s1">'Value'</span>: row[tag]<span class="o">}</span> <span class="k">for </span>tag <span class="k">in </span>MANDATORY_TAGS <span class="k">if </span>row[tag]]
            <span class="k">if </span>tags:
                ec2.create_tags<span class="o">(</span><span class="nv">Resources</span><span class="o">=[</span>instance_id], <span class="nv">Tags</span><span class="o">=</span>tags<span class="o">)</span>

<span class="c"># print status</span>
def process_all_profiles<span class="o">()</span>:
    <span class="k">for </span>profile <span class="k">in </span>AWS_PROFILES:
        try:
            print<span class="o">(</span>f<span class="s2">"Processing profile: {profile}"</span><span class="o">)</span>
            instances <span class="o">=</span> list_ec2_instances<span class="o">(</span>profile<span class="o">)</span>
            export_to_csv<span class="o">(</span>instances, profile<span class="o">)</span>
            print<span class="o">(</span>f<span class="s2">"CSV export complete for profile {profile}. Please update the mandatory tags and save the file as '{profile}_ec2_instances_updated.csv'."</span><span class="o">)</span>
            input<span class="o">(</span>f<span class="s2">"Press Enter after updating the CSV file for profile {profile}..."</span><span class="o">)</span>
            update_tags_from_csv<span class="o">(</span>profile<span class="o">)</span>
            print<span class="o">(</span>f<span class="s2">"Tags updated successfully for profile {profile}."</span><span class="o">)</span>
        except ProfileNotFound:
            print<span class="o">(</span>f<span class="s2">"Profile {profile} not found. Skipping."</span><span class="o">)</span>

<span class="k">if </span>__name__ <span class="o">==</span> <span class="s1">'__main__'</span>:
    process_all_profiles<span class="o">()</span></code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we can use Python Boto3 and file handling to achieve mutiple-aws account EC2 tagging. With Python “CSV” library, functions like “csv.DictReader”, “with open” and “csv.DictWriter” to open, update and export CSV file, Python also supports handling data in JSON format with dictionary.</p>

<p>In the next post I will see how to use Python Flask to redesign Zack’s blog for Web application development.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Python File Handling]]></summary></entry><entry><title type="html">Python: Boto3 for AWS Resource Management</title><link href="http://localhost:4000/jekyll/cat2/2024/05/16/py1.html" rel="alternate" type="text/html" title="Python: Boto3 for AWS Resource Management" /><published>2024-05-16T10:15:29+10:00</published><updated>2024-05-16T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/16/py1</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/16/py1.html"><![CDATA[<p><b> About Boto3 </b></p>

<p>Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python. It enables developers to build software that uses Amazon services like EC2, S3, RDS, etc.</p>

<p>I will build a portable python3.9 + Boto3 docker environment to test some AWS automation tasks.</p>

<p><b> Build and run a docker with Python3.9 + Boto3 </b></p>

<p>As I do not want to install Python, Boto3, and AWScli on my local PC, creating a docker image with all software ready as a portable env is the best way to start.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~# vim Dockerfile
<span class="c"># Build from python:3.9.19-alpine3.19</span>
From python:3.9.19-alpine3.19 
<span class="c"># install boto3 and alwcli</span>
RUN pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip <span class="o">&amp;&amp;</span> <span class="se">\</span>
    pip <span class="nb">install</span> <span class="nt">--upgrade</span> awscli <span class="o">&amp;&amp;</span> <span class="se">\</span>
    pip <span class="nb">install</span> <span class="nt">--upgrade</span> boto3
<span class="c"># set work dir</span>
WORKDIR /work
<span class="c"># run Python</span>
CMD <span class="s2">"python"</span>
<span class="c"># build a docker image from the above Dockerfile </span>
root@ubt-server:~# docker image build <span class="nt">-t</span> zack_aws_boto3 <span class="nb">.</span>

<span class="c"># ls docker images </span>
root@ubt-server:~# docker image <span class="nb">ls
</span>REPOSITORY               TAG             IMAGE ID       CREATED         SIZE
zack_aws_boto3           v1              07a13f7801ed   1 days ago     998MB
zackpy                   latest          287ba6873741   4 days ago      48.2MB
zackz001/gitops-jekyll   latest          d92894f7be6d   6 days ago      70.9MB
postgres                 15.0            027eba2e8939   19 months ago   377MB

<span class="c"># run docker and mount local python work dir</span>
root@ubt-server:~/pythonwork# docker run <span class="nt">-ti</span> <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>:/work zack_aws_boto3:v1 bash
root@c04670a43564:/# 
root@c04670a43564:/# <span class="nb">cd </span>work <span class="o">&amp;&amp;</span> <span class="nb">ls</span>

<span class="c"># configure aws in the container</span>
root@c04670a43564:/work# aws configure 
AWS Access Key ID <span class="o">[</span><span class="k">****************</span>GFNW]: 
AWS Secret Access Key <span class="o">[</span><span class="k">****************</span>Db7O]: 
Default region name <span class="o">[</span>ap-southeast-2]: 
Default output format <span class="o">[</span>None]: 

<span class="c"># validate aws cred by listing ec2 instance id </span>
root@c04670a43564:/work# aws ec2 describe-instances <span class="nt">--query</span> <span class="s2">"Reservations[*].Instances[*].InstanceId"</span> <span class="nt">--output</span> json
<span class="o">[</span>
    <span class="o">[</span>
        <span class="s2">"i-076226daa5aaf7cf2"</span>
    <span class="o">]</span>
<span class="o">]</span></code></pre></figure>

<p><b>Manage AWS resource with Python Boto3 script</b></p>

<p>Here we have Python and boto3 env ready; I will list some aws tasks that I want to be achieved by Python scripts</p>

<ul>
  <li>List ec2 instance name, instanceID and state</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~/pythonwork# vim app.py
<span class="c"># import boto3 library</span>
import boto3

def list_ec2_instances<span class="o">()</span>:
    <span class="c"># Create a session using default AWS profile</span>
    session <span class="o">=</span> boto3.Session<span class="o">()</span>
    <span class="c"># Create an EC2 client</span>
    ec2_client <span class="o">=</span> session.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>

    <span class="c"># Describe EC2 instances</span>
    response <span class="o">=</span> ec2_client.describe_instances<span class="o">()</span>

    <span class="c"># Iterate over the instances</span>
    <span class="k">for </span>reservation <span class="k">in </span>response[<span class="s1">'Reservations'</span><span class="o">]</span>:
        <span class="k">for </span>instance <span class="k">in </span>reservation[<span class="s1">'Instances'</span><span class="o">]</span>:
            <span class="c"># Get the instance ID</span>
            instance_id <span class="o">=</span> instance[<span class="s1">'InstanceId'</span><span class="o">]</span>
            
            <span class="c"># Get the instance state</span>
            instance_state <span class="o">=</span> instance[<span class="s1">'State'</span><span class="o">][</span><span class="s1">'Name'</span><span class="o">]</span>
            
            <span class="c"># Get the instance Name tag if exists</span>
            instance_name <span class="o">=</span> <span class="s1">'No Name'</span>
            <span class="k">if</span> <span class="s1">'Tags'</span> <span class="k">in </span>instance:
                <span class="k">for </span>tag <span class="k">in </span>instance[<span class="s1">'Tags'</span><span class="o">]</span>:
                    <span class="k">if </span>tag[<span class="s1">'Key'</span><span class="o">]</span> <span class="o">==</span> <span class="s1">'Name'</span>:
                        instance_name <span class="o">=</span> tag[<span class="s1">'Value'</span><span class="o">]</span>
                        <span class="nb">break</span>
            
            <span class="c"># Print instance ID, Name, and State</span>
            print<span class="o">(</span>f<span class="s2">"Instance ID: {instance_id}, Name: {instance_name}, State: {instance_state}"</span><span class="o">)</span>

<span class="k">if </span>__name__ <span class="o">==</span> <span class="s2">"__main__"</span>:
    list_ec2_instances<span class="o">()</span>

root@c04670a43564:/work# python app.py 
Instance ID: i-076226daa5aaf7cf2, Name: zack-blog, State: stopped</code></pre></figure>

<ul>
  <li>Filter ec2 instance without tag “owner”</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create app-untagged.py</span>
root@ubt-server:~/pythonwork# vim app-untagged.py
import boto3

def get_untagged_ec2_instances<span class="o">()</span>:
    ec2_client <span class="o">=</span> boto3.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>
    response <span class="o">=</span> ec2_client.describe_instances<span class="o">()</span>
    
    untagged_instances <span class="o">=</span> <span class="o">[]</span>
    
    <span class="k">for </span>reservation <span class="k">in </span>response[<span class="s1">'Reservations'</span><span class="o">]</span>:
        <span class="k">for </span>instance <span class="k">in </span>reservation[<span class="s1">'Instances'</span><span class="o">]</span>:
            has_owner_tag <span class="o">=</span> False
            <span class="k">if</span> <span class="s1">'Tags'</span> <span class="k">in </span>instance:
                <span class="k">for </span>tag <span class="k">in </span>instance[<span class="s1">'Tags'</span><span class="o">]</span>:
                    <span class="k">if </span>tag[<span class="s1">'Key'</span><span class="o">]</span>.lower<span class="o">()</span> <span class="o">==</span> <span class="s1">'owner'</span>:
                        has_owner_tag <span class="o">=</span> True
                        <span class="nb">break</span>
            
            <span class="k">if </span>not has_owner_tag:
                instance_id <span class="o">=</span> instance[<span class="s1">'InstanceId'</span><span class="o">]</span>
                instance_state <span class="o">=</span> instance[<span class="s1">'State'</span><span class="o">][</span><span class="s1">'Name'</span><span class="o">]</span>
                untagged_instances.append<span class="o">({</span><span class="s1">'InstanceId'</span>: instance_id, <span class="s1">'State'</span>: instance_state<span class="o">})</span>
    
    <span class="k">return </span>untagged_instances

untagged_instances <span class="o">=</span> get_untagged_ec2_instances<span class="o">()</span>
print<span class="o">(</span><span class="s2">"Untagged Instances:"</span>, untagged_instances<span class="o">)</span>

<span class="c"># run script to filter untagged "owner" ec2 </span>
root@c04670a43564:/work# python app-untagged.py 
Untagged Instances: <span class="o">[{</span><span class="s1">'InstanceId'</span>: <span class="s1">'i-076226daa5aaf7cf2'</span>, <span class="s1">'State'</span>: <span class="s1">'stopped'</span><span class="o">}]</span></code></pre></figure>

<ul>
  <li>Create lambda function to list ebs volume snapshots older than 30 days and delete them</li>
</ul>

<p>To achieve this we need :</p>

<ol>
  <li>create lambda IAM role for lambda to manage EBS volume snapshot</li>
  <li>create bellow python lambda function</li>
  <li>zip and upload zip function</li>
  <li>create CloudWatch Event to Trigger run it every 30 days</li>
</ol>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create lambda function to delete snapshots older than 30 days</span>
root@ubt-server:~/pythonwork# vim app-snapshot-older-30days.py
import boto3
from datetime import datetime, timezone, timedelta

def lambda_handler<span class="o">(</span>event, context<span class="o">)</span>:
    ec2_client <span class="o">=</span> boto3.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>
    
    <span class="c"># Get the current time</span>
    now <span class="o">=</span> datetime.now<span class="o">(</span>timezone.utc<span class="o">)</span>
    
    <span class="c"># Define the time threshold</span>
    time_threshold <span class="o">=</span> now - <span class="nb">time </span>delta<span class="o">(</span><span class="nv">days</span><span class="o">=</span>30<span class="o">)</span>
    
    <span class="c"># Describe snapshots</span>
    snapshots <span class="o">=</span> ec2_client.describe_snapshots<span class="o">(</span><span class="nv">OwnerIds</span><span class="o">=[</span><span class="s1">'self'</span><span class="o">])[</span><span class="s1">'Snapshots'</span><span class="o">]</span>
    
    <span class="c"># Filter snapshots older than 30 days</span>
    old_snapshots <span class="o">=</span> <span class="o">[</span>snap <span class="k">for </span>snap <span class="k">in </span>snapshots <span class="k">if </span>snap[<span class="s1">'StartTime'</span><span class="o">]</span> &lt; time_threshold]
    
    <span class="c"># Delete old snapshots</span>
    <span class="k">for </span>snapshot <span class="k">in </span>old_snapshots:
        snapshot_id <span class="o">=</span> snapshot[<span class="s1">'SnapshotId'</span><span class="o">]</span>
        ec2_client.delete_snapshot<span class="o">(</span><span class="nv">SnapshotId</span><span class="o">=</span>snapshot_id<span class="o">)</span>
        print<span class="o">(</span>f<span class="s2">"Deleted snapshot: {snapshot_id}"</span><span class="o">)</span>
    
    <span class="k">return</span> <span class="o">{</span>
        <span class="s1">'statusCode'</span>: 200,
        <span class="s1">'body'</span>: f<span class="s2">"Deleted {len(old_snapshots)} snapshots."</span>
    <span class="o">}</span>

<span class="c"># zip Package for the Lambda Function</span>
root@ubt-server:~/pythonwork# zip <span class="k">function</span>.zip app-snapshot-older-30days.py</code></pre></figure>

<ul>
  <li>Email me when a security group allow inbound SSH (port 22) from everywhere (0.0.0.0/0)</li>
</ul>

<p>To achieve this, we need:</p>

<ol>
  <li>AWS CloudTrail enable</li>
  <li>Create CloudWatch Event Rule to capture AWS CloudTrail logs for security group changes</li>
  <li>Create bellow Lambda Function if inbound allow port 22 from everywhere are met</li>
  <li>Allow CloudWatch Events to Invoke the Lambda Function</li>
  <li>Add the Lambda Function as a Target for the CloudWatch Event Rule</li>
</ol>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~/pythonwork# vim app-sg-allow-22.py
import boto3
import json

def lambda_handler<span class="o">(</span>event, context<span class="o">)</span>:
    <span class="c"># Initialize boto3 clients</span>
    ses_client <span class="o">=</span> boto3.client<span class="o">(</span><span class="s1">'ses'</span><span class="o">)</span>
    ec2_client <span class="o">=</span> boto3.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>
    
    <span class="c"># Email details</span>
    sender <span class="o">=</span> <span class="s1">'zhbsoftboy1@gmail.com'</span>
    recipient <span class="o">=</span> <span class="s1">'zhbsoftboy1@gmail.com'</span>
    subject <span class="o">=</span> <span class="s1">'Security Group Alert: Port 22 Open to everywhere'</span>
    
    <span class="c"># Extract details from the event</span>
    detail <span class="o">=</span> event[<span class="s1">'detail'</span><span class="o">]</span>
    event_name <span class="o">=</span> detail[<span class="s1">'eventName'</span><span class="o">]</span>
    security_group_id <span class="o">=</span> None
    
    <span class="k">if </span>event_name <span class="o">==</span> <span class="s1">'AuthorizeSecurityGroupIngress'</span>:
        security_group_id <span class="o">=</span> detail[<span class="s1">'requestParameters'</span><span class="o">][</span><span class="s1">'groupId'</span><span class="o">]</span>
        ip_permissions <span class="o">=</span> detail[<span class="s1">'requestParameters'</span><span class="o">][</span><span class="s1">'ipPermissions'</span><span class="o">][</span><span class="s1">'items'</span><span class="o">]</span>
    <span class="k">elif </span>event_name <span class="o">==</span> <span class="s1">'CreateSecurityGroup'</span>:
        security_group_id <span class="o">=</span> detail[<span class="s1">'responseElements'</span><span class="o">][</span><span class="s1">'groupId'</span><span class="o">]</span>
        ip_permissions <span class="o">=</span> detail[<span class="s1">'requestParameters'</span><span class="o">][</span><span class="s1">'ipPermissionsEgress'</span><span class="o">][</span><span class="s1">'items'</span><span class="o">]</span>
    
    <span class="c"># Check if port 22 is open to 0.0.0.0/0</span>
    <span class="k">if </span>security_group_id and ip_permissions:
        <span class="k">for </span>permission <span class="k">in </span>ip_permissions:
            <span class="k">if</span> <span class="s1">'ipRanges'</span> <span class="k">in </span>permission:
                <span class="k">for </span>ip_range <span class="k">in </span>permission[<span class="s1">'ipRanges'</span><span class="o">][</span><span class="s1">'items'</span><span class="o">]</span>:
                    <span class="k">if </span>ip_range[<span class="s1">'cidrIp'</span><span class="o">]</span> <span class="o">==</span> <span class="s1">'0.0.0.0/0'</span> and permission[<span class="s1">'fromPort'</span><span class="o">]</span> <span class="o">==</span> 22 and permission[<span class="s1">'toPort'</span><span class="o">]</span> <span class="o">==</span> 22:
                        <span class="c"># Compose email body</span>
                        body_text <span class="o">=</span> <span class="o">(</span>f<span class="s2">"Security Group ID: {security_group_id} has been modified to allow port 22 from everywhere (0.0.0.0/0)."</span><span class="o">)</span>
                        body_html <span class="o">=</span> f<span class="s2">"""&lt;html&gt;
                        &lt;head&gt;&lt;/head&gt;
                        &lt;body&gt;
                          &lt;h1&gt;Security Group Alert&lt;/h1&gt;
                          &lt;p&gt;Security Group ID: &lt;b&gt;{security_group_id}&lt;/b&gt; has been modified to allow port 22 from everywhere (0.0.0.0/0).&lt;/p&gt;
                        &lt;/body&gt;
                        &lt;/html&gt;"""</span>
                        
                        <span class="c"># Send email</span>
                        response <span class="o">=</span> ses_client.send_email<span class="o">(</span>
                            <span class="nv">Source</span><span class="o">=</span>sender,
                            <span class="nv">Destination</span><span class="o">={</span><span class="s1">'ToAddresses'</span>: <span class="o">[</span>recipient]<span class="o">}</span>,
                            <span class="nv">Message</span><span class="o">={</span>
                                <span class="s1">'Subject'</span>: <span class="o">{</span><span class="s1">'Data'</span>: subject<span class="o">}</span>,
                                <span class="s1">'Body'</span>: <span class="o">{</span>
                                    <span class="s1">'Text'</span>: <span class="o">{</span><span class="s1">'Data'</span>: body_text<span class="o">}</span>,
                                    <span class="s1">'Html'</span>: <span class="o">{</span><span class="s1">'Data'</span>: body_html<span class="o">}</span>
                                <span class="o">}</span>
                            <span class="o">}</span>
                        <span class="o">)</span>
                        print<span class="o">(</span>f<span class="s2">"Email sent! Message ID: {response['MessageId']}"</span><span class="o">)</span>
    
    <span class="k">return</span> <span class="o">{</span>
        <span class="s1">'statusCode'</span>: 200,
        <span class="s1">'body'</span>: json.dumps<span class="o">(</span><span class="s1">'Lambda function executed successfully!'</span><span class="o">)</span>
    <span class="o">}</span></code></pre></figure>

<p><b> Conclusion</b></p>

<p>There are many ways to automate AWS tasks using Python Boto3 script. Together with Lambda and trigger, many resource tasks can be scheduled and managed in a scripted way.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Boto3]]></summary></entry><entry><title type="html">Customize Helm Chart for Zack’ Blog</title><link href="http://localhost:4000/jekyll/cat2/2024/05/12/Helm.html" rel="alternate" type="text/html" title="Customize Helm Chart for Zack’ Blog" /><published>2024-05-12T10:15:29+10:00</published><updated>2024-05-12T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/12/Helm</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/12/Helm.html"><![CDATA[<p><b> About Helm </b></p>

<p>Helm is the popular package manager for Kubernetes application deployment, not new to me as I had tried many charts previously with Kafka, and Redis helm charts installation, today I am going to explore how to build my own helm chart for this zack blog, also deep dive into the chart development for advanced templating, together with helm release management and version control, finally integrate my own chart with CI/CD for GitOps.  </p>

<p><b> Common Helm command</b></p>

<ul>
  <li>
    <p>helm list -A   # list releases across all namespaces</p>
  </li>
  <li>
    <p>helm pull bitnami/postgresql-ha –untar  # untar the chart after pull online chart</p>
  </li>
  <li>
    <p>helm repo add bitnami https://charts.bitnami.com/bitnami  # add a repo</p>
  </li>
  <li>
    <p>helm create zackblog-helm  # create a new chart</p>
  </li>
  <li>
    <p>helm install zackblog-helm ~/zackblog-helm -n NAMESPACE -f dev-values.yaml # define ns and override with a new value file</p>
  </li>
  <li>
    <p>helm upgrade zackblog-helm ~/zackblog-helm –set image.repository=<new-image-repository> --set image.tag=<new-image-tag> # --set to upgrade chart with override a new value</new-image-tag></new-image-repository></p>
  </li>
  <li>
    <p>helm lint ~/zackblog-helm  # lint syntax</p>
  </li>
  <li>
    <p>helm rollback zackblog-helm 2   # rollback to revision 2 of a release</p>
  </li>
  <li>
    <p>helm uninstall zackblog-helm -n Production  # uninstall a chart from a ns</p>
  </li>
</ul>

<p><b> Start with own chart</b></p>

<ul>
  <li>create a new helm chart</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@freeipa-server ~]# helm create zackblog-helm
Creating zackblog-helm

<span class="c"># modify values.yaml</span>
<span class="o">[</span>root@freeipa-server zackblog]# vim values.yaml

replicaCount: 3

image:
  repository: zackz001/gitops-jekyll
  pullPolicy: IfNotPresent
  <span class="c"># Overrides the image tag.</span>
  tag: <span class="s2">"latest"</span>

service:
  <span class="nb">type</span>: NodePort
  port: 80</code></pre></figure>

<ul>
  <li>Lint chart syntacx before install</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># lint syntax</span>
<span class="o">[</span>root@freeipa-server ~]# helm lint zackblog-helm/
<span class="o">==&gt;</span> Linting zackblog-helm/
<span class="o">[</span>INFO] Chart.yaml: icon is recommended

1 chart<span class="o">(</span>s<span class="o">)</span> linted, 0 chart<span class="o">(</span>s<span class="o">)</span> failed

<span class="c"># install own chart</span>

<span class="o">[</span>root@freeipa-server ~]# helm <span class="nb">install </span>zackblog-helm zackblog-helm
NAME: zackblog-helm
LAST DEPLOYED: Mon May 13 21:27:14 2024
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  <span class="nb">export </span><span class="nv">NODE_PORT</span><span class="o">=</span><span class="si">$(</span>kubectl get <span class="nt">--namespace</span> default <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.ports[0].nodePort}"</span> services zackblog-helm<span class="si">)</span>
  <span class="nb">export </span><span class="nv">NODE_IP</span><span class="o">=</span><span class="si">$(</span>kubectl get nodes <span class="nt">--namespace</span> default <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[0].status.addresses[0].address}"</span><span class="si">)</span>
  <span class="nb">echo </span>http://<span class="nv">$NODE_IP</span>:<span class="nv">$NODE_PORT</span>

<span class="o">[</span>root@freeipa-server ~]# helm list <span class="nt">-a</span>
NAME            NAMESPACE   REVISION    UPDATED                                     STATUS      CHART               APP VERSION
argo-cd         default     1           2024-04-29 06:14:18.117158759 +0000 UTC     deployed    argo-cd-6.7.17      v2.10.8    
zackblog-helm   default     1           2024-05-13 21:27:14.825391301 +1000 AEST    deployed    zackblog-helm-0.1.0 1.16.0 

<span class="o">[</span>root@freeipa-server ~]# kubectl get deployments.apps | <span class="nb">grep </span>zack
zackblog-helm                              3/3     3            3           113s
<span class="o">[</span>root@freeipa-server ~]# kubectl get svc | <span class="nb">grep </span>zack
zackblog-helm                              NodePort    10.43.90.209    &lt;none&gt;        80:31070/TCP                 2m7s</code></pre></figure>

<ul>
  <li>Customize vaule.yaml by change replica and image tag</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># modify vaule.yaml to scale down and change image to v138</span>
<span class="o">[</span>root@freeipa-server ~]# vim zackblog-helm/values.yaml
replicaCount: 1

image:
  repository: zackz001/gitops-jekyll
  pullPolicy: IfNotPresent
  <span class="c"># Overrides the image tag.</span>
  tag: <span class="s2">"v139"</span>

<span class="o">[</span>root@freeipa-server ~]# helm list <span class="nt">-a</span>
NAME            NAMESPACE   REVISION    UPDATED                                     STATUS      CHART               APP VERSION
argo-cd         default     1           2024-04-29 06:14:18.117158759 +0000 UTC     deployed    argo-cd-6.7.17      v2.10.8    
zackblog-helm   default     2           2024-05-13 21:31:16.523093364 +1000 AEST    deployed    zackblog-helm-0.1.0 1.16.0     
<span class="o">[</span>root@freeipa-server ~]# kubectl get deployments.apps | <span class="nb">grep </span>zack
zackblog-helm                              1/1     1            1           4m31s</code></pre></figure>

<ul>
  <li>Override values.yaml by -f and deploy same chart to different environments</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create a dev ns then deploy and override with dev-values.yaml</span>
<span class="o">[</span>root@freeipa-server ~]# vim zackblog-helm/dev-values.yaml
image:
  repository: zackz001/gitops-jekyll
  tag: v140
replicaCount: 2
service:
  <span class="nb">type</span>: NodePort
  port: 80

<span class="o">[</span>root@freeipa-server ~]# kubectl create ns dev
namespace/dev created
<span class="o">[</span>root@freeipa-server ~]# helm <span class="nb">install </span>dev-zackblog-helm zackblog-helm <span class="nt">-f</span> dev-values.yaml <span class="nt">-n</span> dev
Error: INSTALLATION FAILED: open dev-values.yaml: no such file or directory
<span class="o">[</span>root@freeipa-server ~]# helm <span class="nb">install </span>dev-zackblog-helm zackblog-helm <span class="nt">-f</span> zackblog-helm/dev-values.yaml <span class="nt">-n</span> dev
NAME: dev-zackblog-helm
LAST DEPLOYED: Mon May 13 21:36:39 2024
NAMESPACE: dev
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  <span class="nb">export </span><span class="nv">NODE_PORT</span><span class="o">=</span><span class="si">$(</span>kubectl get <span class="nt">--namespace</span> dev <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.ports[0].nodePort}"</span> services dev-zackblog-helm<span class="si">)</span>
  <span class="nb">export </span><span class="nv">NODE_IP</span><span class="o">=</span><span class="si">$(</span>kubectl get nodes <span class="nt">--namespace</span> dev <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[0].status.addresses[0].address}"</span><span class="si">)</span>
  <span class="nb">echo </span>http://<span class="nv">$NODE_IP</span>:<span class="nv">$NODE_PORT</span>

<span class="o">[</span>root@freeipa-server ~]# kubectl get deployments.apps <span class="nt">-n</span> dev
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
dev-zackblog-helm   2/2     2            2           96s
<span class="o">[</span>root@freeipa-server ~]# kubectl get svc <span class="nt">-n</span> dev
NAME                TYPE       CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>        AGE
dev-zackblog-helm   NodePort   10.43.239.229   &lt;none&gt;        80:31391/TCP   103s</code></pre></figure>

<ul>
  <li>Advanced templating to add pvc into chart</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># add templates/pvc.yaml</span>
<span class="o">[</span>root@freeipa-server ~]# vim zackblog-helm/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhron-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn
  resources:
    requests:
      storage: 1Gi

<span class="c"># add pvc in values.yaml</span>
<span class="o">[</span>root@freeipa-server ~]# vim zackblog-helm/values.yaml
pvc:
  enabled: <span class="nb">true</span>
  templateFiles:
    - pvc.yaml

<span class="c"># add persistentVolumeClaim in templates/deployment.yaml</span>
<span class="o">[</span>root@freeipa-server ~]# vim zackblog-helm/templates/deployment.yaml
...
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: longhron-pvc
...
<span class="o">[</span>root@freeipa-server ~]# helm upgrade zackblog-helm zackblog-helm
Release <span class="s2">"zackblog-helm"</span> has been upgraded. Happy Helming!</code></pre></figure>

<p><b> CICD integration to deploy chart with ArgoCD </b></p>

<p>Now commit and upload “zackblog-helm” folder into github repo, create ArgoCD application manifest to sync with from  path of own helm chart</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># argoCD application manifest</span>
project: default
<span class="nb">source</span>:
  repoURL: <span class="s1">'https://github.com/ZackZhouHB/zack-gitops-project.git'</span>
  path: argo-helm-zackblog
  targetRevision: editing
  helm:
    valueFiles:
      - values.yaml
destination:
  server: <span class="s1">'https://kubernetes.default.svc'</span>
  namespace: helm
syncPolicy:
  automated: <span class="o">{}</span>
  syncOptions:
    - <span class="nv">CreateNamespace</span><span class="o">=</span><span class="nb">true</span></code></pre></figure>

<p><img src="/assets/helm1.png" alt="image tooltip here" /></p>

<p><b> Conclusion</b></p>

<p>Finally, I had a chance to go over helm, it makes package management easier and more convenient, through charts, k8s deployment can be more flexible with values and templates that can be deployed and reusable into different environments, it provides versioning and rollbacks, also allow customization of the template. however using on-line chart can also be risky in a production environment with quality, dependency and security risks.</p>

<p>Overall I think helm chart is a very good way to start deployment into k8s.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Helm]]></summary></entry><entry><title type="html">PostgreSQL: Prod-Grade with k8s Operator</title><link href="http://localhost:4000/jekyll/cat2/2024/05/11/PS4.html" rel="alternate" type="text/html" title="PostgreSQL: Prod-Grade with k8s Operator" /><published>2024-05-11T10:15:29+10:00</published><updated>2024-05-11T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/11/PS4</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/11/PS4.html"><![CDATA[<p><b> Production grade PostgreSQL in K8S</b></p>

<p>Despite all the challenges, in the last 2 years, clever people still managed ways to deploy production-grade database within a Kubernetes cluster by using Kubernetes as a platform to develop custom resource definition (CRDs) like helm charts like bitnami/postgresql-ha, and PostgreSQL Operator like CrunchyData/postgres-operator or zalando/postgres-operator.</p>

<p>Last post I was able to deploy a single PostgreSQL in local k8s, but I had to manually create Kubernetes namespaces, define database creds, configuration and environment variables into k8s secret and configmap, also to define the statefulset yaml with volume claim template.</p>

<p>Still I was not able to configure HA and failover as I found it is so limited and a headache within K8S if only relying on statefulset. Luckily there are engineers out there to develop helm and opeartor to get the job done.</p>

<p><b> CrunchyData Postgres-Operator</b></p>

<p>In this session, I will follow bellow steps to</p>

<ul>
  <li>Deploy PostgreSQL Operator</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Clone the CrunchyData Postgres Operator</span>
<span class="o">[</span>root@freeipa-server ~]# git clone https://github.com/CrunchyData/postgres-operator-examples.git

<span class="c"># create namespace and deploy GPO Postgres Operatorusing kustomize</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl apply <span class="nt">-k</span> kustomize/install/namespace
namespace/postgres-operator created
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl apply <span class="nt">--server-side</span> <span class="nt">-k</span> kustomize/install/default
customresourcedefinition.apiextensions.k8s.io/pgadmins.postgres-operator.crunchydata.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/pgupgrades.postgres-operator.crunchydata.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/postgresclusters.postgres-operator.crunchydata.com serverside-applied
serviceaccount/pgo serverside-applied
clusterrole.rbac.authorization.k8s.io/postgres-operator serverside-applied
clusterrolebinding.rbac.authorization.k8s.io/postgres-operator serverside-applied
deployment.apps/pgo serverside-applied

<span class="c"># validate deploy status</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl get all <span class="nt">-n</span> postgres-operator
NAME                      READY   STATUS    RESTARTS   AGE
pod/pgo-77d6b49b8-wrdjp   1/1     Running   0          2m47s

NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pgo   1/1     1            1           2m47s

NAME                            DESIRED   CURRENT   READY   AGE
replicaset.apps/pgo-77d6b49b8   1         1         1       2m47s</code></pre></figure>

<ul>
  <li>Deploy HA PostgreSQL Cluster</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Create a Postgres Cluster named "hippo" in "postgres-operator" ns</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl apply <span class="nt">-k</span> kustomize/postgres
postgrescluster.postgres-operator.crunchydata.com/hippo created

<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl get all <span class="nt">-n</span> postgres-operator
NAME                          READY   STATUS    RESTARTS   AGE
pod/hippo-backup-dvks-m4z5m   1/1     Running   0          56s
pod/hippo-instance1-582s-0    4/4     Running   0          2m14s
pod/hippo-repo-host-0         2/2     Running   0          2m14s
pod/pgo-77d6b49b8-wrdjp       1/1     Running   0          6m38s

NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>    AGE
service/hippo-ha          ClusterIP   10.43.249.159   &lt;none&gt;        5432/TCP   2m14s
service/hippo-ha-config   ClusterIP   None            &lt;none&gt;        &lt;none&gt;     2m14s
service/hippo-pods        ClusterIP   None            &lt;none&gt;        &lt;none&gt;     2m14s
service/hippo-primary     ClusterIP   None            &lt;none&gt;        5432/TCP   2m14s
service/hippo-replicas    ClusterIP   10.43.17.57     &lt;none&gt;        5432/TCP   2m14s

NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pgo   1/1     1            1           6m38s

NAME                            DESIRED   CURRENT   READY   AGE
replicaset.apps/pgo-77d6b49b8   1         1         1       6m38s

NAME                                    READY   AGE
statefulset.apps/hippo-instance1-582s   1/1     2m14s
statefulset.apps/hippo-repo-host        1/1     2m14s

NAME                          COMPLETIONS   DURATION   AGE
job.batch/hippo-backup-dvks   0/1           56s        56s

<span class="c"># retrieve database password from Kubernetes secret</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl get secret hippo-pguser-hippo <span class="nt">-n</span> postgres-operator <span class="nt">-o</span><span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.data.password}'</span> | <span class="nb">base64</span> <span class="nt">--decode</span>
jZiBWXMGRiEOA6wAEj<span class="p">;</span>lRhsM</code></pre></figure>

<p>Connect an application to PostgreSQL cluster</p>

<p>Here we use Keycloak, a popular open-source identity management tool that is backed by a PostgreSQL database. Using the hippo cluster we created, we can deploy the following manifest file</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create deployment keycloak to connect PostgreSQL database</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# vim kustomize/keycloak/keycloak.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: keycloak
  namespace: postgres-operator
  labels:
    app.kubernetes.io/name: keycloak
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: keycloak
  template:
    metadata:
      labels:
        app.kubernetes.io/name: keycloak
    spec:
      containers:
      - image: quay.io/keycloak/keycloak:latest
        args: <span class="o">[</span><span class="s2">"start-dev"</span><span class="o">]</span>
        name: keycloak
        <span class="nb">env</span>:
        - name: DB_VENDOR
          value: <span class="s2">"postgres"</span>
        - name: DB_ADDR
          valueFrom: <span class="o">{</span> secretKeyRef: <span class="o">{</span> name: hippo-pguser-hippo, key: host <span class="o">}</span> <span class="o">}</span>
        - name: DB_PORT
          valueFrom: <span class="o">{</span> secretKeyRef: <span class="o">{</span> name: hippo-pguser-hippo, key: port <span class="o">}</span> <span class="o">}</span>
        - name: DB_DATABASE
          valueFrom: <span class="o">{</span> secretKeyRef: <span class="o">{</span> name: hippo-pguser-hippo, key: dbname <span class="o">}</span> <span class="o">}</span>
        - name: DB_USER
          valueFrom: <span class="o">{</span> secretKeyRef: <span class="o">{</span> name: hippo-pguser-hippo, key: user <span class="o">}</span> <span class="o">}</span>
        - name: DB_PASSWORD
          valueFrom: <span class="o">{</span> secretKeyRef: <span class="o">{</span> name: hippo-pguser-hippo, key: password <span class="o">}</span> <span class="o">}</span>
        - name: KEYCLOAK_ADMIN
          value: <span class="s2">"admin"</span>
        - name: KEYCLOAK_ADMIN_PASSWORD
          value: <span class="s2">"admin"</span>
        - name: KC_PROXY
          value: <span class="s2">"edge"</span>
        ports:
        - name: http
          containerPort: 8080
        - name: https
          containerPort: 8443
        readinessProbe:
          httpGet:
            path: /realms/master
            port: 8080
      restartPolicy: Always

<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl apply <span class="nt">-f</span> kustomize/keycloak/keycloak.yaml
deployment.apps/keycloak created

<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl get deployment <span class="nt">-n</span> postgres-operator 
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
keycloak   1/1     1            1           4m27s
pgo        1/1     1            1           176m</code></pre></figure>

<ul>
  <li>Scale Up / Down</li>
</ul>

<p>Edit manifest to add 2 more replicas</p>

<p><img src="/assets/ps3-1.png" alt="image tooltip here" /></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@freeipa-server kustomize]# kubectl apply <span class="nt">-k</span> postgres <span class="nt">-n</span> postgres-operator
postgrescluster.postgres-operator.crunchydata.com/hippo configured
<span class="c"># watch change</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# watch kubectl get pod <span class="nt">-L</span> postgres-operator.crunchydata.com/role <span class="nt">-l</span> postgres-operator.crunchydata.com/instance <span class="nt">-n</span> postgres-operator</code></pre></figure>

<p><img src="/assets/ps3-2.png" alt="image tooltip here" /></p>

<p>Failover testing:</p>

<p>Now I am going to delete the primary instance, one of the standby pod will take over and become primary automatically</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># delete the primary pod hippo-instance1-nhbc-0, then previous replica pod hippo-instance1-q8kk-0 promoted as master</span>
<span class="c"># pod hippo-instance1-nhbc-0 will up again as a replica</span>
<span class="o">[</span>root@freeipa-server kustomize]# kubectl delete po hippo-instance1-nhbc-0 <span class="nt">-n</span> postgres-operator 
pod <span class="s2">"hippo-instance1-nhbc-0"</span> deleted</code></pre></figure>

<p><img src="/assets/ps3-3.png" alt="image tooltip here" /></p>

<ul>
  <li>Perform Minor version rolling upgrade</li>
</ul>

<p>Here I changed the database version to 16.1, the cluster will start a rolling update by</p>

<ol>
  <li>
    <p>Applying new version to one of the standby pod first</p>
  </li>
  <li>
    <p>Then update another replica pod</p>
  </li>
  <li>
    <p>Promote the first upgraded replica as master</p>
  </li>
  <li>
    <p>Lastly the previous master pod will be updated and become a replica</p>
  </li>
</ol>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># validate DB version before miner upgrade</span>
<span class="o">[</span>root@freeipa-server kustomize]# kubectl <span class="nb">exec</span> <span class="nt">-it</span> hippo-instance1-q8kk-0 <span class="nt">-n</span> postgres-operator <span class="nt">--</span> psql <span class="nt">--version</span>
Defaulted container <span class="s2">"database"</span> out of: database, replication-cert-copy, pgbackrest, pgbackrest-config, postgres-startup <span class="o">(</span>init<span class="o">)</span>, nss-wrapper-init <span class="o">(</span>init<span class="o">)</span>
psql <span class="o">(</span>PostgreSQL<span class="o">)</span> 16.2</code></pre></figure>

<p><img src="/assets/ps3-5.png" alt="image tooltip here" /></p>

<p><img src="/assets/ps3-4.png" alt="image tooltip here" /></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># validate DB version after miner version change</span>
<span class="o">[</span>root@freeipa-server kustomize]# kubectl <span class="nb">exec</span> <span class="nt">-it</span> hippo-instance1-q8kk-0 <span class="nt">-n</span> postgres-operator <span class="nt">--</span> psql <span class="nt">--version</span>
Defaulted container <span class="s2">"database"</span> out of: database, replication-cert-copy, pgbackrest, pgbackrest-config, postgres-startup <span class="o">(</span>init<span class="o">)</span>, nss-wrapper-init <span class="o">(</span>init<span class="o">)</span>
psql <span class="o">(</span>PostgreSQL<span class="o">)</span> 16.1</code></pre></figure>

<ul>
  <li>Backup</li>
</ul>

<p>Add backup Cron job into manifest to add weekly full backup and daily incremental</p>

<p><img src="/assets/ps3-6.png" alt="image tooltip here" /></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@freeipa-server ~]# kubectl get cronjobs <span class="nt">-n</span> postgres-operator 
NAME               SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hippo-repo1-full   0 1 <span class="k">*</span> <span class="k">*</span> 0     False     0        &lt;none&gt;          5m21s
hippo-repo1-incr   0 1 <span class="k">*</span> <span class="k">*</span> 1-6   False     0        &lt;none&gt;          5m21s</code></pre></figure>

<ul>
  <li>Deploy Monitoring (Prom + Grafaba)</li>
</ul>

<p>Finally, let’s set up the monitoring stack for PostgreSQL by using Pormthues and Grafana.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># deploy monitoring stack</span>
<span class="o">[</span>root@freeipa-server kustomize]# kubectl apply <span class="nt">-k</span> monitoring
serviceaccount/alertmanager created
serviceaccount/grafana created
serviceaccount/prometheus created
clusterrole.rbac.authorization.k8s.io/prometheus created
clusterrolebinding.rbac.authorization.k8s.io/prometheus created
configmap/alert-rules-config created
configmap/alertmanager-config created
configmap/crunchy-prometheus created
configmap/grafana-dashboards created
configmap/grafana-datasources created
secret/grafana-admin created
service/crunchy-alertmanager created
service/crunchy-grafana created
service/crunchy-prometheus created
persistentvolumeclaim/alertmanagerdata created
persistentvolumeclaim/grafanadata created
persistentvolumeclaim/prometheusdata created
deployment.apps/crunchy-alertmanager created
deployment.apps/crunchy-grafana created
deployment.apps/crunchy-prometheus created
<span class="c"># Edit Grafana service to NodePort</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl edit svc crunchy-grafana <span class="nt">-n</span> postgres-operator
service/crunchy-grafana edited
<span class="c"># exec into master database container, using pgbench to generate tables</span>
<span class="o">[</span>root@freeipa-server postgres-operator-examples]# kubectl <span class="nb">exec</span> <span class="nt">-it</span> hippo-instance1-nhbc-0 <span class="nt">-c</span> database <span class="nt">-n</span> postgres-operator <span class="nt">--</span> bash

bash-4.4<span class="nv">$ </span>pgbench <span class="nt">-i</span> <span class="nt">-s</span> 100 <span class="nt">-U</span> postgres <span class="nt">-d</span> postgres
dropping old tables...
NOTICE:  table <span class="s2">"pgbench_accounts"</span> does not exist, skipping
NOTICE:  table <span class="s2">"pgbench_branches"</span> does not exist, skipping
NOTICE:  table <span class="s2">"pgbench_history"</span> does not exist, skipping
NOTICE:  table <span class="s2">"pgbench_tellers"</span> does not exist, skipping
creating tables...
generating data <span class="o">(</span>client-side<span class="o">)</span>...
10000000 of 10000000 tuples <span class="o">(</span>100%<span class="o">)</span> <span class="k">done</span> <span class="o">(</span>elapsed 45.61 s, remaining 0.00 s<span class="o">)</span></code></pre></figure>

<p><img src="/assets/ps3-7.png" alt="image tooltip here" /></p>

<p>Some Grafana predefined PostgreSQL dashboard, unfortunately I donot have much data in it to show more monitoring status.</p>

<p><img src="/assets/ps3-8.png" alt="image tooltip here" /></p>

<p><img src="/assets/ps3-9.png" alt="image tooltip here" /></p>

<p><b> Conclusion</b></p>

<p>This is the final session of this PostgreSQL series, together I have explored PostgreSQL from very basic docker deployment with replica, to production-grade deployment in Kubernetes using operator, practise from backup, monitoring, rolling update, to HA, failover and scale up. HAHA!</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Production grade PostgreSQL in K8S]]></summary></entry><entry><title type="html">PostgreSQL: Deploy into K8S</title><link href="http://localhost:4000/jekyll/cat2/2024/05/10/PS3.html" rel="alternate" type="text/html" title="PostgreSQL: Deploy into K8S" /><published>2024-05-10T10:15:29+10:00</published><updated>2024-05-10T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/10/PS3</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/10/PS3.html"><![CDATA[<p><b> Single PostgreSQL deployment in K8S</b></p>

<p>PostgreSQL by default doesnot build for kubernetes, and a database with statefulset workload in k8s can be brutal to manage. In my lab k8s cluster, here we create namespace, secret, configuremap, PVC and statefulset to run a single PostgreSQL</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create namespace "postgresql"</span>
<span class="o">[</span>root@freeipa-server ~]# kubectl create ns postgresql
namespace/postgresql created

<span class="c"># create secret to store database creds</span>
<span class="o">[</span>root@freeipa-server ~]# kubectl <span class="nt">-n</span> postgresql create secret generic postgresql <span class="nt">--from-literal</span> <span class="nv">POSTGRES_USER</span><span class="o">=</span><span class="s2">"postgresadmin"</span> <span class="nt">--from-literal</span> <span class="nv">POSTGRES_PASSWORD</span><span class="o">=</span><span class="s1">'admin123'</span> <span class="nt">--from-literal</span> <span class="nv">POSTGRES_DB</span><span class="o">=</span><span class="s2">"postgresdb"</span> <span class="nt">--from-literal</span> <span class="nv">REPLICATION_USER</span><span class="o">=</span><span class="s2">"replicationuser"</span> <span class="nt">--from-literal</span> <span class="nv">REPLICATION_PASSWORD</span><span class="o">=</span><span class="s1">'replicationPassword'</span>
secret/postgresql created

<span class="c"># create configmap, pvc, statefulset with init container to run postgresql</span>
<span class="o">[</span>root@freeipa-server ~]# vim stateful.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres
data: 
  pg_hba.conf: |+
    <span class="c"># TYPE  DATABASE        USER            ADDRESS                 METHOD</span>
    host     replication     replicationuser         0.0.0.0/0        md5
    <span class="c"># "local" is for Unix domain socket connections only</span>
    <span class="nb">local   </span>all             all                                     trust
    <span class="c"># IPv4 local connections:</span>
    host    all             all             127.0.0.1/32            trust
    <span class="c"># IPv6 local connections:</span>
    host    all             all             ::1/128                 trust
    <span class="c"># Allow replication connections from localhost, by a user with the</span>
    <span class="c"># replication privilege.</span>
    <span class="nb">local   </span>replication     all                                     trust
    host    replication     all             127.0.0.1/32            trust
    host    replication     all             ::1/128                 trust

    host all all all scram-sha-256
  postgresql.conf: |+
    data_directory <span class="o">=</span> <span class="s1">'/data/pgdata'</span>
    hba_file <span class="o">=</span> <span class="s1">'/config/pg_hba.conf'</span>
    ident_file <span class="o">=</span> <span class="s1">'/config/pg_ident.conf'</span>

    port <span class="o">=</span> 5432
    listen_addresses <span class="o">=</span> <span class="s1">'*'</span>
    max_connections <span class="o">=</span> 100
    shared_buffers <span class="o">=</span> 128MB
    dynamic_shared_memory_type <span class="o">=</span> posix
    max_wal_size <span class="o">=</span> 1GB
    min_wal_size <span class="o">=</span> 80MB
    log_timezone <span class="o">=</span> <span class="s1">'Etc/UTC'</span>
    datestyle <span class="o">=</span> <span class="s1">'iso, mdy'</span>
    timezone <span class="o">=</span> <span class="s1">'Etc/UTC'</span>

    <span class="c">#locale settings</span>
    lc_messages <span class="o">=</span> <span class="s1">'en_US.utf8'</span>			<span class="c"># locale for system error message</span>
    lc_monetary <span class="o">=</span> <span class="s1">'en_US.utf8'</span>			<span class="c"># locale for monetary formatting</span>
    lc_numeric <span class="o">=</span> <span class="s1">'en_US.utf8'</span>			<span class="c"># locale for number formatting</span>
    lc_time <span class="o">=</span> <span class="s1">'en_US.utf8'</span>				<span class="c"># locale for time formatting</span>

    default_text_search_config <span class="o">=</span> <span class="s1">'pg_catalog.english'</span>

    <span class="c">#replication</span>
    wal_level <span class="o">=</span> replica
    archive_mode <span class="o">=</span> on
    archive_command <span class="o">=</span> <span class="s1">'test ! -f /data/archive/%f &amp;&amp; cp %p /data/archive/%f'</span>
    max_wal_senders <span class="o">=</span> 3
<span class="nt">---</span>
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  selector:
    matchLabels:
      app: postgres
  serviceName: <span class="s2">"postgres"</span>
  replicas: 1
  template:
    metadata:
      labels:
        app: postgres
    spec:
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: init
        image: postgres:15.0
        <span class="nb">command</span>: <span class="o">[</span> <span class="s2">"bash"</span>, <span class="s2">"-c"</span> <span class="o">]</span>
        args:
        - |
          <span class="c">#create archive directory</span>
          <span class="nb">mkdir</span> <span class="nt">-p</span> /data/archive <span class="o">&amp;&amp;</span> <span class="nb">chown</span> <span class="nt">-R</span> 999:999 /data/archive
        volumeMounts:
        - name: data
          mountPath: /data
          readOnly: <span class="nb">false
      </span>containers:
      - name: postgres
        image: postgres:15.0
        args: <span class="o">[</span><span class="s2">"-c"</span>, <span class="s2">"config_file=/config/postgresql.conf"</span><span class="o">]</span>
        ports:
        - containerPort: 5432
          name: database
        <span class="nb">env</span>:
        - name: PGDATA
          value: <span class="s2">"/data/pgdata"</span>
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: postgresql
              key: POSTGRES_USER
              optional: <span class="nb">false</span>
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgresql
              key: POSTGRES_PASSWORD
              optional: <span class="nb">false</span>
        - name: POSTGRES_DB
          valueFrom:
            secretKeyRef:
              name: postgresql
              key: POSTGRES_DB
              optional: <span class="nb">false
        </span>volumeMounts:
        - name: config
          mountPath: /config
          readOnly: <span class="nb">false</span>
        - name: data
          mountPath: /data
          readOnly: <span class="nb">false
      </span>volumes:
      - name: config
        configMap:
          name: postgres
          defaultMode: 0755
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: <span class="o">[</span> <span class="s2">"ReadWriteOnce"</span> <span class="o">]</span>
      storageClassName: <span class="s2">"standard"</span>
      resources:
        requests:
          storage: 100Mi
<span class="nt">---</span>
apiVersion: v1
kind: Service
metadata:
  name: postgres
  labels:
    app: postgres
spec:
  ports:
  - port: 5432
    targetPort: 5432
    name: postgres
  clusterIP: None
  selector:
    app: postgres

<span class="o">[</span>root@freeipa-server ~]# kubectl create <span class="nt">-f</span> stateful.yaml <span class="nt">-n</span> postgresql 
configmap/postgres created
statefulset.apps/postgres created
service/postgres created

<span class="c"># validate for pvc, pods</span>
<span class="o">[</span>root@freeipa-server ~]# kubectl get pvc <span class="nt">-n</span> postgresql 
NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-postgres-0   Bound    pvc-dd89fc0a-915f-40eb-b61f-917234074a61   100Mi      RWO            longhorn       19m

<span class="o">[</span>root@freeipa-server ~]# kubectl get all <span class="nt">-n</span> postgresql 
NAME             READY   STATUS    RESTARTS   AGE
pod/postgres-0   1/1     Running   0          6m29s

NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>    AGE
service/postgres   ClusterIP   None         &lt;none&gt;        5432/TCP   6m29s

NAME                        READY   AGE
statefulset.apps/postgres   1/1     6m29s

<span class="c"># check container logs for database connection status</span>
<span class="o">[</span>root@freeipa-server ~]# kubectl logs <span class="nt">-n</span> postgresql postgres-0 
Defaulted container <span class="s2">"postgres"</span> out of: postgres, init <span class="o">(</span>init<span class="o">)</span>
The files belonging to this database system will be owned by user <span class="s2">"postgres"</span><span class="nb">.</span>
This user must also own the server process.

The database cluster will be initialized with locale <span class="s2">"en_US.utf8"</span><span class="nb">.</span>
The default database encoding has accordingly been <span class="nb">set </span>to <span class="s2">"UTF8"</span><span class="nb">.</span>
The default text search configuration will be <span class="nb">set </span>to <span class="s2">"english"</span><span class="nb">.</span>

Data page checksums are disabled.

fixing permissions on existing directory /data/pgdata ... ok
creating subdirectories ... ok
selecting dynamic shared memory implementation ... posix
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
selecting default <span class="nb">time </span>zone ... Etc/UTC
creating configuration files ... ok
running bootstrap script ... ok
performing post-bootstrap initialization ... ok
initdb: warning: enabling <span class="s2">"trust"</span> authentication <span class="k">for </span><span class="nb">local </span>connections
initdb: hint: You can change this by editing pg_hba.conf or using the option <span class="nt">-A</span>, or <span class="nt">--auth-local</span> and <span class="nt">--auth-host</span>, the next <span class="nb">time </span>you run initdb.
syncing data to disk ... ok


Success. You can now start the database server using:

    pg_ctl <span class="nt">-D</span> /data/pgdata <span class="nt">-l</span> logfile start

waiting <span class="k">for </span>server to start....2024-05-12 00:52:56.718 UTC <span class="o">[</span>49] LOG:  starting PostgreSQL 15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">)</span> on x86_64-pc-linux-gnu, compiled by gcc <span class="o">(</span>Debian 10.2.1-6<span class="o">)</span> 10.2.1 20210110, 64-bit
2024-05-12 00:52:56.719 UTC <span class="o">[</span>49] LOG:  listening on Unix socket <span class="s2">"/var/run/postgresql/.s.PGSQL.5432"</span>
2024-05-12 00:52:56.730 UTC <span class="o">[</span>49] LOG:  could not open usermap file <span class="s2">"/config/pg_ident.conf"</span>: No such file or directory
2024-05-12 00:52:56.733 UTC <span class="o">[</span>52] LOG:  database system was shut down at 2024-05-12 00:52:55 UTC
2024-05-12 00:52:56.744 UTC <span class="o">[</span>49] LOG:  database system is ready to accept connections
 <span class="k">done
</span>server started
CREATE DATABASE


/usr/local/bin/docker-entrypoint.sh: ignoring /docker-entrypoint-initdb.d/<span class="k">*</span>

2024-05-12 00:52:56.957 UTC <span class="o">[</span>49] LOG:  received fast shutdown request
waiting <span class="k">for </span>server to shut down....2024-05-12 00:52:56.961 UTC <span class="o">[</span>49] LOG:  aborting any active transactions
2024-05-12 00:52:56.962 UTC <span class="o">[</span>49] LOG:  background worker <span class="s2">"logical replication launcher"</span> <span class="o">(</span>PID 56<span class="o">)</span> exited with <span class="nb">exit </span>code 1
2024-05-12 00:52:56.963 UTC <span class="o">[</span>50] LOG:  shutting down
2024-05-12 00:52:57.042 UTC <span class="o">[</span>50] LOG:  checkpoint starting: shutdown immediate
..2024-05-12 00:52:59.314 UTC <span class="o">[</span>50] LOG:  checkpoint <span class="nb">complete</span>: wrote 918 buffers <span class="o">(</span>5.6%<span class="o">)</span><span class="p">;</span> 0 WAL file<span class="o">(</span>s<span class="o">)</span> added, 0 removed, 1 recycled<span class="p">;</span> <span class="nv">write</span><span class="o">=</span>0.434 s, <span class="nb">sync</span><span class="o">=</span>0.014 s, <span class="nv">total</span><span class="o">=</span>2.279 s<span class="p">;</span> <span class="nb">sync </span><span class="nv">files</span><span class="o">=</span>250, <span class="nv">longest</span><span class="o">=</span>0.007 s, <span class="nv">average</span><span class="o">=</span>0.001 s<span class="p">;</span> <span class="nv">distance</span><span class="o">=</span>11271 kB, <span class="nv">estimate</span><span class="o">=</span>11271 kB
2024-05-12 00:52:59.318 UTC <span class="o">[</span>49] LOG:  database system is shut down
 <span class="k">done
</span>server stopped

PostgreSQL init process <span class="nb">complete</span><span class="p">;</span> ready <span class="k">for </span>start up.

2024-05-12 00:52:59.385 UTC <span class="o">[</span>1] LOG:  starting PostgreSQL 15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">)</span> on x86_64-pc-linux-gnu, compiled by gcc <span class="o">(</span>Debian 10.2.1-6<span class="o">)</span> 10.2.1 20210110, 64-bit
2024-05-12 00:52:59.385 UTC <span class="o">[</span>1] LOG:  listening on IPv4 address <span class="s2">"0.0.0.0"</span>, port 5432
2024-05-12 00:52:59.385 UTC <span class="o">[</span>1] LOG:  listening on IPv6 address <span class="s2">"::"</span>, port 5432
2024-05-12 00:52:59.389 UTC <span class="o">[</span>1] LOG:  listening on Unix socket <span class="s2">"/var/run/postgresql/.s.PGSQL.5432"</span>
2024-05-12 00:52:59.398 UTC <span class="o">[</span>1] LOG:  could not open usermap file <span class="s2">"/config/pg_ident.conf"</span>: No such file or directory
2024-05-12 00:52:59.404 UTC <span class="o">[</span>67] LOG:  database system was shut down at 2024-05-12 00:52:59 UTC
2024-05-12 00:52:59.415 UTC <span class="o">[</span>1] LOG:  database system is ready to accept connections</code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we are able to deploy a PostgreSQL in local k8s cluster, with defined environment variables in kubernetes secret and configmap, together with init container to create data archive volume in presistent storage class, the next blog I will discover how to run PostgreSQL HA with presistent volume on kubernetes with both Helm and operater, then validate scale up and down, backup using cronjob and etc.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Single PostgreSQL deployment in K8S]]></summary></entry><entry><title type="html">PostgreSQL: Replication &amp;amp; Failover</title><link href="http://localhost:4000/jekyll/cat2/2024/05/08/PS2.html" rel="alternate" type="text/html" title="PostgreSQL: Replication &amp;amp; Failover" /><published>2024-05-08T10:15:29+10:00</published><updated>2024-05-08T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/08/PS2</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/08/PS2.html"><![CDATA[<p><b> The Replication</b></p>

<p>Last post I started hands-on session with PostgreSQL installation on both docker and docker-compose, explored important PostgreSQL configuration and was able to mount persistent volumes and config files to customize PostgreSQL.</p>

<p>In this post I will setup a second PostgreSQL instance to setup a primaty and standby replication for PostgreSQL HA, by using some pg tools, last we will test failover by shut down primary and promot standby instance.</p>

<p><img src="/assets/ps2-1.png" alt="image tooltip here" /></p>

<p>To achieve this, steps can be followed by:  </p>

<ul>
  <li>Setup docker network and Create Replication User in Primary instance</li>
</ul>

<p>To establish PostgreSQL replication, it is necessary to set unique data volumes for data between instances and unique config files for each instance.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create both primary and standby folders</span>
root@ubt-server:~# <span class="nb">mkdir </span>postgres-1
root@ubt-server:~# <span class="nb">mkdir </span>postgres-2

<span class="c"># move previous post config file to postgres-1 and postgres-2 </span>
root@ubt-server:~# <span class="nb">cp</span> <span class="nt">-r</span> config/<span class="k">*</span> postgres-1/config/
root@ubt-server:~# <span class="nb">mv </span>config/<span class="k">*</span> postgres-2/config/

<span class="c"># create docker network so PostgreSQL containers on the same network</span>
root@ubt-server:~/postgres-1/config# docker network create postgres
9891c6d9cd3bdbeea2fdfc2b287c868a0f67a3cec7f2939e1299cfb0ae293021

<span class="c"># run primary </span>
root@ubt-server:~/postgres-1# docker run <span class="nt">-it</span> <span class="nt">--rm</span> <span class="nt">--name</span> postgres-1 <span class="se">\</span>
<span class="nt">--net</span> postgres <span class="se">\</span>
<span class="nt">-e</span> <span class="nv">POSTGRES_USER</span><span class="o">=</span>postgresadmin <span class="se">\</span>
<span class="nt">-e</span> <span class="nv">POSTGRES_PASSWORD</span><span class="o">=</span>admin123 <span class="se">\</span>
<span class="nt">-e</span> <span class="nv">POSTGRES_DB</span><span class="o">=</span>postgresdb <span class="se">\</span>
<span class="nt">-e</span> <span class="nv">PGDATA</span><span class="o">=</span><span class="s2">"/data"</span> <span class="se">\</span>
<span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/postgres-1/pgdata:/data <span class="se">\</span>
<span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/postgres-1/config:/config <span class="se">\</span>
<span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/postgres-1/archive:/mnt/server/archive <span class="se">\</span>
<span class="nt">-p</span> 5000:5432 postgres:15.0 <span class="se">\</span>
<span class="nt">-c</span> <span class="s1">'config_file=/config/postgresql.conf'</span>

2024-05-11 13:15:07.762 UTC <span class="o">[</span>1] LOG:  starting PostgreSQL 15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">)</span> on x86_64-pc-linux-gnu, compiled by gcc <span class="o">(</span>Debian 10.2.1-6<span class="o">)</span> 10.2.1 20210110, 64-bit
2024-05-11 13:15:07.763 UTC <span class="o">[</span>1] LOG:  listening on IPv4 address <span class="s2">"0.0.0.0"</span>, port 5432
2024-05-11 13:15:07.763 UTC <span class="o">[</span>1] LOG:  listening on IPv6 address <span class="s2">"::"</span>, port 5432
2024-05-11 13:15:07.764 UTC <span class="o">[</span>1] LOG:  listening on Unix socket <span class="s2">"/var/run/postgresql/.s.PGSQL.5432"</span>
2024-05-11 13:15:07.766 UTC <span class="o">[</span>63] LOG:  database system was shut down at 2024-05-11 13:15:07 UTC
2024-05-11 13:15:07.768 UTC <span class="o">[</span>1] LOG:  database system is ready to accept connections

<span class="c"># create Replication User, chown postgres to access archive folder</span>
root@ubt-server:~# docker <span class="nb">exec</span> <span class="nt">-it</span> postgres-1 bash
root@2bf6be3e4fa8:/# createuser <span class="nt">-U</span> postgresadmin <span class="nt">-P</span> <span class="nt">-c</span> 5 <span class="nt">--replication</span> replicationUser
Enter password <span class="k">for </span>new role: 
Enter it again: 
root@2bf6be3e4fa8:/# <span class="nb">chown </span>postgres:postgres /mnt/server/archive

<span class="c"># add replication into configration file</span>
root@ubt-server:~/postgres-1/config# vim pg_hba.conf
<span class="c"># TYPE  DATABASE        USER            ADDRESS                 METHOD</span>
<span class="c"># add replication user</span>
host     replication     replicationUser         0.0.0.0/0        md5</code></pre></figure>

<ul>
  <li>Enable Write-Ahead Log, archive and Replication</li>
</ul>

<p>Write-Ahead Log (WAL) is a PostgreSQL data integrity mechanism of writing transaction logs to file and does not accept the transaction until it has been written to the transaction log and flushed to disk. This ensures that if there is a crash in the system, that the database can be recovered from the transaction log.</p>

<p>So we need to add bellow lines into postgresql.conf to enable Write-Ahead Log and replica and archive</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~/postgres-1/config# vim postgresql.conf

<span class="c">#replication</span>
wal_level <span class="o">=</span> replica
archive_mode <span class="o">=</span> on
archive_command <span class="o">=</span> <span class="s1">'test ! -f /mnt/server/archive/%f &amp;&amp; cp %p /mnt/server/archive/%f'</span>
max_wal_senders <span class="o">=</span> 3</code></pre></figure>

<ul>
  <li>set up standby instance and validate replication
here we need to use tool “pgbase_backup” to create standby instance by taking a primary instance base backup, type “replicationUser” passwd, then postgres-1 database will be back up into postgres-2 pgdata folder</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~# docker run <span class="nt">-it</span> <span class="nt">--rm</span> <span class="nt">--net</span> postgres <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/postgres-2/pgdata:/data <span class="nt">--entrypoint</span> /bin/bash postgres:15.0

root@56e38636a87b:/# pg_basebackup <span class="nt">-h</span> postgres-1 <span class="nt">-p</span> 5432 <span class="nt">-U</span> replicationUser <span class="nt">-D</span> /data/ <span class="nt">-Fp</span> <span class="nt">-Xs</span> <span class="nt">-R</span>
Password: </code></pre></figure>

<p>Now, we start the standby instance. See the log below. Postgres-2 is entering standby mode, ready to accept read-only connections, and starting streaming WAL from the primary.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~# docker run <span class="nt">-it</span> <span class="nt">--rm</span> <span class="nt">--name</span> postgres-2 <span class="nt">--net</span> postgres <span class="nt">-e</span> <span class="nv">POSTGRES_USER</span><span class="o">=</span>postgresadmin <span class="nt">-e</span> <span class="nv">POSTGRES_PASSWORD</span><span class="o">=</span>admin123 <span class="nt">-e</span> <span class="nv">POSTGRES_DB</span><span class="o">=</span>postgresdb <span class="nt">-e</span> <span class="nv">PGDATA</span><span class="o">=</span><span class="s2">"/data"</span> <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/postgres-2/pgdata:/data <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/postgres-2/config:/config <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/postgres-2/archive:/mnt/server/archive <span class="nt">-p</span> 5001:5432 postgres:15.0 <span class="nt">-c</span> <span class="s1">'config_file=/config/postgresql.conf'</span>

PostgreSQL Database directory appears to contain a database<span class="p">;</span> Skipping initialization

2024-05-11 14:25:21.008 UTC <span class="o">[</span>1] LOG:  starting PostgreSQL 15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">)</span> on x86_64-pc-linux-gnu, compiled by gcc <span class="o">(</span>Debian 10.2.1-6<span class="o">)</span> 10.2.1 20210110, 64-bit
2024-05-11 14:25:21.008 UTC <span class="o">[</span>1] LOG:  listening on IPv4 address <span class="s2">"0.0.0.0"</span>, port 5432
2024-05-11 14:25:21.008 UTC <span class="o">[</span>1] LOG:  listening on IPv6 address <span class="s2">"::"</span>, port 5432
2024-05-11 14:25:21.010 UTC <span class="o">[</span>1] LOG:  listening on Unix socket <span class="s2">"/var/run/postgresql/.s.PGSQL.5432"</span>
2024-05-11 14:25:21.012 UTC <span class="o">[</span>29] LOG:  database system was interrupted<span class="p">;</span> last known up at 2024-05-11 14:21:16 UTC
2024-05-11 14:25:21.017 UTC <span class="o">[</span>29] LOG:  entering standby mode
2024-05-11 14:25:21.026 UTC <span class="o">[</span>29] LOG:  redo starts at 0/5000028
2024-05-11 14:25:21.026 UTC <span class="o">[</span>29] LOG:  consistent recovery state reached at 0/5000100
2024-05-11 14:25:21.026 UTC <span class="o">[</span>1] LOG:  database system is ready to accept read-only connections
2024-05-11 14:25:21.034 UTC <span class="o">[</span>30] LOG:  started streaming WAL from primary at 0/6000000 on timeline 1</code></pre></figure>

<ul>
  <li>Test replication and failover</li>
</ul>

<p>First, let us test the replication, by login to postgres-1, create a zack_customers table, then validating from postgres-2</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># bash into postgres-1, create zack_customers table</span>
root@ubt-server:~# docker <span class="nb">exec</span> <span class="nt">-it</span> postgres-1 bash
root@06dd98085df7:/# psql <span class="nt">--username</span><span class="o">=</span>postgresadmin postgresdb
psql <span class="o">(</span>15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">))</span>
Type <span class="s2">"help"</span> <span class="k">for </span>help.

<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># CREATE TABLE zack_customers (zackname text, z_customer_id serial, date_created timestamp);</span>
CREATE TABLE
<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># \dt</span>
                List of relations
 Schema |      Name      | Type  |     Owner     
<span class="nt">--------</span>+----------------+-------+---------------
 public | zack_customers | table | postgresadmin
<span class="o">(</span>1 row<span class="o">)</span>

<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># \q</span>
root@06dd98085df7:/# <span class="nb">exit
exit</span>

<span class="c"># bash into postgres-2, validate zack_customers table</span>
root@ubt-server:~/postgres-2/pgdata# docker <span class="nb">exec</span> <span class="nt">-it</span> postgres-2 bash
root@b333ff290624:/# psql <span class="nt">--username</span><span class="o">=</span>postgresadmin postgresdb
psql <span class="o">(</span>15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">))</span>
Type <span class="s2">"help"</span> <span class="k">for </span>help.

<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># \dt</span>
                List of relations
 Schema |      Name      | Type  |     Owner     
<span class="nt">--------</span>+----------------+-------+---------------
 public | zack_customers | table | postgresadmin
<span class="o">(</span>1 row<span class="o">)</span>

<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># \q</span>
root@b333ff290624:/# <span class="nb">exit
exit</span></code></pre></figure>

<p>now we simulate failover by using loadbalancer tool “pgctl”, to shut down the primary instance, then promote the standby read-only instance into a read-write instance</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># shut down the primary instance</span>
root@ubt-server:~# docker <span class="nb">rm</span> <span class="nt">-f</span> postgres-1
postgres-1
<span class="c"># exec standby try to create a table zack_customers_2, get error as it's read-only</span>
root@ubt-server:~# docker <span class="nb">exec</span> <span class="nt">-it</span> postgres-2 bash
root@b333ff290624:/# psql <span class="nt">--username</span><span class="o">=</span>postgresadmin postgresdb
psql <span class="o">(</span>15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">))</span>
Type <span class="s2">"help"</span> <span class="k">for </span>help.

<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># CREATE TABLE zack_customers_2 (zackname text, z_customer_id serial, date_created timestamp);</span>
ERROR:  cannot execute CREATE TABLE <span class="k">in </span>a read-only transaction

postgresdb-# <span class="se">\q</span>

<span class="c"># promote postgres-2 from standby to primary</span>
root@b333ff290624:/# runuser <span class="nt">-u</span> postgres <span class="nt">--</span> pg_ctl promote
waiting <span class="k">for </span>server to promote.... <span class="k">done
</span>server promoted

<span class="c"># exec to create table zack_customers_2, this time works</span>
root@b333ff290624:/# psql <span class="nt">--username</span><span class="o">=</span>postgresadmin postgresdb
psql <span class="o">(</span>15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">))</span>
Type <span class="s2">"help"</span> <span class="k">for </span>help.

<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># CREATE TABLE zack_customers_2 (zackname text, z_customer_id serial, date_created timestamp);</span>
CREATE TABLE
<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># \dt</span>
                 List of relations
 Schema |       Name       | Type  |     Owner     
<span class="nt">--------</span>+------------------+-------+---------------
 public | zack_customers   | table | postgresadmin
 public | zack_customers_2 | table | postgresadmin
<span class="o">(</span>2 rows<span class="o">)</span>

<span class="nv">postgresdb</span><span class="o">=</span><span class="c"># \q</span>
root@b333ff290624:/# <span class="nb">exit
exit
</span>root@ubt-server:~# </code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we are able to run a PostgreSQL primary and standby instances to test replication and failover, in the next blog I will discover how to depoly a single PostgreSQL on kubernetes.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[The Replication]]></summary></entry><entry><title type="html">PostgreSQL: Get Started</title><link href="http://localhost:4000/jekyll/cat2/2024/05/06/PS1.html" rel="alternate" type="text/html" title="PostgreSQL: Get Started" /><published>2024-05-06T10:15:29+10:00</published><updated>2024-05-06T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/06/PS1</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/06/PS1.html"><![CDATA[<p><b> The Scenario</b></p>

<p>PostgreSQL is a very popular open-source relational database management systems (RDBMS), for its Extensibility and Feature-Rich, suitable for mission-critical applications, not to mention its active PostgreSQL community.</p>

<p>In the upcoming posts, I will start a series of PostgreSQL study to :</p>

<ul>
  <li>
    <p>explore PostgreSQL main features, installation, basic administration tasks</p>
  </li>
  <li>
    <p>deploy PostgreSQL cluster onto K8S with PostgreSQL Operater, validate backup and rolling upgrade</p>
  </li>
  <li>
    <p>create a simple Flash microservice application to connect PostgreSQL cluster and validate failover</p>
  </li>
  <li>
    <p>integrate the whole deployment into CICD pipeline for automation</p>
  </li>
  <li>
    <p>create AWS RDS PostgreSQL, with S3 Block storage as replica</p>
  </li>
</ul>

<p>By the end of the series we should be able to have a comprehensive understanding of PostgreSQL from a DevOps perspective</p>

<p><b>PostgreSQL Basic</b></p>

<p>To begin, we will</p>

<ul>
  <li>install PostgreSQL as a docker container on a local Ubuntu machine to get it up and running,</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># ubuntu install docker</span>
root@ubt-server:~# curl <span class="nt">-fsSL</span> https://get.docker.com <span class="nt">-o</span> get-docker.sh
root@ubt-server:~# sh get-docker.sh
root@ubt-server:~# docker <span class="nt">--version</span>
root@ubt-server:~# systemctl <span class="nb">enable </span>docker

<span class="c"># install PostgreSQL 15.0</span>
root@ubt-server:~#  docker run <span class="nt">--name</span> zack-postgres <span class="nt">-e</span> <span class="nv">POSTGRES_PASSWORD</span><span class="o">=</span>password <span class="nt">-d</span> postgres:15.0
root@ubt-server:~# docker ps
CONTAINER ID   IMAGE           COMMAND                  CREATED         STATUS         PORTS      NAMES
b4fc638dfde3   postgres:15.0   <span class="s2">"docker-entrypoint.s…"</span>   7 seconds ago   Up 6 seconds   5432/tcp   zack-postgres</code></pre></figure>

<ul>
  <li>Run a simple PostgreSQL database with docker compose</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create docker-compose.yaml and run postgres and adminer from dockercompose</span>
root@ubt-server:~# vim docker-compose.yaml

version: <span class="s1">'3.1'</span>
services:
  db:
    image: postgres:15.0
    restart: always
    environment:
      POSTGRES_PASSWORD: password
    ports:
    - 5000:5432
  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080

<span class="c"># run docker compose</span>
root@ubt-server:~# docker compose up</code></pre></figure>

<ul>
  <li>Validate from adminer web console locahost:8080 with password set in the environment variables</li>
</ul>

<p><img src="/assets/ps1-1.png" alt="image tooltip here" />
<img src="/assets/ps1-2.png" alt="image tooltip here" /></p>

<ul>
  <li>Persist data to mount the PostgreSQL container volume, validate data table after start/stop container 
PostgreSQL stores its data by default under /var/lib/postgresql/data, here we create a /pgdata folder on local machine to mount PostgreSQL default volume</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create local Persist data directory /pgdata</span>
root@ubt-server:~# <span class="nb">mkdir </span>pgdata
<span class="c"># run PostgreSQL to mount local Persist data and Bind a different port</span>
root@ubt-server:~# docker run <span class="nt">-d</span> <span class="nt">-it</span> <span class="nt">--rm</span> <span class="nt">--name</span> zack-postgres2 <span class="nt">-e</span> <span class="nv">POSTGRES_PASSWORD</span><span class="o">=</span>password <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/pgdata:/var/lib/postgresql/data <span class="nt">-p</span> 5000:5432 postgres:15.0

PostgreSQL Database directory appears to contain a database<span class="p">;</span> Skipping initialization

2024-05-08 00:58:47.540 UTC <span class="o">[</span>1] LOG:  starting PostgreSQL 15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">)</span> on x86_64-pc-linux-gnu, compiled by gcc <span class="o">(</span>Debian 10.2.1-6<span class="o">)</span> 10.2.1 20210110, 64-bit
2024-05-08 00:58:47.541 UTC <span class="o">[</span>1] LOG:  listening on IPv4 address <span class="s2">"0.0.0.0"</span>, port 5432
2024-05-08 00:58:47.541 UTC <span class="o">[</span>1] LOG:  listening on IPv6 address <span class="s2">"::"</span>, port 5432
2024-05-08 00:58:47.542 UTC <span class="o">[</span>1] LOG:  listening on Unix socket <span class="s2">"/var/run/postgresql/.s.PGSQL.5432"</span>
2024-05-08 00:58:47.545 UTC <span class="o">[</span>28] LOG:  database system was shut down at 2024-05-08 00:57:45 UTC
2024-05-08 00:58:47.547 UTC <span class="o">[</span>1] LOG:  database system is ready to accept connections</code></pre></figure>

<ul>
  <li>Connect to DB container and validate</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># enter the container</span>
root@ubt-server:~# docker <span class="nb">exec</span> <span class="nt">-it</span> zack-postgres2 bash
<span class="c"># login to postgres</span>
root@d7386c566872:/# psql <span class="nt">-h</span> localhost <span class="nt">-U</span> postgres
psql <span class="o">(</span>15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">))</span>
Type <span class="s2">"help"</span> <span class="k">for </span>help.
<span class="c"># create a table</span>
<span class="nv">postgres</span><span class="o">=</span><span class="c"># CREATE TABLE customers (firstname text,lastname text, customer_id serial);</span>
CREATE TABLE
<span class="c"># add record</span>
<span class="nv">postgres</span><span class="o">=</span><span class="c"># INSERT INTO customers (firstname, lastname) VALUES ( 'Bob', 'Smith');</span>
INSERT 0 1
<span class="c"># show table</span>
<span class="nv">postgres</span><span class="o">=</span><span class="c"># \dt</span>
           List of relations
 Schema |   Name    | Type  |  Owner   
<span class="nt">--------</span>+-----------+-------+----------
 public | customers | table | postgres
<span class="o">(</span>1 row<span class="o">)</span>
<span class="c"># get records</span>
<span class="nv">postgres</span><span class="o">=</span><span class="c"># SELECT * FROM customers;</span>
 firstname | lastname | customer_id 
<span class="nt">-----------</span>+----------+-------------
 Bob       | Smith    |           1
<span class="o">(</span>1 row<span class="o">)</span>
<span class="c"># quit </span>
<span class="nv">postgres</span><span class="o">=</span><span class="c"># \q</span>
<span class="c"># exit db container</span>
root@d7386c566872:/# <span class="nb">exit
exit</span></code></pre></figure>

<ul>
  <li>add persist data in docker-compose and run PostgreSQL from compose</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># add presist data folder in compose yaml</span>
root@ubt-server:~# vim docker-compose.yaml

version: <span class="s1">'3.1'</span>
services:
  db:
    image: postgres:15.0
    restart: always
    environment:
      POSTGRES_PASSWORD: admin123
    ports:
    - 5000:5432
    volumes:
    - ./pgdata:/var/lib/postgresql/data
  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080

root@ubt-server:~# docker compose up</code></pre></figure>

<ul>
  <li>Validate the previous table and record from adminer console</li>
</ul>

<p>Table and record still there because of the persistent data mount
<img src="/assets/ps1-3.png" alt="image tooltip here" /></p>

<p><b>PostgreSQL Configuration</b></p>

<p>Before jumping into replication, it is more important to explore the PostgreSQL configuration files to have a better understanding of its important config, take the default conf files out of a running database and learn it and make own configuration, then mount these conf files into container, so tell PostgreSQL to use my own configuration files to perform my prefered way.</p>

<p>To achieve this, we need the db user “postgres” has ID of 999 with access to custom conf files.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~/pgdata# <span class="nb">chown </span>999:999 config/postgresql.conf
root@ubt-server:~/pgdata# <span class="nb">chown </span>999:999 config/pg_hba.conf
root@ubt-server:~/pgdata# <span class="nb">chown </span>999:999 config/pg_ident.conf

root@ubt-server:~/pgdata# ll <span class="k">*</span>.conf
<span class="nt">-rw-------</span> 1 lxd docker  4821 May  8 00:30 pg_hba.conf
<span class="nt">-rw-------</span> 1 lxd docker  1636 May  8 00:30 pg_ident.conf
<span class="nt">-rw-------</span> 1 lxd docker    88 May  8 00:30 postgresql.auto.conf
<span class="nt">-rw-------</span> 1 lxd docker 29525 May  8 00:30 postgresql.conf

root@ubt-server:~# <span class="nb">mkdir </span>config
root@ubt-server:~# <span class="nb">cd </span>config/
root@ubt-server:~# <span class="nb">cp</span> <span class="k">*</span>.conf /config

root@ubt-server:~/pgdata# <span class="nb">chown </span>999:999 config/postgresql.conf
root@ubt-server:~/pgdata# <span class="nb">chown </span>999:999 config/pg_hba.conf
root@ubt-server:~/pgdata# <span class="nb">chown </span>999:999 config/pg_ident.conf</code></pre></figure>

<p>The official PostgreSQL documentation explains those configuration files as below:</p>

<ul>
  <li>pg_hba.conf:</li>
</ul>

<p>This file stands for “PostgreSQL Host-Based Authentication.” It controls client authentication based on the host and user information. It specifies which hosts are allowed to connect to the PostgreSQL server, which databases and users they can access, and what authentication methods they must use. It’s a crucial security measure for controlling access to PostgreSQL server.</p>

<ul>
  <li>pg_ident.conf:</li>
</ul>

<p>This file, “PostgreSQL Identification Mapping,” allows to define mappings between external (e.g., operating system) and internal (PostgreSQL) user names.</p>

<ul>
  <li>postgresql.conf:
Main configuration file for PostgreSQL which contains global settings to tailor its behavior to specific requirements and environment.</li>
</ul>

<p>Create custom config file 
<img src="/assets/ps1-4.png" alt="image tooltip here" /></p>

<p>now we can adjust the command by adding environment variables to run PostgreSQL from docker and docker-compose using our custom conf files</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~# vim docker-compose.yaml
version: <span class="s1">'3.1'</span>
services:
  db:
    container_name: postgres
    image: postgres:15.0
<span class="c"># important: passing argument to postgres container tell where conf file located # to match the custom conf file we created before when DB initiate       </span>
    <span class="nb">command</span>: <span class="s2">"postgres -c config_file=/config/postgresql.conf"</span>
    environment:
      POSTGRES_USER: <span class="s2">"postgresadmin"</span>
      POSTGRES_PASSWORD: <span class="s2">"admin123"</span>
      POSTGRES_DB: <span class="s2">"postgresdb"</span>
      PGDATA: <span class="s2">"/data"</span>
    volumes:
    - ./pgdata:/data
    - ./config:/config/
    ports:
    - 5000:5432
  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080

root@ubt-server:~# docker run <span class="nt">-it</span> <span class="nt">--rm</span> <span class="nt">--name</span> postgres <span class="nt">-e</span> <span class="nv">POSTGRES_USER</span><span class="o">=</span>postgresadmin <span class="nt">-e</span> <span class="nv">POSTGRES_PASSWORD</span><span class="o">=</span>admin123 <span class="nt">-e</span> <span class="nv">POSTGRES_DB</span><span class="o">=</span>postgresdb <span class="nt">-e</span> <span class="nv">PGDATA</span><span class="o">=</span><span class="s2">"/data"</span> <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/pgdata:/data <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/config:/config <span class="nt">-p</span> 5000:5432 postgres:15.0 <span class="nt">-c</span> <span class="s1">'config_file=/config/postgresql.conf'</span>

PostgreSQL Database directory appears to contain a database<span class="p">;</span> Skipping initialization

2024-05-10 10:40:44.685 UTC <span class="o">[</span>1] LOG:  starting PostgreSQL 15.0 <span class="o">(</span>Debian 15.0-1.pgdg110+1<span class="o">)</span> on x86_64-pc-linux-gnu, compiled by gcc <span class="o">(</span>Debian 10.2.1-6<span class="o">)</span> 10.2.1 20210110, 64-bit
2024-05-10 10:40:44.685 UTC <span class="o">[</span>1] LOG:  listening on IPv4 address <span class="s2">"0.0.0.0"</span>, port 5432
2024-05-10 10:40:44.685 UTC <span class="o">[</span>1] LOG:  listening on IPv6 address <span class="s2">"::"</span>, port 5432
2024-05-10 10:40:44.686 UTC <span class="o">[</span>1] LOG:  listening on Unix socket <span class="s2">"/var/run/postgresql/.s.PGSQL.5432"</span>
2024-05-10 10:40:44.688 UTC <span class="o">[</span>28] LOG:  database system was shut down at 2024-05-10 10:30:42 UTC
2024-05-10 10:40:44.690 UTC <span class="o">[</span>1] LOG:  database system is ready to accept connections

root@ubt-server:~# docker compose up <span class="nt">-d</span>
WARN[0000] /root/docker-compose.yaml: <span class="sb">`</span>version<span class="sb">`</span> is obsolete 
<span class="o">[</span>+] Running 2/2
 ✔ Container postgres        Started                                                                                            0.4s 
 ✔ Container root-adminer-1  Started</code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we can run a PostgreSQL container from docker and docker-compose with Persist data and custom configuration mount into the container, in the next blog we will discover primary and standby replication, WAL (write ahead log) options.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[The Scenario]]></summary></entry><entry><title type="html">Automate AWS EC2 tagging</title><link href="http://localhost:4000/jekyll/cat2/2024/05/01/AWS-tagging.html" rel="alternate" type="text/html" title="Automate AWS EC2 tagging" /><published>2024-05-01T10:15:29+10:00</published><updated>2024-05-01T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/01/AWS-tagging</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/01/AWS-tagging.html"><![CDATA[<p><b> Backgroud </b></p>

<p>I was tasked to enforce mandatory tagging for ec2 instances, as there are a lot of machines and a lot of tags need to be attached to each machine, here I need a scripted way to get the job done.</p>

<p><b> How to achieve </b></p>

<ul>
  <li>Prepare a list of ec2 instances with default name tag only,</li>
</ul>

<p><img src="/assets/awstag2.png" alt="image tooltip here" /></p>

<ul>
  <li>Open cloud shell or ssh to a linux box where AWSCli installed and configured to a AWS account, export the instances with ID to a csv file</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~# aws ec2 describe-instances <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[*].Instances[*].[InstanceId]'</span> <span class="o">&gt;</span> zztag.csv</code></pre></figure>

<ul>
  <li>Then add the header row for “instance ID” “tagA”  ”valueA”    ”tagB”  ”valueB”</li>
</ul>

<p><img src="/assets/awstag1.png" alt="image tooltip here" /></p>

<p><b> Create shell script to read the CSV file line by line, and add tags for each instance </b></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~# vim zacktag.sh

<span class="c">#!/bin/bash</span>

<span class="c"># Read the CSV file line by line</span>
<span class="k">while </span><span class="nv">IFS</span><span class="o">=</span>, <span class="nb">read</span> <span class="nt">-r</span> instance_id tagA valueA tagB valueB <span class="o">||</span> <span class="o">[</span> <span class="nt">-n</span> <span class="s2">"</span><span class="nv">$instance_id</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">do</span>
    <span class="c"># Add tagA</span>
    aws ec2 create-tags <span class="nt">--resources</span> <span class="s2">"</span><span class="nv">$instance_id</span><span class="s2">"</span> <span class="nt">--tags</span> <span class="nv">Key</span><span class="o">=</span><span class="s2">"</span><span class="nv">$tagA</span><span class="s2">"</span>,Value<span class="o">=</span><span class="s2">"</span><span class="nv">$valueA</span><span class="s2">"</span>
    <span class="c"># Add tagB</span>
    aws ec2 create-tags <span class="nt">--resources</span> <span class="s2">"</span><span class="nv">$instance_id</span><span class="s2">"</span> <span class="nt">--tags</span> <span class="nv">Key</span><span class="o">=</span><span class="s2">"</span><span class="nv">$tagB</span><span class="s2">"</span>,Value<span class="o">=</span><span class="s2">"</span><span class="nv">$valueB</span><span class="s2">"</span>
<span class="k">done</span> &lt; zztag.csv

root@ubt-server:~# <span class="nb">chmod</span> +x zacktag.sh <span class="o">&amp;&amp;</span> sh zacktag.sh</code></pre></figure>

<ul>
  <li>Validate now ec2 instances with all tags attached</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubt-server:~# aws ec2 describe-tags <span class="nt">--filters</span> <span class="s2">"Name=resource-id,Values=i-0980018fc6f4f722c"</span>
<span class="o">{</span>
    <span class="s2">"Tags"</span>: <span class="o">[</span>
        <span class="o">{</span>
            <span class="s2">"Key"</span>: <span class="s2">"Name"</span>,
            <span class="s2">"ResourceId"</span>: <span class="s2">"i-0980018fc6f4f722c"</span>,
            <span class="s2">"ResourceType"</span>: <span class="s2">"instance"</span>,
            <span class="s2">"Value"</span>: <span class="s2">"testing-for-tagging"</span>
        <span class="o">}</span>,
        <span class="o">{</span>
            <span class="s2">"Key"</span>: <span class="s2">"zz1"</span>,
            <span class="s2">"ResourceId"</span>: <span class="s2">"i-0980018fc6f4f722c"</span>,
            <span class="s2">"ResourceType"</span>: <span class="s2">"instance"</span>,
            <span class="s2">"Value"</span>: <span class="s2">"aa5"</span>
        <span class="o">}</span>,
        <span class="o">{</span>
            <span class="s2">"Key"</span>: <span class="s2">"zz2"</span>,
            <span class="s2">"ResourceId"</span>: <span class="s2">"i-0980018fc6f4f722c"</span>,
            <span class="s2">"ResourceType"</span>: <span class="s2">"instance"</span>,
            <span class="s2">"Value"</span>: <span class="s2">"bb4"</span>
        <span class="o">}</span>
    <span class="o">]</span>
<span class="o">}</span>
root@ubt-server:~# aws ec2 describe-tags <span class="nt">--filters</span> <span class="s2">"Name=resource-id,Values=i-076226daa5aaf7cf2"</span>
<span class="o">{</span>
    <span class="s2">"Tags"</span>: <span class="o">[</span>
        <span class="o">{</span>
            <span class="s2">"Key"</span>: <span class="s2">"Name"</span>,
            <span class="s2">"ResourceId"</span>: <span class="s2">"i-076226daa5aaf7cf2"</span>,
            <span class="s2">"ResourceType"</span>: <span class="s2">"instance"</span>,
            <span class="s2">"Value"</span>: <span class="s2">"zack-blog"</span>
        <span class="o">}</span>,
        <span class="o">{</span>
            <span class="s2">"Key"</span>: <span class="s2">"zz1"</span>,
            <span class="s2">"ResourceId"</span>: <span class="s2">"i-076226daa5aaf7cf2"</span>,
            <span class="s2">"ResourceType"</span>: <span class="s2">"instance"</span>,
            <span class="s2">"Value"</span>: <span class="s2">"aa1"</span>
        <span class="o">}</span>,
        <span class="o">{</span>
            <span class="s2">"Key"</span>: <span class="s2">"zz2"</span>,
            <span class="s2">"ResourceId"</span>: <span class="s2">"i-076226daa5aaf7cf2"</span>,
            <span class="s2">"ResourceType"</span>: <span class="s2">"instance"</span>,
            <span class="s2">"Value"</span>: <span class="s2">"aa2"</span>
        <span class="o">}</span>
    <span class="o">]</span>
<span class="o">}</span></code></pre></figure>

<p><img src="/assets/awstag3.png" alt="image tooltip here" /></p>

<ul>
  <li>create cronjob to update tagging monthly</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># create a monthly cron to run the script</span>
root@ubt-server:~# crontab <span class="nt">-e</span>
no crontab <span class="k">for </span>root - using an empty one

Select an editor.  To change later, run <span class="s1">'select-editor'</span><span class="nb">.</span>
  1. /bin/nano        &lt;<span class="nt">----</span> easiest
  2. /usr/bin/vim.basic
  3. /usr/bin/vim.tiny
  4. /bin/ed

Choose 1-4 <span class="o">[</span>1]: 2
crontab: installing new crontab

<span class="c"># List the monthly scheduled cronjob</span>
root@ubt-server:~# crontab <span class="nt">-l</span>
0 0 1 <span class="k">*</span> <span class="k">*</span> ~/zacktag.sh</code></pre></figure>

<p><b> Conclusion </b></p>

<p>Now we have a scripted way to achieve adding different tags for multiple ec2 instances via AWS CLI and shell script, same method to any other AWS resources that needed to be tagged, together with cronjob, we can only update the csv file which regularly updates resource ID and tags we want to attach, upload the csv file, every month their tags will be updated accordingly.</p>

<p>Furthermore, the resouces can be queried by setting up filter by different tag criteria:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># query EC2 with certain tag</span>
aws ec2 describe-instances <span class="nt">--filters</span> <span class="s2">"Name=tag:TagName,Values=TagValue"</span>

<span class="c"># query EC2 without certain tag</span>
aws ec2 describe-instances <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[?not_null(Tags[?Key==`TagName` &amp;&amp; Value==`TagValue`])].InstanceId'</span></code></pre></figure>

<p>Or we can use lambda function together with AWS Config rules to list and remediate the untagged EC2 resource accordingly</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">import boto3

def lambda_handler<span class="o">(</span>event, context<span class="o">)</span>:
    <span class="c"># Initialize AWS clients for services to be scanned </span>
    ec2_client <span class="o">=</span> boto3.client<span class="o">(</span><span class="s1">'ec2'</span><span class="o">)</span>
    
    <span class="c"># Retrieve a list of untagged EC2 instances</span>
    untagged_instances <span class="o">=</span> <span class="o">[]</span>
    response <span class="o">=</span> ec2_client.describe_instances<span class="o">()</span>
    <span class="k">for </span>reservation <span class="k">in </span>response[<span class="s1">'Reservations'</span><span class="o">]</span>:
        <span class="k">for </span>instance <span class="k">in </span>reservation[<span class="s1">'Instances'</span><span class="o">]</span>:
            <span class="k">if</span> <span class="s1">'Tags'</span> not <span class="k">in </span>instance:
                untagged_instances.append<span class="o">(</span>instance[<span class="s1">'InstanceId'</span><span class="o">])</span>
                
    <span class="k">return </span>untagged_instances</code></pre></figure>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Backgroud]]></summary></entry><entry><title type="html">ZackBlog AWS Serverless webhosting</title><link href="http://localhost:4000/jekyll/cat2/2024/04/30/serverless.html" rel="alternate" type="text/html" title="ZackBlog AWS Serverless webhosting" /><published>2024-04-30T10:15:29+10:00</published><updated>2024-04-30T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/04/30/serverless</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/04/30/serverless.html"><![CDATA[<p><b> How about this zackblog go serverless ?</b></p>

<p>In this article, I will see how to host “zackweb” as a static web application using bellow AWS serverless options:</p>

<ul>
  <li>
    <p>S3 static webhosting</p>
  </li>
  <li>
    <p>AWS CDK + CloudFront</p>
  </li>
</ul>

<p><b> Prerequisite </b></p>

<ul>
  <li>Add one more step in existing Github Action workflow to copy the static web content to newly created S3 bucket</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># edit github action workflow</span>
aws s3 <span class="nb">cp</span> ~/zack-gitops-project/zack_blog/_site/<span class="k">*</span> s3://zackweb-serverless/ <span class="nt">--recursive</span>

<span class="c"># validate content in s3 bucket</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span>aws s3 <span class="nb">ls </span>s3://zackweb-serverless <span class="nt">--summarize</span>
                           PRE aboutme/
                           PRE assets/
                           PRE certificate/
                           PRE gitrepo/
                           PRE jekyll/
                           PRE pro/
                           PRE skillroadmap/
2024-04-30 14:55:05       4455 404.html
2024-04-30 14:55:05        504 Dockerfile
2024-04-30 14:55:06      80555 feed.xml
2024-04-30 14:55:06       7760 index.html
2024-04-30 14:55:06          0 nginx.conf

Total Objects: 5
   Total Size: 93274</code></pre></figure>

<p><b> Option 1: S3 static webhosting </b></p>

<p>Go AWS console, under S3 bucket “zackweb-serverless” properties, enable static website hosting, update the bucket website endpoint address to Godaddy DNS record.</p>

<p><img src="/assets/serverless2.png" alt="image tooltip here" /></p>

<p><b> Option 2: using AWS CDK + CDN </b></p>

<p>With AWS CDK and CDN, the “zackweb” can be straightforward distributed from an S3 bucket accessible to the public by using CloudFront.</p>

<p><img src="/assets/serverless3.png" alt="image tooltip here" /></p>

<p>the steps will be:</p>

<ol>
  <li>
    <p>Enable AWS CDK on EC2 bastion host.</p>
  </li>
  <li>
    <p>S3 bucker ready and copy static web content into it (done above with modification of existing github action workflow)</p>
  </li>
  <li>
    <p>Establish a CloudFront distribution to host a static To-Do web application.</p>
  </li>
  <li>
    <p>Deploy the AWS CDK solution to host the To-do application.</p>
  </li>
</ol>

<ul>
  <li>install AWS CDK on bastion EC2 host</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># AWS CDK requires nodejs newer version</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span><span class="nb">sudo </span>apt-get <span class="nb">install </span>nodejs <span class="nt">-y</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span><span class="nb">sudo </span>npm cache clean <span class="nt">-f</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span><span class="nb">sudo </span>npm <span class="nb">install</span> <span class="nt">-g</span> n
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span><span class="nb">sudo </span>n stable
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span>nodejs <span class="nt">--version</span>
v12.22.9

<span class="c"># install aws-cdk cli</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span>npm <span class="nb">install</span> <span class="nt">-g</span> aws-cdk
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span>cdk <span class="nt">--version</span>
2.139.1 <span class="o">(</span>build b88f959<span class="o">)</span>

<span class="c"># check aws credential and bootstrap CDK</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span>aws sts get-caller-identity
<span class="o">{</span>
    <span class="s2">"UserId"</span>: <span class="s2">"AIDxxxxxxxxx7ZV"</span>,
    <span class="s2">"Account"</span>: <span class="s2">"8xxxxxx342"</span>,
    <span class="s2">"Arn"</span>: <span class="s2">"arn:aws:iam::8xxxxx342:user/zackcdk"</span>
<span class="o">}</span>

<span class="c"># bootstrap CDK</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span><span class="nb">sudo </span>cdk bootstrap aws://8xxxxxxx2/ap-southeast-2

<span class="c"># init app</span>
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span> mkdir cdk
ubuntu@ip-172-31-26-78:~<span class="nv">$ </span><span class="nb">cd </span>cdk
ubuntu@ip-172-31-26-78:~/cdk# cdk init app <span class="nt">--language</span><span class="o">=</span>typescript
Initializing a new git repository...
Executing npm install...
✅ All <span class="k">done</span><span class="o">!</span>

<span class="c"># create CDK code</span>

ubuntu@ip-172-31-26-78:~/cdk/lib# vim cdk-stack.ts

import <span class="k">*</span> as cdk from <span class="s1">'@aws-cdk/core'</span><span class="p">;</span>
import <span class="k">*</span> as cloudfront from <span class="s1">'@aws-cdk/aws-cloudfront'</span><span class="p">;</span>
import <span class="k">*</span> as origins from <span class="s1">'@aws-cdk/aws-cloudfront-origins'</span><span class="p">;</span>

<span class="nb">export </span>class ZackWebStack extends cdk.Stack <span class="o">{</span>
  constructor<span class="o">(</span>scope: cdk.Construct, <span class="nb">id</span>: string, props?: cdk.StackProps<span class="o">)</span> <span class="o">{</span>
    super<span class="o">(</span>scope, <span class="nb">id</span>, props<span class="o">)</span><span class="p">;</span>

    // existing S3 bucket
    const existingBucketName <span class="o">=</span> <span class="s1">'zackweb-serverless'</span><span class="p">;</span>

    // Create a CloudFront distribution
    const distribution <span class="o">=</span> new cloudfront.Distribution<span class="o">(</span>this, <span class="s1">'MyDistribution'</span>, <span class="o">{</span>
      defaultBehavior: <span class="o">{</span>
        origin: new origins.S3OriginFromBucketName<span class="o">(</span>existingBucketName<span class="o">)</span>
      <span class="o">}</span>,
      defaultRootObject: <span class="s1">'index.html'</span> // default root object
    <span class="o">})</span><span class="p">;</span>

    // Output the CloudFront distribution domain name
    new cdk.CfnOutput<span class="o">(</span>this, <span class="s1">'CloudFrontDomainName'</span>, <span class="o">{</span>
      value: distribution.distributionDomainName
    <span class="o">})</span><span class="p">;</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c"># install required module</span>

ubuntu@ip-172-31-26-78:~/cdk/lib# npm <span class="nb">install</span> @aws-cdk/core
ubuntu@ip-172-31-26-78:~/cdk/lib# npm <span class="nb">install</span> @aws-cdk/aws-cloudfront
ubuntu@ip-172-31-26-78:~/cdk/lib# npm <span class="nb">install</span> @aws-cdk/aws-cloudfront-origins

<span class="c"># Deploy stack</span>
ubuntu@ip-172-31-26-78:~/cdk/lib# <span class="nb">cd</span> ..
ubuntu@ip-172-31-26-78:~/cdk/# cdk deploy</code></pre></figure>

<ul>
  <li>The “zackweb” is now hosted on the AWS with serverless deployment !</li>
</ul>

<p><img src="/assets/serverless4.png" alt="image tooltip here" /></p>

<p><b> Conclusion</b></p>

<p>Now we move the blog onto AWS with serverless website hosting, using both S3 static webhosting and AWS CDK plus Cloudfront.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[How about this zackblog go serverless ?]]></summary></entry><entry><title type="html">Ubuntu 18.04 to 22.04 in-place upgrade</title><link href="http://localhost:4000/jekyll/cat2/2024/04/28/ubt-upg.html" rel="alternate" type="text/html" title="Ubuntu 18.04 to 22.04 in-place upgrade" /><published>2024-04-28T10:15:29+10:00</published><updated>2024-04-28T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/04/28/ubt-upg</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/04/28/ubt-upg.html"><![CDATA[<p><b> Ubuntu releases EOL roadmap </b></p>

<p>Every single Ubuntu LTS comes with 5 years of standard support. During those five years, bug fixes and security patches will be provided. Ubuntu 18.04 ‘Bionic Beaver’ is reaching End of Standard Support this May. so today we are going to run in-place upgrade for ubuntu 18.04 LTS to 22.04 LTS.</p>

<p><img src="/assets/ubt-upg1.png" alt="image tooltip here" /></p>

<p><b> Pre-upgrade checklist </b></p>

<ul>
  <li>validate current OS version and running service (nginx)</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># current OS version</span>
root@ubuntu-test:~# <span class="nb">cat</span> /etc/os-release 
<span class="nv">NAME</span><span class="o">=</span><span class="s2">"Ubuntu"</span>
<span class="nv">VERSION</span><span class="o">=</span><span class="s2">"18.04.6 LTS (Bionic Beaver)"</span>
<span class="nv">ID</span><span class="o">=</span>ubuntu
<span class="nv">ID_LIKE</span><span class="o">=</span>debian
<span class="nv">PRETTY_NAME</span><span class="o">=</span><span class="s2">"Ubuntu 18.04.6 LTS"</span>
<span class="nv">VERSION_ID</span><span class="o">=</span><span class="s2">"18.04"</span>
<span class="nv">HOME_URL</span><span class="o">=</span><span class="s2">"https://www.ubuntu.com/"</span>
<span class="nv">SUPPORT_URL</span><span class="o">=</span><span class="s2">"https://help.ubuntu.com/"</span>
<span class="nv">BUG_REPORT_URL</span><span class="o">=</span><span class="s2">"https://bugs.launchpad.net/ubuntu/"</span>
<span class="nv">PRIVACY_POLICY_URL</span><span class="o">=</span><span class="s2">"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"</span>
<span class="nv">VERSION_CODENAME</span><span class="o">=</span>bionic
<span class="nv">UBUNTU_CODENAME</span><span class="o">=</span>bionic

<span class="c"># nginx service status</span>

root@ubuntu-test:~# root@ubuntu-test:~# <span class="nb">echo</span> <span class="s2">"ubuntu-inplace-upgrade zack-testing-nginx-service!!"</span>  <span class="o">&gt;&gt;</span> /var/www/html/index.html
root@ubuntu-test:~# systemctl restart nginx
root@ubuntu-test:~# curl localhost
ubuntu-inplace-upgrade zack-testing-nginx-service!!</code></pre></figure>

<ul>
  <li>Fully update the system</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># update system</span>
root@ubuntu-test:~# <span class="nb">sudo </span>apt update
Hit:1 http://au.archive.ubuntu.com/ubuntu bionic InRelease
Hit:2 http://au.archive.ubuntu.com/ubuntu bionic-updates InRelease
Hit:3 http://au.archive.ubuntu.com/ubuntu bionic-backports InRelease
Hit:4 http://au.archive.ubuntu.com/ubuntu bionic-security InRelease
Reading package lists... Done                      
Building dependency tree       
Reading state information... Done
All packages are up to date.
root@ubuntu-test:~# <span class="nb">sudo </span>apt upgrade <span class="nt">-y</span>
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Calculating upgrade... Done
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.

<span class="c"># reboot system before upgrade</span>
root@ubuntu-test:~# <span class="nb">sudo </span><span class="k">do</span><span class="nt">-release-upgrade</span>
Checking <span class="k">for </span>a new Ubuntu release
You have not rebooted after updating a package which requires a reboot. Please reboot before upgrading.
root@ubuntu-test:~# reboot
Connection closing...Socket close.</code></pre></figure>

<ul>
  <li>take full system backup</li>
</ul>

<p>here I took a VM snapshot before upgrade</p>

<p><b> 18.04 to 22.04 upgrade</b></p>

<p>There is no direct upgrade path from 18.04 LTS to Ubuntu 22.04 LTS, so we go Ubuntu 20.04 LTS first and then to Ubuntu 22.04 LTS.</p>

<ul>
  <li>first upgrade to 20.04</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># run upgrade</span>

root@ubuntu-test:~# <span class="nb">sudo </span><span class="k">do</span><span class="nt">-release-upgrade</span>

This session appears to be running under ssh. It is not recommended 
to perform a upgrade over ssh currently because <span class="k">in case</span> of failure it 
is harder to recover. 

If you <span class="k">continue</span>, an additional ssh daemon will be started at port 
<span class="s1">'1022'</span><span class="nb">.</span> 
Do you want to <span class="k">continue</span>? 

Continue <span class="o">[</span>yN] y

Starting additional sshd 

Calculating the changes
  MarkInstall libfwupdplugin1:amd64 &lt; none -&gt; 1.5.11-0ubuntu1~20.04.2 @un uN Ib <span class="o">&gt;</span> <span class="nv">FU</span><span class="o">=</span>1
  Installing libxmlb1 as Depends of libfwupdplugin1
    MarkInstall libxmlb1:amd64 &lt; none -&gt; 0.1.15-2ubuntu1~20.04.1 @un uN <span class="o">&gt;</span> <span class="nv">FU</span><span class="o">=</span>0

Do you want to start the upgrade? 

Continue <span class="o">[</span>yN]  Details <span class="o">[</span>d]y</code></pre></figure>

<ul>
  <li>allow service restart during upgrade</li>
</ul>

<p><img src="/assets/ubt-upg2.png" alt="image tooltip here" /></p>

<ul>
  <li>reboot after upgrade</li>
</ul>

<p>The installation and removing of packages may take some time, then reboot is required after ungrade completion</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">Purging configuration files <span class="k">for </span>ebtables <span class="o">(</span>2.0.11-3build1<span class="o">)</span> ...
Purging configuration files <span class="k">for </span>python3.6-minimal <span class="o">(</span>3.6.9-1~18.04ubuntu1.12<span class="o">)</span> ...
Purging configuration files <span class="k">for </span>mlocate <span class="o">(</span>0.26-3ubuntu3<span class="o">)</span> ...
Processing triggers <span class="k">for </span>dbus <span class="o">(</span>1.12.16-2ubuntu2.3<span class="o">)</span> ...
Processing triggers <span class="k">for </span>systemd <span class="o">(</span>245.4-4ubuntu3.23<span class="o">)</span> ...

System upgrade is complete.

Restart required 

To finish the upgrade, a restart is required. 
If you <span class="k">select</span> <span class="s1">'y'</span> the system will be restarted. 

Continue <span class="o">[</span>yN] y</code></pre></figure>

<ul>
  <li>validate OS and service</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># validate nginx service</span>
root@ubuntu-test:~# curl localhost
ubuntu-inplace-upgrade  zack-testing-nginx-service!!
<span class="c"># validate OS version</span>
root@ubuntu-test:~# <span class="nb">cat</span> /etc/os-release 
<span class="nv">NAME</span><span class="o">=</span><span class="s2">"Ubuntu"</span>
<span class="nv">VERSION</span><span class="o">=</span><span class="s2">"20.04.6 LTS (Focal Fossa)"</span>
<span class="nv">ID</span><span class="o">=</span>ubuntu
<span class="nv">ID_LIKE</span><span class="o">=</span>debian
<span class="nv">PRETTY_NAME</span><span class="o">=</span><span class="s2">"Ubuntu 20.04.6 LTS"</span>
<span class="nv">VERSION_ID</span><span class="o">=</span><span class="s2">"20.04"</span>
<span class="nv">HOME_URL</span><span class="o">=</span><span class="s2">"https://www.ubuntu.com/"</span>
<span class="nv">SUPPORT_URL</span><span class="o">=</span><span class="s2">"https://help.ubuntu.com/"</span>
<span class="nv">BUG_REPORT_URL</span><span class="o">=</span><span class="s2">"https://bugs.launchpad.net/ubuntu/"</span>
<span class="nv">PRIVACY_POLICY_URL</span><span class="o">=</span><span class="s2">"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"</span>
<span class="nv">VERSION_CODENAME</span><span class="o">=</span>focal
<span class="nv">UBUNTU_CODENAME</span><span class="o">=</span>focal</code></pre></figure>

<ul>
  <li>then upgrade to 22.04</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubuntu-test:~# <span class="nb">sudo </span>apt update
Hit:1 http://au.archive.ubuntu.com/ubuntu focal InRelease
Hit:2 http://au.archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:3 http://au.archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:4 http://au.archive.ubuntu.com/ubuntu focal-security InRelease
Reading package lists... Done
Building dependency tree       
Reading state information... Done
All packages are up to date.

root@ubuntu-test:~# <span class="nb">sudo </span>apt upgrade
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Calculating upgrade... Done
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.

root@ubuntu-test:~# <span class="nb">sudo </span><span class="k">do</span><span class="nt">-release-upgrade</span>

<span class="c"># validate after upgrade </span>
root@ubuntu-test:~# curl localhost
ubuntu-inplace-upgrade  zack-testing-nginx-service!!

root@ubuntu-test:~# lsb_release <span class="nt">-a</span>
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 22.04.4 LTS
Release:	22.04
Codename:	jammy</code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we complete the in-place ubuntu OS release upgrade from 18.04 to 22.04. The whole upgrade took about 1 hour to finish, with several comfirmation required during upgrade process. The service nginx was running after each upgrade.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Ubuntu releases EOL roadmap]]></summary></entry></feed>