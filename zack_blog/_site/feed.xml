<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-10-09T01:38:05+11:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Zack’s Blog</title><subtitle>## AWS   ## Jenkins  ## Microservices ## Automation ## K8S   ## CICD     ## Gitops </subtitle><entry><title type="html">EKS Cluster Autoscaler and Horizontal Pod Autoscaler (HPA)</title><link href="http://localhost:4000/jekyll/cat2/2024/09/12/eks1.html" rel="alternate" type="text/html" title="EKS Cluster Autoscaler and Horizontal Pod Autoscaler (HPA)" /><published>2024-09-12T10:15:29+10:00</published><updated>2024-09-12T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/09/12/eks1</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/09/12/eks1.html"><![CDATA[<p><b>Autoscaler and Horizontal Pod Autoscaler (HPA) Practice on EKS </b></p>

<p>In last post, I was able to deploy EKS cluster via Jenkins and Terraform:</p>

<ul>
  <li><a href="https://zackz.site/jekyll/cat2/2024/08/15/Jenkins-redo2-copy.html">Jenkins - Multi-Destnation Continuse Deployment with Terraform</a></li>
</ul>

<p>This post walks through the process of setting up an Autoscaler and Horizontal Pod Autoscaler (HPA) in an Amazon EKS cluster. We will explore how to dynamically scale the number of nodes in EKS cluster and how to autoscale K8S pods based on CPU utilization using HPA.</p>

<p>Prerequisites:</p>

<ul>
  <li>Use the <code class="language-plaintext highlighter-rouge">AWS CLI</code> to update kubeconfig so that kubectl can communicate with the EKS cluster</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nv">$ </span>aws eks <span class="nt">--region</span> ap-southeast-2 update-kubeconfig <span class="nt">--name</span> module-eks-cluster
Updated context arn:aws:eks:ap-southeast-2:851725491342:cluster/module-eks-cluster <span class="k">in </span>C:<span class="se">\U</span>sers<span class="se">\z</span>ack<span class="se">\.</span>kube<span class="se">\c</span>onfig

Verify that the nodes are ready:
<span class="nv">$ </span>kubectl.exe get node
NAME                                              STATUS   ROLES    AGE     VERSION
ip-172-31-37-57.ap-southeast-2.compute.internal   Ready    &lt;none&gt;   2m40s   v1.31.0-eks-a737599</code></pre></figure>

<ul>
  <li>Deploy the <code class="language-plaintext highlighter-rouge">Cluster Autoscaler</code> from the official Kubernetes Autoscaler GitHub repository, The Cluster Autoscaler automatically adjusts the number of nodes in eks cluster based on the resource requirements of the workloads.</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nv">$ </span>kubectl.exe apply <span class="nt">-f</span> https://raw.githubusercontent.com/kubernetes/autoscaler/refs/heads/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-one-asg.yaml
serviceaccount/cluster-autoscaler created
clusterrole.rbac.authorization.k8s.io/cluster-autoscaler created
role.rbac.authorization.k8s.io/cluster-autoscaler created
clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created
rolebinding.rbac.authorization.k8s.io/cluster-autoscaler created
deployment.apps/cluster-autoscaler created</code></pre></figure>

<ul>
  <li>Modify the Autoscaler Parameters to tuning Scale-Up and Down Behavior.</li>
</ul>

<p>Customized Autoscaler parameters to add arguments to control the scaling behavior, such as the <code class="language-plaintext highlighter-rouge">minimum and maximum node counts</code>, the <code class="language-plaintext highlighter-rouge">time between scale-down events</code>, <code class="language-plaintext highlighter-rouge">stabilization window</code> and <code class="language-plaintext highlighter-rouge">cooldown period</code>.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system edit deployment.apps/cluster-autoscaler
deployment.apps/cluster-autoscaler edited

      labels:
        app: cluster-autoscaler
    spec:
      containers:
      - <span class="nb">command</span>:
        - ./cluster-autoscaler
        - <span class="nt">--cluster-name</span><span class="o">=</span>module-eks-cluster
        - <span class="nt">--v</span><span class="o">=</span>4
        - <span class="nt">--stderrthreshold</span><span class="o">=</span>info
        - <span class="nt">--cloud-provider</span><span class="o">=</span>aws
        - <span class="nt">--skip-nodes-with-local-storage</span><span class="o">=</span><span class="nb">false</span>
        - <span class="nt">--balance-similar-node-groups</span>
        - <span class="nt">--skip-nodes-with-system-pods</span><span class="o">=</span><span class="nb">false</span>
        - <span class="nt">--scale-down-unneeded-time</span><span class="o">=</span>1m
        - <span class="nt">--scale-down-delay-after-add</span><span class="o">=</span>1m
        - <span class="nt">--nodes</span><span class="o">=</span>1:3:eks-module-eks-cluster-node-group-5ec93604-4bdc-a740-1fcc-707afc8431b</code></pre></figure>

<p><b>HPA testing based on Pod CPU utilization metric </b></p>

<ul>
  <li>Now, let’s deploy zackweb and set up an <code class="language-plaintext highlighter-rouge">Horizontal Pod Autoscaler (HPA)</code> to scale the number of pods based on CPU utilization.</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nv">$ </span><span class="nb">cat </span>deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: zackblog
spec:
  replicas: 1  <span class="c"># Start with 1 replica</span>
  selector:
    matchLabels:
      app: zackblog
  template:
    metadata:
      labels:
        app: zackblog
    spec:
      containers:
      - name: zackblog
        image: zackz001/gitops-jekyll:latest
        resources:
          requests:
            cpu: <span class="s2">"500m"</span>  <span class="c"># 0.5 vCPU</span>
            memory: <span class="s2">"512Mi"</span>  <span class="c"># 0.5 GiB</span>
          limits:
            cpu: <span class="s2">"1"</span>  <span class="c"># 1 vCPU</span>
            memory: <span class="s2">"1Gi"</span>  <span class="c"># 1 GiB</span>
<span class="nt">---</span>
apiVersion: v1
kind: Service
metadata:
  name: zackblog
spec:
  selector:
    app: zackblog  <span class="c"># This must match the labels in the Deployment</span>
  ports:
    - protocol: TCP
      port: 80        <span class="c"># Port that the service will expose</span>
      targetPort: 80 <span class="c"># Port that the container listens on</span>
  <span class="nb">type</span>: LoadBalancer

kubectl.exe apply <span class="nt">-f</span> deployment.yaml
deployment.apps/zackblog created
service/zackblog created</code></pre></figure>

<ul>
  <li>Set Resource Requests and Limits, and Configure Horizontal Pod Autoscaler (HPA) to automatically scale the deployment based on CPU usage</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nv">$ </span>kubectl <span class="nb">set </span>resources deployment zackblog <span class="nt">--limits</span><span class="o">=</span><span class="nv">cpu</span><span class="o">=</span>200m,memory<span class="o">=</span>200Mi <span class="nt">--requests</span><span class="o">=</span><span class="nv">cpu</span><span class="o">=</span>100m,memory<span class="o">=</span>100Mi
deployment.apps/zackblog resource requirements updated

kubectl autoscale deployment zackblog <span class="nt">--cpu-percent</span><span class="o">=</span>50 <span class="nt">--min</span><span class="o">=</span>1 <span class="nt">--max</span><span class="o">=</span>3

<span class="nv">$ </span>kubectl get hpa zackblog
NAME       REFERENCE             TARGETS              MINPODS   MAXPODS   REPLICAS   AGE
zackblog   Deployment/zackblog   cpu: &lt;unknown&gt;/50%   1         3         1          18s</code></pre></figure>

<ul>
  <li>Generate CPU Load to Test the HPA</li>
</ul>

<p>To test the HPA, we need to generate CPU load by runing a busybox container to repeatedly request the zackblog service.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl run <span class="nt">-i</span> <span class="nt">--tty</span> load-generator <span class="nt">--image</span><span class="o">=</span>busybox /bin/sh
<span class="c"># Inside the busybox shell, run this:</span>
<span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>wget <span class="nt">-q</span> <span class="nt">-O-</span> http://zackblog <span class="o">&gt;</span> /dev/null<span class="p">;</span> <span class="nb">sleep </span>0.5<span class="p">;</span> <span class="k">done</span></code></pre></figure>

<ul>
  <li>Monitor the HPA and Scaling Events, As CPU utilization increases, the HPA will automatically scale the number of pods:</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nv">$ </span>kubectl.exe get po
NAME                       READY   STATUS             RESTARTS      AGE
load-generator             1/1     Running            0             2m6s
zackblog-95f746486-wlmgx   1/1     Running            0             6m20s

kubectl get hpa zackblog <span class="nt">-w</span>
NAME       REFERENCE             TARGETS       MINPODS   MAXPODS   REPLICAS   AGE
zackblog   Deployment/zackblog   cpu: 0%/50%   1         10        1          12m
zackblog   Deployment/zackblog   cpu: 92%/50%  1         10        2          5m
zackblog   Deployment/zackblog   cpu: 52%/50%  1         10        3          2m

kubectl get hpa zackblog <span class="nt">-w</span>
zackblog   Deployment/zackblog   cpu: 37%/50%   1         10        3          27m
zackblog   Deployment/zackblog   cpu: 38%/50%   1         10        3          27m
zackblog   Deployment/zackblog   cpu: 6%/50%    1         10        3          27m
zackblog   Deployment/zackblog   cpu: 0%/50%    1         10        3          27m
zackblog   Deployment/zackblog   cpu: 0%/50%    1         10        1          28m</code></pre></figure>

<p>It can be seen that pods got  scaled up to 3 when CPU utilization reached 92% and scaled down to 1 when CPU utilization dropped below 50%.</p>

<p><b>Testing EKS Cluster Autoscaler by changing deployment replicas: </b></p>

<p><code class="language-plaintext highlighter-rouge">EKS Cluster Autoscaler</code> comes with scale-in and scale-out policies, which define when nodes should be added or removed. The Cluster Autoscaler adds nodes when there aren’t enough resources to schedule pending pods and removes nodes when they are underutilized.</p>

<ul>
  <li>Change the resource limitation for zackblog deployment for EKS node autoscaler testing, Set zackweb deployment with requests cpu 500m. As the EKS node group with a t3.small instance type (2c2g), which means one node can only handle one pod, so to make the Autoscaler testing easier to achieve.</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nv">$ </span>vim deployment.yaml

        resources:
          requests:
            cpu: <span class="s2">"500m"</span>  <span class="c"># 0.5 vCPU</span>
            memory: <span class="s2">"512Mi"</span>  <span class="c"># 0.5 GiB</span>
          limits:
            cpu: <span class="s2">"1"</span>  <span class="c"># 1 vCPU</span>
            memory: <span class="s2">"1Gi"</span>  <span class="c"># 1 GiB</span></code></pre></figure>

<ul>
  <li>Gracefully increase the number of deployment replicas to 2, to test EKS node scale up</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nv">$ </span>kubectl scale deployment zackblog <span class="nt">--replicas</span><span class="o">=</span>2
deployment.apps/zackblog scaled

<span class="nv">$ </span>kubectl.exe get po
NAME                        READY   STATUS             RESTARTS      AGE
load-generator              1/1     Running            0             7m39s
zackblog-7f67584fbd-47jvt   1/1     Running            0             31s
zackblog-7f67584fbd-9gtfn   0/1     Pending            0             6s

<span class="nv">$ </span>kubectl.exe describe po zackblog-7f67584fbd-9gtfn

Events:
  Type     Reason            Age   From                Message
  <span class="nt">----</span>     <span class="nt">------</span>            <span class="nt">----</span>  <span class="nt">----</span>                <span class="nt">-------</span>
  Warning  FailedScheduling  18s   default-scheduler   0/1 nodes are available: 1 Insufficient memory. preemption: 0/1 nodes are available: 1 No preemption victims found <span class="k">for </span>incoming pod.
  Normal   TriggeredScaleUp  9s    cluster-autoscaler  pod triggered scale-up: <span class="o">[{</span>eks-module-eks-cluster-node-group-5ec93604-4bdc-a740-1fcc-707afc8431b3 1-&gt;2 <span class="o">(</span>max: 3<span class="o">)}]</span>

<span class="nv">$ </span>kubectl.exe get node
NAME                                               STATUS   ROLES    AGE   VERSION
ip-172-31-15-152.ap-southeast-2.compute.internal   Ready    &lt;none&gt;   31s   v1.31.0-eks-a737599
ip-172-31-37-57.ap-southeast-2.compute.internal    Ready    &lt;none&gt;   37m   v1.31.0-eks-a737599

<span class="nv">$ </span>kubectl.exe get po
NAME                        READY   STATUS             RESTARTS        AGE
load-generator              1/1     Running            0               9m14s
zackblog-7f67584fbd-47jvt   1/1     Running            0               2m6s
zackblog-7f67584fbd-9gtfn   1/1     Running            0               101s</code></pre></figure>

<p>It can be seen that pod <code class="language-plaintext highlighter-rouge">zackblog-7f67584fbd-9gtfn</code> was in pending state due to waiting for node to be scale-up, once we have 2 nodes in EKS cluster, that pod can be scheduled and run.</p>

<ul>
  <li>Increase the number of deployment replicas to 3, to trigger EKS node scale up again</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nv">$ </span>kubectl scale deployment zackblog <span class="nt">--replicas</span><span class="o">=</span>3
deployment.apps/zackblog scaled

<span class="nv">$ </span>kubectl.exe get po
NAME                        READY   STATUS             RESTARTS        AGE
load-generator              1/1     Running            0               9m44s
zackblog-7f67584fbd-47jvt   1/1     Running            0               2m36s
zackblog-7f67584fbd-9gtfn   1/1     Running            0               2m11s
zackblog-7f67584fbd-f5s9d   1/1     Running            0               3s


<span class="nv">$ </span>kubectl.exe get po <span class="nt">-o</span> wide
NAME                        READY   STATUS    RESTARTS   AGE     IP              NODE                                               NOMINATED NODE   READINESS GATES
zackblog-7f67584fbd-47jvt   1/1     Running   0          5m58s   172.31.37.227   ip-172-31-37-57.ap-southeast-2.compute.internal    &lt;none&gt;           &lt;none&gt;
zackblog-7f67584fbd-9gtfn   1/1     Running   0          5m33s   172.31.5.139    ip-172-31-15-152.ap-southeast-2.compute.internal   &lt;none&gt;           &lt;none&gt;
zackblog-7f67584fbd-vzcx4   1/1     Running   0          2m3s    172.31.18.111   ip-172-31-20-24.ap-southeast-2.compute.internal    &lt;none&gt;           &lt;none&gt;</code></pre></figure>

<p>Now EKS scale up to 3 nodes to handle the replica increase again.</p>

<ul>
  <li>Change the number of deployment replicas down to 1, to trigger EKS node scale down, and monitor by autoscaler log file to see the scale down behavior:</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nv">$ </span>kubectl scale deployment zackblog <span class="nt">--replicas</span><span class="o">=</span>1

<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system logs <span class="nt">-f</span> deployment/cluster-autoscaler

I1008 12:53:22.686377       1 static_autoscaler.go:598] Starting scale down
I1008 12:53:22.686430       1 nodes.go:123] ip-172-31-15-152.ap-southeast-2.compute.internal was unneeded <span class="k">for </span>40.253885622s
I1008 12:53:22.686452       1 nodes.go:123] ip-172-31-37-57.ap-southeast-2.compute.internal was unneeded <span class="k">for </span>1m0.362059191s
I1008 12:53:22.686502       1 cluster.go:153] ip-172-31-37-57.ap-southeast-2.compute.internal <span class="k">for </span>removal
I1008 12:53:22.686644       1 hinting_simulator.go:77] Pod kube-system/cluster-autoscaler-5767f77d77-xgfjq can be moved to ip-172-31-20-24.ap-southeast-2.compute.internal
I1008 12:53:22.686717       1 hinting_simulator.go:77] Pod kube-system/coredns-7575495454-9n6kd can be moved to ip-172-31-15-152.ap-southeast-2.compute.internal
I1008 12:53:22.686832       1 hinting_simulator.go:77] Pod kube-system/coredns-7575495454-dxzdc can be moved to ip-172-31-15-152.ap-southeast-2.compute.internal
I1008 12:53:22.686875       1 cluster.go:176] node ip-172-31-37-57.ap-southeast-2.compute.internal may be removed
I1008 12:53:22.705270       1 delete.go:103] Successfully added ToBeDeletedTaint on node ip-172-31-37-57.ap-southeast-2.compute.internal
I1008 12:53:22.705367       1 actuator.go:212] Scale-down: removing node ip-172-31-37-57.ap-southeast-2.compute.internal, utilization: <span class="o">{</span>0.23316062176165803 0.49755477462428627 0 memory 0.49755477462428627<span class="o">}</span>, pods to reschedule: cluster-autoscaler-5767f77d77-xgfjq,coredns-7575495454-9n6kd,coredns-7575495454-dxzdc

<span class="nv">$ </span>kubectl.exe get node
NAME                                               STATUS                        ROLES    AGE     VERSION
ip-172-31-15-152.ap-southeast-2.compute.internal   NotReady,SchedulingDisabled   &lt;none&gt;   9m10s   v1.31.0-eks-a737599
ip-172-31-20-24.ap-southeast-2.compute.internal    Ready                         &lt;none&gt;   6m59s   v1.31.0-eks-a737599
ip-172-31-37-57.ap-southeast-2.compute.internal    Ready,SchedulingDisabled      &lt;none&gt;   45m     v1.31.0-eks-a737599

<span class="nv">$ </span>kubectl.exe get node
NAME                                              STATUS   ROLES    AGE     VERSION
ip-172-31-20-24.ap-southeast-2.compute.internal   Ready    &lt;none&gt;   7m58s   v1.31.0-eks-a737599</code></pre></figure>

<p>It can be seen that Cluster Autoscaler identified nodes that were underutilized or idle and marked them as “unneeded.”
It simulated moving the existing pods to other nodes. Once it determined that the pods could be rescheduled, it marked the node for deletion (using a ToBeDeletedTaint), preventing new workloads from being scheduled. Finally, the node was removed from the cluster, and the pods were successfully rescheduled on other nodes.</p>

<p>This behavior ensures that the cluster’s resources are used efficiently, scaling down when there is no workload, thereby reducing costs.</p>

<p><img src="/assets/eks1.png" alt="image tooltip here" /></p>

<p><b> Conclusion: </b></p>

<p>By following these steps, we can effectively manage the scaling of k8s applications and nodes in an EKS cluster using both the Cluster Autoscaler and Horizontal Pod Autoscaler (HPA). These tools ensure that the infrastructure adapts to varying workloads, optimizing resource utilization and costs.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Autoscaler and Horizontal Pod Autoscaler (HPA) Practice on EKS]]></summary></entry><entry><title type="html">Jenkins - Multi-Destnation Continuse Deployment with Terraform</title><link href="http://localhost:4000/jekyll/cat2/2024/08/15/Jenkins-redo2-copy.html" rel="alternate" type="text/html" title="Jenkins - Multi-Destnation Continuse Deployment with Terraform" /><published>2024-08-15T10:15:29+10:00</published><updated>2024-08-15T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/08/15/Jenkins-redo2%20copy</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/08/15/Jenkins-redo2-copy.html"><![CDATA[<p><b>Jenkins CD Pipeline Design</b></p>

<p>In last post, I was able to create a Jenkins Universal CI Pipeline to create blog docker image and push to DockerHub:</p>

<p><a href="https://zackz.site/jekyll/cat2/2024/08/02/Jenkins-redo1-copy.html">Jenkins - Universal CI Pipeline with Ansible &amp; Terraform</a></p>

<p>Now it is time to design the continuse deployment pipeline with Ansible and Terrofrm for infrustructure provision and application configration and deployment.</p>

<ul>
  <li>Continuse Deployment Consideration</li>
</ul>

<p>Continuse deployment will be more terraform focused. Starting Jenkins CD pipeline with single EC2 instance deployment for Blog website.  The pipeline can be reusable for multiple destinations in later design (EC2, ECS, EKS). At the moment, this EC2 deployment can be achieved via bellow folder structure:</p>

<ol>
  <li>Jenkins CD pipeline with muti-stage</li>
  <li>Terraform to provision AWS EC2</li>
  <li>Ansible to configure docker and deploy blog</li>
  <li>Validate Web Blog Access:</li>
  <li>Delete Terraform Resources</li>
</ol>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Tree</span>
terraform-ec2# tree
<span class="nb">.</span>
├── Jenkinsfile  <span class="c"># the CD pipeline file </span>
├── deploy-docker-playbook.yml <span class="c"># the Ansible playbook for ec2 webblog deployment </span>
├── hosts <span class="c"># the Ansible inventory file</span>
├── main.tf <span class="c"># the terraform file to provison AWS EC2 </span>
├── test-playbook.yaml <span class="c"># the playbook for Ansible testing and validation </span>
└── variables.tf the terraform var file to provison AWS EC2 </code></pre></figure>

<ul>
  <li>The CD pipeline design</li>
</ul>

<p>This Jenkins CD (Continuous Deployment) pipeline covers the following task:</p>

<ol>
  <li>Jenkins Cred and Environment Setup</li>
  <li>Check Installed Package Versions (AWSCli, Ansible, Terraform)</li>
  <li>Validate Ansible and AWS credential</li>
  <li>Run Terraform Initialization and Apply</li>
  <li>Validate EC2 Readiness and then Deploy Docker Using Ansible</li>
  <li>Validate Web Blog Access by extract EC2 public IP</li>
  <li>Delete Terraform Resources</li>
</ol>

<p>Jenkinsfile</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c">#  Jenkinsfile</span>
pipeline <span class="o">{</span>
    agent any
    environment <span class="o">{</span>
        IMAGE_NAME <span class="o">=</span> <span class="s2">"zackz001/jenkins"</span>
        IMAGE_TAG <span class="o">=</span> <span class="s2">"</span><span class="k">${</span><span class="nv">env</span><span class="p">.BUILD_NUMBER</span><span class="k">}</span><span class="s2">"</span>
        LATEST_TAG <span class="o">=</span> <span class="s2">"latest"</span>
        EMAIL_RECIPIENT <span class="o">=</span> <span class="s2">"zhbsoftboy1@gmail.com"</span>
        GIT_REPO_URL <span class="o">=</span> <span class="s1">'https://github.com/ZackZhouHB/zack-gitops-project.git'</span>  // Git repository URL
        GIT_BRANCH <span class="o">=</span> <span class="s1">'jenkins-cd'</span>  // Git branch
        DOCKERHUB_CREDENTIALS_ID <span class="o">=</span> <span class="s1">'dockerhub'</span> // Docker Hub credentials
        REGION <span class="o">=</span> <span class="s1">'ap-southeast-2'</span>  // AWS region
    <span class="o">}</span>
    stages <span class="o">{</span>
        stage<span class="o">(</span><span class="s1">'Clean Workspace'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                cleanWs<span class="o">()</span>
            <span class="o">}</span>
       <span class="o">}</span>
        stage<span class="o">(</span><span class="s1">'Checkout Code'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                git branch: <span class="s2">"</span><span class="k">${</span><span class="nv">GIT_BRANCH</span><span class="k">}</span><span class="s2">"</span>,
                    credentialsId: <span class="s1">'gittoken'</span>,
                    url: <span class="s2">"</span><span class="k">${</span><span class="nv">GIT_REPO_URL</span><span class="k">}</span><span class="s2">"</span>
            <span class="o">}</span>
        <span class="o">}</span>
        stage<span class="o">(</span><span class="s1">'Check Installed Package Versions'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                script <span class="o">{</span>
                    try <span class="o">{</span>
                        // Check Docker version
                        sh <span class="s1">'''
                            if command -v docker &gt;/dev/null 2&gt;&amp;1; then
                                echo "Docker Version: $(docker --version)"
                            else
                                echo "Docker is not installed"
                                exit 1
                            fi
                        '''</span>
                    <span class="o">}</span> catch <span class="o">(</span>Exception e<span class="o">)</span> <span class="o">{</span>
                        <span class="nb">echo</span> <span class="s2">"Error: Docker not found. </span><span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">"</span>
                    <span class="o">}</span>

                    try <span class="o">{</span>
                        // Check Terraform version
                        sh <span class="s1">'''
                            if command -v terraform &gt;/dev/null 2&gt;&amp;1; then
                                echo "Terraform Version: $(terraform -version)"
                            else
                                echo "Terraform is not installed"
                                exit 1
                            fi
                        '''</span>
                    <span class="o">}</span> catch <span class="o">(</span>Exception e<span class="o">)</span> <span class="o">{</span>
                        <span class="nb">echo</span> <span class="s2">"Error: Terraform not found. </span><span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">"</span>
                    <span class="o">}</span>

                    try <span class="o">{</span>
                        // Check Kubectl version
                        sh <span class="s1">'''
                            if command -v kubectl &gt;/dev/null 2&gt;&amp;1; then
                                echo "Kubectl Version: $(kubectl version --client)"
                            else
                                echo "Kubectl is not installed"
                                exit 1
                            fi
                        '''</span>
                    <span class="o">}</span> catch <span class="o">(</span>Exception e<span class="o">)</span> <span class="o">{</span>
                        <span class="nb">echo</span> <span class="s2">"Error: Kubectl not found. </span><span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">"</span>
                    <span class="o">}</span>

                    try <span class="o">{</span>
                        // Check Trivy version
                        sh <span class="s1">'''
                            if command -v trivy &gt;/dev/null 2&gt;&amp;1; then
                                echo "Trivy Version: $(trivy --version)"
                            else
                                echo "Trivy is not installed"
                                exit 1
                            fi
                        '''</span>
                    <span class="o">}</span> catch <span class="o">(</span>Exception e<span class="o">)</span> <span class="o">{</span>
                        <span class="nb">echo</span> <span class="s2">"Error: Trivy not found. </span><span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">"</span>
                    <span class="o">}</span>

                    try <span class="o">{</span>
                        // Check Ansible version
                        sh <span class="s1">'''
                            if command -v ansible &gt;/dev/null 2&gt;&amp;1; then
                                echo "Ansible Version: $(ansible --version)"
                            else
                                echo "Ansible is not installed"
                                exit 1
                            fi
                        '''</span>
                    <span class="o">}</span> catch <span class="o">(</span>Exception e<span class="o">)</span> <span class="o">{</span>
                        <span class="nb">echo</span> <span class="s2">"Error: Ansible not found. </span><span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">"</span>
                    <span class="o">}</span>

                    try <span class="o">{</span>
                        // Check AWS CLI version
                        sh <span class="s1">'''
                            if command -v aws &gt;/dev/null 2&gt;&amp;1; then
                                echo "AWS CLI Version: $(aws --version)"
                            else
                                echo "AWS CLI is not installed"
                                exit 1
                            fi
                        '''</span>
                    <span class="o">}</span> catch <span class="o">(</span>Exception e<span class="o">)</span> <span class="o">{</span>
                        <span class="nb">echo</span> <span class="s2">"Error: AWS CLI not found. </span><span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">"</span>
                    <span class="o">}</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>
        stage<span class="o">(</span><span class="s1">'Run a testing Ansible Playbook'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                script <span class="o">{</span>
                    // Run the Ansible playbook using the hosts file from the repo
                    sh <span class="s1">'''
                        echo "Running Ansible playbook:"
                        ansible-playbook -i "${WORKSPACE}/jenkins/terraform-ec2/hosts" "${WORKSPACE}/jenkins/terraform-ec2/test-playbook.yaml"
                    '''</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>
        stage<span class="o">(</span><span class="s1">'Verify AWS credential'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                withAWS<span class="o">(</span>credentials: <span class="s1">'aws'</span>, region: <span class="s1">'ap-southeast-2'</span><span class="o">)</span> <span class="o">{</span> // Replace with correct AWS credentials ID
                    script <span class="o">{</span>
                        // List all existing S3 buckets and output the result to the Jenkins console
                        sh <span class="s1">'''
                            echo "Listing all S3 buckets:"
                            aws s3 ls
                        '''</span>
                    <span class="o">}</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>
        stage<span class="o">(</span><span class="s1">'Terraform Init and Apply'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                withCredentials<span class="o">([[</span><span class="nv">$class</span>: <span class="s1">'AmazonWebServicesCredentialsBinding'</span>, credentialsId: <span class="s1">'aws'</span><span class="o">]])</span> <span class="o">{</span>
                    sh <span class="s1">'''
                        export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
                        export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
                        cd jenkins/terraform-ec2

                        # Check if Terraform has been initialized
                        if [ ! -d ".terraform" ]; then
                            echo "Terraform not initialized. Running '</span>terraform init<span class="s1">'..."
                            terraform init
                        else
                            echo "Terraform already initialized. Skipping '</span>terraform init<span class="s1">'."
                        fi

                        terraform apply -auto-approve -var "aws_region=${REGION}"
                    '''</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>
        // Stage to extract EC2 public IP
        stage<span class="o">(</span><span class="s1">'Extract EC2 Public IP'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                withCredentials<span class="o">([[</span><span class="nv">$class</span>: <span class="s1">'AmazonWebServicesCredentialsBinding'</span>, credentialsId: <span class="s1">'aws'</span><span class="o">]])</span> <span class="o">{</span>
                    script <span class="o">{</span>
                        def ec2Ip <span class="o">=</span> sh<span class="o">(</span>script: <span class="s1">'''
                            cd jenkins/terraform-ec2
                            terraform output -raw ec2_public_ip
                        '''</span>, returnStdout: <span class="nb">true</span><span class="o">)</span>.trim<span class="o">()</span>
                        <span class="nb">echo</span> <span class="s2">"EC2 Public IP: </span><span class="k">${</span><span class="nv">ec2Ip</span><span class="k">}</span><span class="s2">"</span>
                        // Set the environment variable <span class="k">for </span>the next stages explicitly
                        env.EC2_PUBLIC_IP <span class="o">=</span> ec2Ip
                    <span class="o">}</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>

        // <span class="k">**</span>Fix: Adding a small <span class="nb">sleep </span>to ensure <span class="nb">env </span>is populated<span class="k">**</span>
        stage<span class="o">(</span><span class="s1">'Validate EC2 Public IP'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                script <span class="o">{</span>
                    <span class="nb">sleep </span>2 // Ensure enough <span class="nb">time </span><span class="k">for </span>variable propagation
                    <span class="k">if</span> <span class="o">(</span>env.EC2_PUBLIC_IP <span class="o">==</span> null <span class="o">||</span> env.EC2_PUBLIC_IP <span class="o">==</span> <span class="s2">""</span><span class="o">)</span> <span class="o">{</span>
                        error <span class="s2">"EC2 Public IP is not available or failed to fetch."</span>
                    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
                        <span class="nb">echo</span> <span class="s2">"EC2 Public IP is successfully fetched: </span><span class="k">${</span><span class="nv">env</span><span class="p">.EC2_PUBLIC_IP</span><span class="k">}</span><span class="s2">"</span>
                    <span class="o">}</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>

        // Wait <span class="k">for </span>EC2 Readiness <span class="o">(</span>SSH Validation<span class="o">)</span>
        stage<span class="o">(</span><span class="s1">'Wait for EC2 Readiness'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                retry<span class="o">(</span>20<span class="o">)</span> <span class="o">{</span> // Retry up to 4 <span class="nb">times </span><span class="k">in case</span> EC2 is not immediately ready
                    <span class="nb">sleep </span>2  // Wait <span class="k">for </span>a bit before checking readiness
                    withCredentials<span class="o">([</span>sshUserPrivateKey<span class="o">(</span>credentialsId: <span class="s1">'sshkey'</span>, keyFileVariable: <span class="s1">'SSH_KEY'</span><span class="p">)</span><span class="o">])</span> <span class="o">{</span>
                        script <span class="o">{</span>
                            sh <span class="s2">"ssh -o StrictHostKeyChecking=no -i </span><span class="k">${</span><span class="nv">SSH_KEY</span><span class="k">}</span><span class="s2"> ubuntu@</span><span class="k">${</span><span class="nv">env</span><span class="p">.EC2_PUBLIC_IP</span><span class="k">}</span><span class="s2"> 'echo EC2 is ready for deployment'"</span>
                        <span class="o">}</span>
                    <span class="o">}</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>

        // Deploy Docker using Ansible
        stage<span class="o">(</span><span class="s1">'Deploy Docker with Ansible'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                withCredentials<span class="o">([</span>sshUserPrivateKey<span class="o">(</span>credentialsId: <span class="s1">'sshkey'</span>, keyFileVariable: <span class="s1">'SSH_KEY'</span><span class="o">)])</span> <span class="o">{</span>
                    script <span class="o">{</span>
                        sh <span class="s1">'''
                            echo "Running Ansible Playbook for Docker Deployment..."
                            ansible-playbook -i "${EC2_PUBLIC_IP}," "${WORKSPACE}/jenkins/terraform-ec2/deploy-docker-playbook.yml" \
                            --user ubuntu \
                            --private-key ${SSH_KEY} \
                            --extra-vars "ansible_ssh_private_key_file=${SSH_KEY} ec2_ip=${EC2_PUBLIC_IP}"
                        '''</span>
                    <span class="o">}</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>
        // New stage: Validate web blog accessibility
        stage<span class="o">(</span><span class="s1">'Validate Web Blog Access'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                script <span class="o">{</span>
                    <span class="nb">echo</span> <span class="s2">"Validating web blog access via http://</span><span class="k">${</span><span class="nv">env</span><span class="p">.EC2_PUBLIC_IP</span><span class="k">}</span><span class="s2">..."</span>

                    // Use curl to validate HTTP response from the web blog
                    def response <span class="o">=</span> sh<span class="o">(</span>script: <span class="s2">"curl -o /dev/null -s -w '%{http_code}' http://</span><span class="k">${</span><span class="nv">env</span><span class="p">.EC2_PUBLIC_IP</span><span class="k">}</span><span class="s2">"</span>, returnStdout: <span class="nb">true</span><span class="o">)</span>.trim<span class="o">()</span>

                    <span class="k">if</span> <span class="o">(</span>response <span class="o">==</span> <span class="s1">'200'</span><span class="o">)</span> <span class="o">{</span>
                        <span class="nb">echo</span> <span class="s2">"Web blog is accessible and returned HTTP status code 200."</span>
                    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
                        error <span class="s2">"Web blog is not accessible. HTTP status code: </span><span class="k">${</span><span class="nv">response</span><span class="k">}</span><span class="s2">"</span>
                    <span class="o">}</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span> 
        stage<span class="o">(</span><span class="s1">'delete terraform resource'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                withCredentials<span class="o">([[</span><span class="nv">$class</span>: <span class="s1">'AmazonWebServicesCredentialsBinding'</span>, credentialsId: <span class="s1">'aws'</span><span class="o">]])</span> <span class="o">{</span>
                    sh <span class="s1">'''
                        export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
                        export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
                        cd jenkins/terraform-ec2

                        # Check if Terraform has been initialized
                        if [ ! -d ".terraform" ]; then
                            echo "Terraform not initialized. Running '</span>terraform init<span class="s1">'..."
                            terraform init
                        else
                            echo "Terraform already initialized. Skipping '</span>terraform init<span class="s1">'."
                        fi

                        terraform destroy -auto-approve -var "aws_region=${REGION}"
                    '''</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>                   
    <span class="o">}</span>
    post <span class="o">{</span>
        success <span class="o">{</span>
            <span class="nb">echo</span> <span class="s2">"Pipeline completed successfully."</span>
        <span class="o">}</span>
        failure <span class="o">{</span>
            <span class="nb">echo</span> <span class="s2">"Pipeline failed."</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

<p>Terraform main.tf</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Terraform main.tf</span>

provider <span class="s2">"aws"</span> <span class="o">{</span>
  region <span class="o">=</span> <span class="s2">"ap-southeast-2"</span>
<span class="o">}</span>

terraform <span class="o">{</span>
  backend <span class="s2">"s3"</span> <span class="o">{</span>
    bucket         <span class="o">=</span> <span class="s2">"zz-lambda-tag"</span>
    key            <span class="o">=</span> <span class="s2">"terraform/state/terraform.tfstate"</span>
    region         <span class="o">=</span> <span class="s2">"ap-southeast-2"</span>
    encrypt        <span class="o">=</span> <span class="nb">true</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c"># Security Group Data</span>
data <span class="s2">"aws_security_group"</span> <span class="s2">"existing_sg"</span> <span class="o">{</span>
  filter <span class="o">{</span>
    name   <span class="o">=</span> <span class="s2">"group-name"</span>
    values <span class="o">=</span> <span class="o">[</span><span class="s2">"launch-wizard-1"</span><span class="o">]</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c"># Key Pair Data</span>
data <span class="s2">"aws_key_pair"</span> <span class="s2">"existing_key"</span> <span class="o">{</span>
  key_name <span class="o">=</span> <span class="s2">"zzzzzzzzzzzz"</span>
<span class="o">}</span>

<span class="c"># EC2 Instance Definition</span>
resource <span class="s2">"aws_instance"</span> <span class="s2">"web"</span> <span class="o">{</span>
  ami           <span class="o">=</span> <span class="s2">"ami-040e71e7b8391cae4"</span> <span class="c"># Choose AMI</span>
  instance_type <span class="o">=</span> <span class="s2">"t2.micro"</span>
  key_name      <span class="o">=</span> data.aws_key_pair.existing_key.key_name
  security_groups <span class="o">=</span> <span class="o">[</span>
    data.aws_security_group.existing_sg.name
  <span class="o">]</span>

  tags <span class="o">=</span> <span class="o">{</span>
    Name <span class="o">=</span> <span class="s2">"Jenkins-EC2"</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c"># Output EC2 Public IP</span>
output <span class="s2">"ec2_public_ip"</span> <span class="o">{</span>
  value <span class="o">=</span> aws_instance.web.public_ip
<span class="o">}</span></code></pre></figure>

<p>Ansible Playbook deploy-docker-playbook.yml</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Ansible Playbook for EC2 web blog deployment</span>
<span class="nt">---</span>
- hosts: all
  become: <span class="nb">yes
  </span>tasks:
  
    - name: Check <span class="k">if </span>Docker is already installed
      <span class="nb">command</span>: docker <span class="nt">--version</span>
      register: docker_installed
      ignore_errors: <span class="nb">yes
      </span>changed_when: <span class="nb">false</span>

    - name: Install required packages <span class="o">(</span><span class="k">if </span>Docker is not installed<span class="o">)</span>
      apt:
        name: 
          - apt-transport-https
          - ca-certificates
          - curl
          - software-properties-common
        state: present
        update_cache: <span class="nb">yes
      </span>when: docker_installed.rc <span class="o">!=</span> 0

    - name: Add Docker<span class="s1">'s official GPG key (if Docker is not installed)
      apt_key:
        url: https://download.docker.com/linux/ubuntu/gpg
        state: present
      when: docker_installed.rc != 0

    - name: Add Docker'</span>s official APT repository <span class="o">(</span><span class="k">if </span>Docker is not installed<span class="o">)</span>
      apt_repository:
        repo: deb <span class="o">[</span><span class="nb">arch</span><span class="o">=</span>amd64] https://download.docker.com/linux/ubuntu  stable
        state: present
      when: docker_installed.rc <span class="o">!=</span> 0

    - name: Update APT cache <span class="o">(</span><span class="k">if </span>Docker is not installed<span class="o">)</span>
      apt:
        update_cache: <span class="nb">yes
      </span>when: docker_installed.rc <span class="o">!=</span> 0

    - name: Install Docker CE <span class="o">(</span><span class="k">if </span>Docker is not installed<span class="o">)</span>
      apt:
        name: docker-ce
        state: present
        update_cache: <span class="nb">yes
      </span>when: docker_installed.rc <span class="o">!=</span> 0

    - name: Start and <span class="nb">enable </span>Docker service
      systemd:
        name: docker
        enabled: <span class="nb">yes
        </span>state: started

    - name: Stop all running containers
      shell: docker stop <span class="si">$(</span>docker ps <span class="nt">-q</span><span class="si">)</span>
      ignore_errors: <span class="nb">true
      </span>register: stopped_containers

    - name: Remove all stopped containers
      shell: docker <span class="nb">rm</span> <span class="si">$(</span>docker ps <span class="nt">-a</span> <span class="nt">-q</span><span class="si">)</span>
      when: stopped_containers.rc <span class="o">==</span> 0
      ignore_errors: <span class="nb">true</span>

    - name: Pull Docker image
      docker_image:
        name: zackz001/gitops-jekyll
        tag: latest
        <span class="nb">source</span>: pull

    - name: Run Docker container
      docker_container:
        name: zackblog
        image: zackz001/gitops-jekyll:latest
        state: started
        restart_policy: unless-stopped
        published_ports:
          - <span class="s2">"80:80"</span></code></pre></figure>

<ul>
  <li>Pipeline debug and testing</li>
</ul>

<p>After thorough testing and validation, the CD pipeline also works like a charm.</p>

<p><img src="/assets/jenkins2.png" alt="image tooltip here" /></p>

<p><b>Terraform Modularization for Multi-Destination Deployment</b></p>

<p>The folder structure bellow is designed to organize Infrastructure as Code (IaC) using Terraform, breaking down the configuration into reusable modules for ECS, EKS, and EC2 deployments, along with different environments (production, stage, etc.). So The Jenkins reusable CD pipeline can manage multi-destination deployments based on this structure.</p>

<ol>
  <li>Single EC2 deployment</li>
  <li>Single ECS deployment</li>
  <li>Terraform ECS Module with multi-environment deployment (production and stage)</li>
  <li>Terraform single EKS deployment</li>
  <li>Terraform EKS Module with multi-environment deployment (production and stage)</li>
</ol>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@zackz:~/zack-gitops-project/jenkins# tree

├── terraform-ec2
│   ├── Jenkinsfile
│   ├── deploy-docker-playbook.yml
│   ├── hosts
│   ├── main.tf
│   ├── test-playbook.yaml
│   └── variables.tf

├── module-ecs-cluster
│   ├── Jenkinsfile
│   ├── main.tf
│   ├── modules
│   │   ├── alb
│   │   │   ├── main.tf
│   │   │   ├── outputs.tf
│   │   │   └── variables.tf
│   │   ├── ecs_cluster
│   │   │   ├── main.tf
│   │   │   ├── outputs.tf
│   │   │   └── variables.tf
│   │   ├── ecs_service
│   │   │   ├── main.tf
│   │   │   ├── outputs.tf
│   │   │   └── variables.tf
│   │   ├── iam
│   │   │   ├── main.tf
│   │   │   ├── outputs.tf
│   │   │   └── variables.tf
│   │   ├── security_groups
│   │   │   ├── main.tf
│   │   │   ├── outputs.tf
│   │   │   └── variables.tf
│   │   └── task_definition
│   │       ├── main.tf
│   │       ├── outputs.tf
│   │       └── variables.tf
│   ├── outputs.tf
│   ├── terraform.tfstate
│   ├── terraform.tfstate.backup
│   ├── terraform.tfvars
│   └── variables.tf

├── module-ecs-env
│   ├── environments
│   │   ├── production
│   │   │   ├── Jenkinsfile
│   │   │   ├── backend.tf
│   │   │   ├── main.tf
│   │   │   ├── outputs.tf
│   │   │   ├── terraform.tfvars
│   │   │   └── variables.tf
│   │   └── stage
│   │       ├── Jenkinsfile
│   │       ├── backend.tf
│   │       ├── main.tf
│   │       ├── outputs.tf
│   │       ├── terraform.tfvars
│   │       └── variables.tf
│   └── modules
│       ├── alb
│       │   ├── main.tf
│       │   ├── outputs.tf
│       │   └── variables.tf
│       ├── ecs_cluster
│       │   ├── main.tf
│       │   ├── outputs.tf
│       │   └── variables.tf
│       ├── ecs_service
│       │   ├── main.tf
│       │   ├── outputs.tf
│       │   └── variables.tf
│       ├── iam
│       │   ├── main.tf
│       │   ├── outputs.tf
│       │   └── variables.tf
│       ├── security_groups
│       │   ├── main.tf
│       │   ├── outputs.tf
│       │   └── variables.tf
│       └── task_definition
│           ├── main.tf
│           ├── outputs.tf
│           └── variables.tf

└── terraform-eks
    ├── Jenkinsfile
    ├── argo-setup.sh
    ├── backend.tf
    ├── deployment.yaml
    ├── iam.tf
    ├── main.tf
    ├── output.tf
    ├── terraform.tfvars
    └── variables.tf

├── module-eks-env
│   ├── environments
│   │   ├── prod
│   │   │   ├── backend.tf
│   │   │   ├── main.tf
│   │   │   ├── output.tf
│   │   │   ├── provider.tf
│   │   │   └── variables.tf
│   │   └── stage
│   │       ├── backend.tf
│   │       ├── main.tf
│   │       ├── output.tf
│   │       ├── provider.tf
│   │       └── variables.tf
│   └── modules
│       └── eks
│           ├── main.tf
│           ├── output.tf
│           └── variables.tf</code></pre></figure>

<p><img src="/assets/jenkins3.png" alt="image tooltip here" /></p>

<p><b>Conclusion</b></p>

<p>Using Terraform and Jenkins practices enables efficient management of complex, multi-environment, and multi-service deployments, which are crucial for cloud-native CI/CD processes to achieve:</p>

<ol>
  <li>
    <p>Version Control: Allows tracking and managing infrastructure changes across different environments.</p>
  </li>
  <li>
    <p>Multi-Destination Deployment with Jenkins: Enables dynamic deployments to different environments (e.g., production, stage) by passing environment-specific parameters in the pipeline.</p>
  </li>
  <li>
    <p>Environment Separation: Each environment (production, stage) has its own Terraform configuration, ensuring proper isolation and customization.</p>
  </li>
  <li>
    <p>Terraform Modules Reusability: Reusable modules for infrastructure components (ECS, EKS, etc.) reduce code duplication and simplify updates.</p>
  </li>
  <li>
    <p>Multi-Environment and Multi-Component Deployment: Jenkins pipelines can deploy multiple services and environments concurrently by leveraging modular infrastructure and dynamic inputs.</p>
  </li>
</ol>

<p><b>Jenkins Recap Summary</b></p>

<p>Over this recap for Jenkins, I believe I had achieved :</p>

<ul>
  <li>
    <p><b>Multi-Stage, Multi-Environment Pipelines: </b>
These handle conditional execution using when blocks, try-catch for error handling, and post sections for notifications and cleanup.</p>
  </li>
  <li>
    <p><b>Integration with IaC Tools: </b>
Seamlessly provisions AWS resources using Terraform or CloudFormation within the pipeline.</p>
  </li>
  <li>
    <p><b>Security and Compliance: </b>
Integrates tools like Snyk, Trivy, and SonarQube to perform vulnerability scanning and code quality checks during the build.</p>
  </li>
  <li>
    <p><b>Secret Management: </b>
Securely manages sensitive data using AWS Secrets Manager, HashiCorp Vault, or Jenkins credentials plugin.</p>
  </li>
  <li>
    <p><b>Real-World Automation: </b>
Solves complex problems and improves efficiency, reducing build times and increasing deployment reliability.</p>
  </li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Jenkins CD Pipeline Design]]></summary></entry><entry><title type="html">Jenkins - Universal CI Pipeline with Ansible &amp;amp; Terraform</title><link href="http://localhost:4000/jekyll/cat2/2024/08/02/Jenkins-redo1-copy.html" rel="alternate" type="text/html" title="Jenkins - Universal CI Pipeline with Ansible &amp;amp; Terraform" /><published>2024-08-02T10:15:29+10:00</published><updated>2024-08-02T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/08/02/Jenkins-redo1%20copy</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/08/02/Jenkins-redo1-copy.html"><![CDATA[<p><b>Jenkins recap</b></p>

<p>It has been some time since I adapted CI/CD pipelines from Jenkins to AWS CodePipeline and GitHub Actions workflows. Now, it’s time to recap and improve some of my previous Jenkins practices.</p>

<ul>
  <li>Universal Jenkins Docker image design</li>
</ul>

<p>This time, instead of installing Jenkins on a server, I prefer to containerize a universal Jenkins Docker image with the necessary packages installed, so it provides consistency and reproducibility, portability, and easy updates and rollbacks.</p>

<ol>
  <li>Docker CLI</li>
  <li>Terraform</li>
  <li>Kubectl</li>
  <li>Trivy</li>
  <li>AWS CLI</li>
  <li>Ansible </li>
</ol>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Dockerfile</span>

FROM jenkins/jenkins:lts

USER root

<span class="c"># Install necessary packages, Docker CLI, Terraform, Kubectl, Trivy, AWS CLI, and Ansible</span>
RUN apt-get update <span class="o">&amp;&amp;</span> <span class="se">\</span>
    apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="se">\</span>
        curl <span class="se">\</span>
        wget <span class="se">\</span>
        unzip <span class="se">\</span>
        gnupg2 <span class="se">\</span>
        apt-transport-https <span class="se">\</span>
        lsb-release <span class="se">\</span>
        ca-certificates <span class="se">\</span>
        software-properties-common <span class="se">\</span>
        python3 <span class="se">\</span>
        python3-venv <span class="se">\</span>
        python3-pip <span class="o">&amp;&amp;</span> <span class="se">\</span>

    <span class="c"># Install Docker CLI</span>
    curl <span class="nt">-fsSL</span> https://download.docker.com/linux/debian/gpg | apt-key add - <span class="o">&amp;&amp;</span> <span class="se">\</span>
    <span class="nb">echo</span> <span class="s2">"deb [arch=amd64] https://download.docker.com/linux/debian </span><span class="si">$(</span>lsb_release <span class="nt">-cs</span><span class="si">)</span><span class="s2"> stable"</span> <span class="o">&gt;</span> /etc/apt/sources.list.d/docker.list <span class="o">&amp;&amp;</span> <span class="se">\</span>
    apt-get update <span class="o">&amp;&amp;</span> <span class="se">\</span>
    apt-get <span class="nb">install</span> <span class="nt">-y</span> docker-ce-cli <span class="o">&amp;&amp;</span> <span class="se">\</span>

    <span class="c"># Install Terraform</span>
    wget <span class="nt">-O-</span> https://apt.releases.hashicorp.com/gpg | gpg <span class="nt">--dearmor</span> <span class="o">&gt;</span> /usr/share/keyrings/hashicorp-archive-keyring.gpg <span class="o">&amp;&amp;</span> <span class="se">\</span>
    <span class="nb">echo</span> <span class="s2">"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com </span><span class="si">$(</span>lsb_release <span class="nt">-cs</span><span class="si">)</span><span class="s2"> main"</span> <span class="o">&gt;</span> /etc/apt/sources.list.d/hashicorp.list <span class="o">&amp;&amp;</span> <span class="se">\</span>
    apt-get update <span class="o">&amp;&amp;</span> <span class="se">\</span>
    apt-get <span class="nb">install</span> <span class="nt">-y</span> terraform <span class="o">&amp;&amp;</span> <span class="se">\</span>

    <span class="c"># Install Kubectl</span>
    curl <span class="nt">-LO</span> <span class="s2">"https://dl.k8s.io/release/</span><span class="si">$(</span>curl <span class="nt">-L</span> <span class="nt">-s</span> https://dl.k8s.io/release/stable.txt<span class="si">)</span><span class="s2">/bin/linux/amd64/kubectl"</span> <span class="o">&amp;&amp;</span> <span class="se">\</span>
    <span class="nb">chmod</span> +x kubectl <span class="o">&amp;&amp;</span> <span class="se">\</span>
    <span class="nb">mv </span>kubectl /usr/local/bin/ <span class="o">&amp;&amp;</span> <span class="se">\</span>

    <span class="c"># Install Trivy</span>
    wget https://github.com/aquasecurity/trivy/releases/download/v0.56.0/trivy_0.56.0_Linux-64bit.deb <span class="o">&amp;&amp;</span> <span class="se">\</span>
    dpkg <span class="nt">-i</span> trivy_0.56.0_Linux-64bit.deb <span class="o">&amp;&amp;</span> <span class="se">\</span>
    <span class="nb">rm </span>trivy_0.56.0_Linux-64bit.deb <span class="o">&amp;&amp;</span> <span class="se">\</span>

    <span class="c"># Install AWS CLI</span>
    curl <span class="s2">"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip"</span> <span class="nt">-o</span> <span class="s2">"awscliv2.zip"</span> <span class="o">&amp;&amp;</span> <span class="se">\</span>
    unzip awscliv2.zip <span class="o">&amp;&amp;</span> <span class="se">\</span>
    ./aws/install <span class="o">&amp;&amp;</span> <span class="se">\</span>
    <span class="nb">rm</span> <span class="nt">-rf</span> awscliv2.zip aws/ <span class="o">&amp;&amp;</span> <span class="se">\</span>

    <span class="c"># Create a virtual environment for Python and Ansible</span>
    python3 <span class="nt">-m</span> venv /opt/ansible_venv <span class="o">&amp;&amp;</span> <span class="se">\</span>
    /opt/ansible_venv/bin/pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip <span class="o">&amp;&amp;</span> <span class="se">\</span>
    /opt/ansible_venv/bin/pip <span class="nb">install </span>ansible <span class="o">&amp;&amp;</span> <span class="se">\</span>

    <span class="c"># Create symlinks to make Ansible easily accessible</span>
    <span class="nb">ln</span> <span class="nt">-s</span> /opt/ansible_venv/bin/ansible /usr/local/bin/ansible <span class="o">&amp;&amp;</span> <span class="se">\</span>
    <span class="nb">ln</span> <span class="nt">-s</span> /opt/ansible_venv/bin/ansible-playbook /usr/local/bin/ansible-playbook <span class="o">&amp;&amp;</span> <span class="se">\</span>

    <span class="c"># Clean up</span>
    apt-get clean <span class="o">&amp;&amp;</span> <span class="se">\</span>
    <span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/apt/lists/<span class="k">*</span>

USER jenkins</code></pre></figure>

<p>Build and run this universal Jenkins image to mount Jenkins home directory and Docker socket from the host to the container, also add Docker group to the container so Jenkins can run Docker commands inside the container without needing root privileges.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">docker build <span class="nt">-t</span> jenkins-all <span class="nb">.</span>

docker run <span class="nt">-d</span> <span class="nt">--name</span> jenkins <span class="nt">-p</span> 8080:8080 <span class="nt">-p</span> 50000:50000 <span class="se">\</span>
<span class="nt">-v</span> /var/run/docker.sock:/var/run/docker.sock <span class="se">\</span>
<span class="nt">-v</span> /var/jenkins_home:/var/jenkins_home <span class="se">\</span>
<span class="nt">--group-add</span> <span class="si">$(</span>getent group docker | <span class="nb">cut</span> <span class="nt">-d</span>: <span class="nt">-f3</span><span class="si">)</span> <span class="se">\ </span>
jenkins-all</code></pre></figure>

<ul>
  <li>Universal Jenkins CI pipeline design</li>
</ul>

<p>After installing a list of plugins and configuring all credentials and the GitHub webhook, the CI pipeline is designed bellow and can be triggered by a Git push event and will run automatically:</p>

<ol>
  <li>Enable multi-language artifact build support.</li>
  <li>Integrate testing of Java versions.</li>
  <li>Enable security checks for static code analysis and Docker image scanning.</li>
  <li>Implement advanced Jenkins pipeline structuring using <b>try-catch</b>, <b>if-else</b>, <b>timeouts</b>,<b>environment variables</b>,<b>post actions</b> and <b>error handling</b>.</li>
</ol>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c">#  Jenkinsfile</span>

// Define the detectJavaVersion <span class="k">function </span>outside of the pipeline block
def detectJavaVersion<span class="o">()</span> <span class="o">{</span>
    def javaVersionOutput <span class="o">=</span> sh<span class="o">(</span>script: <span class="s1">'java -version 2&gt;&amp;1'</span>, returnStatus: <span class="nb">false</span>, returnStdout: <span class="nb">true</span><span class="o">)</span>.trim<span class="o">()</span>
    def javaVersionMatch <span class="o">=</span> javaVersionOutput <span class="o">=</span>~ /openjdk version <span class="s2">"(</span><span class="se">\d</span><span class="s2">+</span><span class="se">\.\d</span><span class="s2">+)/

    if (javaVersionMatch) {
        def javaVersion = javaVersionMatch[0][1]

        if (javaVersion.startsWith("</span>1.8<span class="s2">")) {
            return '8'
        } else if (javaVersion.startsWith("</span>11<span class="s2">")) {
            return '11'
        } else if (javaVersion.startsWith("</span>17<span class="s2">")) {
            return '17'
        } else {
            error("</span>Unsupported Java version detected: <span class="k">${</span><span class="nv">javaVersion</span><span class="k">}</span><span class="s2">")
        }
    } else {
        error("</span>Java version information not found <span class="k">in </span>output.<span class="s2">")
    }
}

pipeline {
    agent any
    environment {
        REGISTRY_URL = 'https://index.docker.io/v1/'
        IMAGE_NAME = "</span>zackz001/jenkins<span class="s2">"
        IMAGE_TAG = "</span><span class="k">${</span><span class="nv">env</span><span class="p">.BUILD_NUMBER</span><span class="k">}</span><span class="s2">"
        LATEST_TAG = "</span>latest<span class="s2">"
        TRIVY_OUTPUT = "</span>trivy-report.txt<span class="s2">"
        EMAIL_RECIPIENT = "</span>zhbsoftboy1@gmail.com<span class="s2">"
        GIT_REPO_URL = 'https://github.com/ZackZhouHB/zack-gitops-project.git'  // Git repository URL
        GIT_BRANCH = 'jenkins'  // Git branch
        DOCKERHUB_CREDENTIALS_ID = 'dockerhub' // Docker Hub credentials
        SONAR_TOKEN = 'sonar'  // Fetch Sonar token securely
        SNYK_INSTALLATION = 'snyk' // Replace with your Snyk installation
        SNYK_TOKEN = 'snyktoken'  // Fetch Snyk token securely
    }
    stages {
        stage('Clean Workspace') {
            steps {
                cleanWs()
            }
        }   
        stage('Checkout Code') {
            steps {
                git branch: "</span><span class="k">${</span><span class="nv">GIT_BRANCH</span><span class="k">}</span><span class="s2">",
                    credentialsId: 'gittoken',
                    url: "</span><span class="k">${</span><span class="nv">GIT_REPO_URL</span><span class="k">}</span><span class="s2">"
            }   
        }
        stage('Detect and Set Java') {
            steps {
                script {
                    try {
                        def javaVersion = detectJavaVersion()  // Detect the Java version, e.g., "</span>17<span class="s2">"
                        def javaToolName = "</span>Java_<span class="k">${</span><span class="nv">javaVersion</span><span class="k">}</span><span class="s2">"  // Expected tool name

                        // Try to set the Java version; fallback if the specific version isn't found
                        try {
                            tool name: javaToolName, type: 'jdk'
                            echo "</span>Using Java version <span class="k">${</span><span class="nv">javaVersion</span><span class="k">}</span>.<span class="s2">"
                        } catch (Exception toolError) {
                            echo "</span>No JDK named <span class="k">${</span><span class="nv">javaToolName</span><span class="k">}</span> found. Using default JDK.<span class="s2">"
                        }

                        // Verify Java version, regardless of whether the specific version was found
                        sh 'java --version'

                    } catch (Exception e) {
                        echo "</span>Error during Java version detection: <span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">"
                        // Continue pipeline even if Java detection fails
                    }
                }
            }
        }
        // for Code Security Analysis and Fixes
        stage('snyk_analysis') {
            steps {
                script {
                    echo 'Running Snyk security analysis...'
                    timeout(time: 5, unit: 'MINUTES') {  // Adjust the timeout value as necessary
                        try {
                            snykSecurity(
                                snykInstallation: SNYK_INSTALLATION,
                                snykTokenId: SNYK_TOKEN,
                                failOnIssues: false,
                                monitorProjectOnBuild: true,
                                additionalArguments: '--severity-threshold=low'
                            )
                       } catch (Exception e) {
                            currentBuild.result = 'FAILURE'
                            error("</span>Error during snyk_analysis: <span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">")
                        }
                    }
                }
            }
        }
        
        // Language-specific build and test stages
        stage('Frontend Build and Test') {
            steps {
                script {
                    try {
                        if (fileExists('package.json')) {
                            sh 'npm install --force'
                            sh 'npm test'
                        } else {
                            echo 'No package.json found, skipping Frontend build and test.'
                        }
                    } catch (Exception e) {
                        currentBuild.result = 'FAILURE'
                        error("</span>Error during Frontend build and <span class="nb">test</span>: <span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">")
                    }
                }
            }
        }

        stage('Java Spring Boot Build and Test') {
            steps {
                script {
                    try {
                        if (fileExists('pom.xml')) {
                            sh 'mvn clean package'
                            sh 'mvn test'
                        } else {
                            echo 'No pom.xml found, skipping Java Spring Boot build and test.'
                        }
                    } catch (Exception e) {
                        currentBuild.result = 'FAILURE'
                        error("</span>Error during Java Spring Boot build and <span class="nb">test</span>: <span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">")
                    }
                }
            }
        }

        stage('.NET Build and Test') {
            steps {
                script {
                    try {
                        if (fileExists('YourSolution.sln')) {
                            sh 'dotnet build'
                            sh 'dotnet test'
                        } else {
                            echo 'No YourSolution.sln found, skipping .NET build and test.'
                        }
                    } catch (Exception e) {
                        currentBuild.result = 'FAILURE'
                        error("</span>Error during .NET build and <span class="nb">test</span>: <span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">")
                    }
                }
            }
        }

        stage('PHP Build and Test') {
            steps {
                script {
                    try {
                        if (fileExists('composer.json')) {
                            sh 'composer install'
                            sh 'phpunit'
                        } else {
                            echo 'No composer.json found, skipping PHP build and test.'
                        }
                    } catch (Exception e) {
                        currentBuild.result = 'FAILURE'
                        error("</span>Error during PHP build and <span class="nb">test</span>: <span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">")
                    }
                }
            }
        }

        stage('iOS Build and Test') {
            steps {
                script {
                    try {
                        if (fileExists('YourProject.xcodeproj')) {
                            xcodebuild(buildDir: 'build', scheme: 'YourScheme')
                        } else {
                            echo 'No YourProject.xcodeproj found, skipping iOS build and test.'
                        }
                    } catch (Exception e) {
                        currentBuild.result = 'FAILURE'
                        error("</span>Error during iOS build and <span class="nb">test</span>: <span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">")
                    }
                }
            }
        }

        stage('Android Build and Test') {
            steps {
                script {
                    try {
                        if (fileExists('build.gradle')) {
                            sh './gradlew build'
                            sh './gradlew test'
                        } else {
                            echo 'No build.gradle found, skipping Android build and test.'
                        }
                    } catch (Exception e) {
                        currentBuild.result = 'FAILURE'
                        error("</span>Error during Android build and <span class="nb">test</span>: <span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">")
                    }
                }
            }
        }

        stage('Ruby on Rails Build and Test') {
            steps {
                script {
                    try {
                        if (fileExists('Gemfile.lock')) {
                            sh 'bundle install'
                            sh 'bundle exec rake db:migrate'
                            sh 'bundle exec rails test'
                        } else {
                            echo 'No Gemfile.lock found, skipping Ruby on Rails build and test.'
                        }
                    } catch (Exception e) {
                        currentBuild.result = 'FAILURE'
                        error("</span>Error during Ruby on Rails build and <span class="nb">test</span>: <span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">")
                    }
                }
            }
        }

        stage('Flask Build and Test') {
            steps {
                script {
                    try {
                        if (fileExists('app.py')) {
                            sh 'pip install -r requirements.txt'
                            sh 'python -m unittest discover'
                        } else {
                            echo 'No app.py found, skipping Flask build and test.'
                        }
                    } catch (Exception e) {
                        currentBuild.result = 'FAILURE'
                        error("</span>Error during Flask build and <span class="nb">test</span>: <span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">")
                    }
                }
            }
        }

        stage('Django Build and Test') {
            steps {
                script {
                    try {
                        if (fileExists('manage.py')) {
                            sh 'pip install -r requirements.txt'
                            sh 'python manage.py migrate'
                            sh 'python manage.py test'
                        } else {
                            echo 'No manage.py found, skipping Django build and test.'
                        }
                    } catch (Exception e) {
                        currentBuild.result = 'FAILURE'
                        error("</span>Error during Django build and <span class="nb">test</span>: <span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">")
                    }
                }
            }
        }

        stage('Rust Build and Test') {
            steps {
                script {
                    try {
                        if (fileExists('Cargo.toml')) {
                            env.RUST_BACKTRACE = 'full'
                            sh 'cargo build'
                            sh 'cargo test'
                        } else {
                            echo "</span>No Cargo.toml file found. Skipping Rust build and test.<span class="s2">"
                        }
                    } catch (Exception e) {
                        currentBuild.result = 'FAILURE'
                        error("</span>Error during Rust build and <span class="nb">test</span>: <span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">")
                    }
                }
            }
        }

        stage('Ruby Sinatra Build and Test') {
            steps {
                script {
                    try {
                        if (fileExists('app.rb')) {
                            sh 'gem install bundler'
                            sh 'bundle install'
                            sh 'bundle exec rake test'
                        } else {
                            echo "</span>No app.rb file found. Skipping Ruby Sinatra build and test.<span class="s2">"
                        }
                    } catch (Exception e) {
                        currentBuild.result = 'FAILURE'
                        error("</span>Error during Ruby Sinatra build and <span class="nb">test</span>: <span class="k">${</span><span class="nv">e</span><span class="p">.message</span><span class="k">}</span><span class="s2">")
                    }
                }
            }
        }
        // Build ZackBlog docker image 
        stage('Check and Build Docker Image') {
            steps {
                script {
                    try {
                        // Check if Docker is available
                        sh 'docker --version'
                        echo "</span>Docker is installed. Proceeding to build the Docker image...<span class="s2">"
                        
                        // Build the Docker image from the 'zack_blog' folder
                        dockerImage = docker.build("</span><span class="k">${</span><span class="nv">IMAGE_NAME</span><span class="k">}</span>:<span class="k">${</span><span class="nv">IMAGE_TAG</span><span class="k">}</span><span class="s2">", "</span>zack_blog/<span class="s2">")
                    } catch (Exception e) {
                        // Handle the error if Docker is not available
                        error("</span>Docker is not installed or accessible. Cannot proceed with the build.<span class="s2">")
                    }
                }
            }
        }
        // Scan docker image with Trivy
        stage('Docker Image Scan') {
            steps {
                // Use Trivy to scan the built Docker image
                sh "</span>trivy image <span class="nt">--severity</span> HIGH,CRITICAL <span class="k">${</span><span class="nv">IMAGE_NAME</span><span class="k">}</span>:<span class="k">${</span><span class="nv">IMAGE_TAG</span><span class="k">}</span> <span class="o">&gt;</span> <span class="k">${</span><span class="nv">TRIVY_OUTPUT</span><span class="k">}</span><span class="s2">"
            }
        }
        //Push to Dockerhub
        stage('Push Docker Image to DockerHub') {
            steps {
                script {
                    docker.withRegistry("</span><span class="k">${</span><span class="nv">REGISTRY_URL</span><span class="k">}</span><span class="s2">", "</span><span class="k">${</span><span class="nv">DOCKERHUB_CREDENTIALS_ID</span><span class="k">}</span><span class="s2">") {
                        dockerImage.push("</span><span class="k">${</span><span class="nv">IMAGE_TAG</span><span class="k">}</span><span class="s2">")
                        dockerImage.push("</span><span class="k">${</span><span class="nv">LATEST_TAG</span><span class="k">}</span><span class="s2">") // Push 'latest' tag
                    }
                }
            }
        }
        //Output image scan result
        stage('Display Trivy Scan Results') {
            steps {
                script {
                    // Display the contents of the Trivy report
                    def scanReport = readFile("</span><span class="k">${</span><span class="nv">TRIVY_OUTPUT</span><span class="k">}</span><span class="s2">")
                    echo "</span>Trivy Scan Report:<span class="se">\n</span><span class="k">${</span><span class="nv">scanReport</span><span class="k">}</span><span class="s2">"
                }
            }
        }
        // Additional stages like Docker build, image scan, etc.
    }
    // Post Build Emailing 
    post {
        success {
            script {
                def scanReport = readFile("</span><span class="k">${</span><span class="nv">TRIVY_OUTPUT</span><span class="k">}</span><span class="s2">")
                emailext(
                    to: "</span><span class="k">${</span><span class="nv">EMAIL_RECIPIENT</span><span class="k">}</span><span class="s2">",
                    subject: "</span>CI Pipeline Success: Build <span class="k">${</span><span class="nv">IMAGE_TAG</span><span class="k">}</span><span class="s2">",
                    body: """</span>
                    The pipeline has successfully completed.

                    Docker image <span class="k">${</span><span class="nv">IMAGE_NAME</span><span class="k">}</span>:<span class="k">${</span><span class="nv">IMAGE_TAG</span><span class="k">}</span> has been built and pushed to DockerHub.

                    Trivy Scan Report:
                    <span class="k">${</span><span class="nv">scanReport</span><span class="k">}</span>
                    <span class="s2">"""
                )
            }
        }
        failure {
            emailext(
                to: "</span><span class="k">${</span><span class="nv">EMAIL_RECIPIENT</span><span class="k">}</span><span class="s2">",
                subject: "</span>CI Pipeline Failed: Build <span class="k">${</span><span class="nv">IMAGE_TAG</span><span class="k">}</span><span class="s2">",
                body: """</span>
                The pipeline has failed at some stage.

                Please check the Jenkins console logs <span class="k">for </span>more details.
                <span class="s2">"""
            )
        }
    }
}</span></code></pre></figure>

<ul>
  <li>Pipeline test and debug</li>
</ul>

<p>After thorough testing and validation, the CI pipeline finally works like a charm.</p>

<p><img src="/assets/jenkins1.png" alt="image tooltip here" /></p>

<p>Next, I will create a CD pipeline to integrate with Ansible, AWS, and Terraform to deploy the blog onto AWS EC2, ECS, and EKS.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Jenkins recap]]></summary></entry><entry><title type="html">RedHat Identity Management (IdM) with AD Intergration</title><link href="http://localhost:4000/jekyll/cat2/2024/07/13/Redhatidm.html" rel="alternate" type="text/html" title="RedHat Identity Management (IdM) with AD Intergration" /><published>2024-07-13T10:15:29+10:00</published><updated>2024-07-13T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/07/13/Redhatidm</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/07/13/Redhatidm.html"><![CDATA[<p><b> About Linux Identity Management </b></p>

<p>When a company faces challenge to manage its Linux environments across local and public cloud, RedHat Identity management can be the solution to achieve:</p>

<ul>
  <li>
    <p>With Local AD and Azure AD (AAD) Integration</p>
  </li>
  <li>
    <p>With AWS SSO Integration as externel identity provider</p>
  </li>
  <li>
    <p>LDAP, Kerberos and NTP</p>
  </li>
  <li>
    <p>A web-based management front-end running on Apache</p>
  </li>
</ul>

<p><b> A Typical AD User Authentication Flow End-to-End: </b></p>

<p>User Creation and Management:</p>

<ul>
  <li>
    <p>Azure AD / Local AD: Users are created in the Azure Active Directory or local Active Directory.</p>
  </li>
  <li>
    <p>Synchronization to RedHat IdM: The users are synchronized from AD to RedHat IdM using the two-way trust established between AD and IdM.</p>
  </li>
</ul>

<p>Accessing EC2 Instances via SSH:</p>

<ul>
  <li>User Sync to RedHat IdM: Users synchronized to RedHat IdM are assigned roles and permissions, including SSH access to specific EC2 instances.</li>
</ul>

<p>Host-Based Access Control (HBAC):</p>

<ul>
  <li>
    <p>HBAC Rules: RedHat IdM enforces HBAC rules to control which users can access specific EC2 instances.</p>
  </li>
  <li>
    <p>SSH Access Control: When a user attempts to SSH into an EC2 instance, RedHat IdM verifies the user’s identity and permissions, allowing or denying access based on the defined HBAC rules.</p>
  </li>
</ul>

<p><b> The design:</b></p>

<p>For Idm on AWS, configure the security groups to allow ports required by IdM. IdM desires below to be open:</p>

<p>HTTP/HTTPS — 80, 443 — TCP</p>

<p>LDAP/LDAPS — 389, 636 — TCP</p>

<p>Kerberos — 88, 464 — Both TCP and UDP</p>

<p>DNS — 53 — Both TCP and UDP</p>

<p>NTP — 123 — UDP</p>

<p>Here I am going to:</p>

<ul>
  <li>
    <p>install and configure a local freeIPA server</p>
  </li>
  <li>
    <p>enroll 2 Linux client machines (both CentOS and Ubuntu)</p>
  </li>
  <li>
    <p>Setup a local AD,</p>
  </li>
  <li>
    <p>build a 2 way trust between idm and AD</p>
  </li>
  <li>
    <p>Validate IDM and AD user to ssh into idm client machines.</p>
  </li>
</ul>

<p><b> Prerequisites: </b></p>

<ul>
  <li>
    <p>Windows AD Domain <b> ad.zack.world</b> and Idm Domain <b> ipa.zack.world</b></p>
  </li>
  <li>
    <p>Windows AD: 11.0.1.181 dc01.ad.zack.world (win server 2019)</p>
  </li>
  <li>
    <p>Windows client1: 11.0.1.182 win-client.ad.zack.world (win server 2019)</p>
  </li>
  <li>
    <p>idm Server: 11.0.1.180 server1.ipa.zack.world (CentOS 9)</p>
  </li>
  <li>
    <p>idm Client2: 11.0.1.184 ubt-client02.ipa.zack.world (Ubuntu 24.04)</p>
  </li>
  <li>
    <p>idm Client3: 11.0.1.185 idm-client3-centos7.ipa.zack.world (CentOS 9)</p>
  </li>
</ul>

<p><b> FreeIPA Installation </b></p>

<p>On freeIPA Server server1.ipa.zack.world 11.0.1.180 (CentOS 9):</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># set hostname, IP and DNS</span>
hostnamectl set-hostname server1.ipa.zack.world

<span class="c"># add 3 hosts to /etc/hosts</span>
<span class="nb">echo </span>11.0.1.180 server1.ipa.zack.world  ipa <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.184 ubt-client02.ipa.zack.world ipa <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.185  idm-client3-centos7.ipa.zack.world  ipa <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.181  dc01.ad.zack.world ipa <span class="o">&gt;&gt;</span> /etc/hosts

<span class="c"># install ipa-server</span>
dnf <span class="nt">-y</span> <span class="nb">install </span>freeipa-server freeipa-server-dns freeipa-client

<span class="c"># Configure ipa-server and DNS, here set ipa console and domain admin password</span>
ipa-server-install <span class="nt">--setup-dns</span>

<span class="c"># confirm or change NetBIOS domain name</span>
NetBIOS domain name <span class="o">[</span>IPA]: IPA01

The ipa-server-install <span class="nb">command </span>was successful.

<span class="c"># Configure firewall rules and services</span>
firewall-cmd <span class="nt">--add-service</span><span class="o">={</span>freeipa-ldap,freeipa-ldaps,dns,ntp<span class="o">}</span>
firewall-cmd <span class="nt">--runtime-to-permanent</span>
firewall-cmd <span class="nt">--reload</span> 

<span class="c"># check ipastatus</span>
<span class="o">[</span>root@freeipa ~]# ipactl status
Directory Service: RUNNING
krb5kdc Service: RUNNING
kadmin Service: RUNNING
httpd Service: RUNNING
ipa-custodia Service: RUNNING
ntpd Service: RUNNING
pki-tomcatd Service: RUNNING
ipa-otpd Service: RUNNING
ipa: INFO: The ipactl <span class="nb">command </span>was successful

<span class="c"># Obtain a Kerberos ticket for the Kerberos admin user and Verify the ticket</span>
kinit admin
klist

Ticket cache: KEYRING:persistent:0:0
Default principal: admin@ZACKZ.OONLINE

Valid starting     Expires            Service principal
07/13/24 22:17:29  07/14/24 22:02:43  HTTP/server1.ipa.zack.world@IPA.ZACK.WORLD

<span class="c"># check content of /etc/resolv.conf</span>
<span class="nb">cat</span> /etc/resolv.conf
search ipa.zack.world
nameserver 127.0.0.1

<span class="c"># Configure default login shell to Bash and Create User tina</span>
ipa config-mod <span class="nt">--defaultshell</span><span class="o">=</span>/bin/bash
ipa user-add tina <span class="nt">--first</span><span class="o">=</span>tina <span class="nt">--last</span><span class="o">=</span>qi <span class="nt">--password</span></code></pre></figure>

<p><img src="/assets/idm3.png" alt="image tooltip here" /></p>

<p><b> Idm client Enrollment </b></p>

<p>Now the idm web portal should be accessible, by adding “11.0.1.180 server1.ipa.zack.world” into local “c:/wondows/system32/drivers/etc/hosts.</p>

<p>Then enrol both centos and Ubuntu IDM client machines</p>

<ul>
  <li>On FreeIPA Server, add DNS entry for FreeIPA Client machines</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># ipa dnsrecord-add [domain name] [record name] [record type] [record]</span>
ipa dnsrecord-add ipa.zack.world idm-client3-centos7 <span class="nt">--a-rec</span> 11.0.1.185
ipa dnsrecord-add ipa.zack.world ubt-client02 <span class="nt">--a-rec</span> 11.0.1.184

- <span class="nb">set </span>IP, <span class="nb">hostname</span>, DNS on idm client
<span class="c"># set idm server ID as client DNS</span>
nmcli connection modify ens33 ipv4.dns 11.0.1.180
nmcli connection up ens33

<span class="c"># Install FreeIPA Client packages.</span>
dnf <span class="nt">-y</span> <span class="nb">install </span>freeipa-client

<span class="c"># enrol client to idm server with domain name</span>
ipa-client-install <span class="nt">--server</span><span class="o">=</span>server1.ipa.zack.world <span class="nt">--domain</span> ipa.zack.world

Enrolled <span class="k">in </span>IPA realm IPA.ZACK.WORLD
Configuring ipa.zack.world as NIS domain.
Client configuration complete.
The ipa-client-install <span class="nb">command </span>was successful

<span class="c"># set create home directory at initial login</span>
authselect enable-feature with-mkhomedir
systemctl <span class="nb">enable</span> <span class="nt">--now</span> oddjobd

<span class="c"># same as Ubuntu client</span>
<span class="nb">echo </span>11.0.1.180 server1.ipa.zack.world server1 <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.184 ubt-client02.ipa.zack.world ubt-client02 <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.185 idm-client3-centos7.ipa.zack.world idm-client3-centos7 <span class="o">&gt;&gt;</span> /etc/hosts

<span class="c"># Edit host file and install client, then enrol into idm server domain</span>
apt update <span class="o">&amp;&amp;</span> apt <span class="nb">install </span>freeipa-client oddjob-mkhomedir <span class="nt">-y</span>
ipa-client-install <span class="nt">--server</span><span class="o">=</span>server1.ipa.zack.world <span class="nt">--domain</span> ipa.zack.world

Client configuration complete.
The ipa-client-install <span class="nb">command </span>was successful</code></pre></figure>

<p><img src="/assets/idm2.png" alt="image tooltip here" /></p>

<p><b> Setup idm and AD trust</b></p>

<p>On Windows DC, setup AD</p>

<ul>
  <li>
    <p>install ADDC role and feature</p>
  </li>
  <li>
    <p>create forest “ad.zack.world”</p>
  </li>
  <li>
    <p>promote to primary DC</p>
  </li>
  <li>
    <p>test AD to join Windows client machine to domain</p>
  </li>
  <li>
    <p>create AD user joez@ad.zack.world</p>
  </li>
  <li>
    <p>add idm domain to Windows AD zones</p>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># dnscmd 127.0.0.1 /ZoneAdd [FreeIPA domain name] /Secondary [FreeIPA IP address]</span>
C:<span class="se">\U</span>sers<span class="se">\A</span>dministrator&gt;dnscmd 127.0.0.1 /ZoneAdd ipa.zack.world /Secondary 11.0.1.180
DNS Server 127.0.0.1 created zone ipa.zack.world:

Command completed successfully.

<span class="c"># Verify both AD and Idm DNS resolution, then setup trust</span>
dig SRV _ldap._tcp.ipa.zack.world
dig SRV _ldap._tcp.ad.zack.world</code></pre></figure>

<p><img src="/assets/idm1.png" alt="image tooltip here" /></p>

<ul>
  <li>Install required packages then setup trust on FreeIPA Server</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Install packages</span>
dnf <span class="nt">-y</span> <span class="nb">install </span>ipa-server-trust-ad
<span class="c"># setup ad trust</span>
ipa-adtrust-install

<span class="c"># FreeIPA admin password</span>
admin password:
<span class="o">=============================================================================</span>
Setup <span class="nb">complete</span>

<span class="c"># add firewall service and ports for ad trust</span>
firewall-cmd <span class="nt">--add-service</span><span class="o">=</span>freeipa-trust

firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>135/tcp
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>138/tcp
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>139/tcp
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>445/tcp
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>1024-1300/tcp
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>3268/tcp
<span class="c"># Open UDP ports</span>
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>138/udp
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>139/udp
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>389/udp
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>445/udp
<span class="c"># Open TCP ports</span>
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>80/tcp
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>443/tcp
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>389/tcp
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>636/tcp
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>88/tcp
<span class="nb">sudo </span>firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>464/tcp
<span class="nb">sudo </span>firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>53/tcp
<span class="c"># Open UDP ports</span>
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>88/udp
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>464/udp
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>53/udp
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-port</span><span class="o">=</span>123/udp

firewall-cmd <span class="nt">--reload</span>

<span class="c"># Configure DNS Setting on FreeIPA Server</span>
<span class="c"># ipa dnsforwardzone-add [AD domain name] --forwarder=[AD IP address] --forward-policy=only</span>
ipa dnsforwardzone-add ad.zack.world <span class="nt">--forwarder</span><span class="o">=</span>11.0.1.181 <span class="nt">--forward-policy</span><span class="o">=</span>only
<span class="c"># ipa dnszone-mod [IPA domain name] --allow-transfer=[AD IP address]</span>
ipa dnszone-mod ipa.zack.world <span class="nt">--allow-transfer</span><span class="o">=</span>11.0.1.181

<span class="c"># ipa trust-add --type=ad [AD domain name] --admin Administrator --password</span>
ipa trust-add <span class="nt">--two-way</span><span class="o">=</span><span class="nb">true</span> <span class="nt">--type</span><span class="o">=</span>ad ad.zack.world <span class="nt">--admin</span> Administrator <span class="nt">--password</span>
Active Directory domain administrator<span class="s1">'s password:
-----------------------------------------------------
Added Active Directory trust for realm "ad.zack.world"
-----------------------------------------------------
 Realm name: ad.zack.world
 Domain NetBIOS name: AD01
 Domain Security Identifier: S-1-5-21-726412840-3773945212-2352305327
 Trust direction: Two-way trust
 Trust type: Active Directory domain
 Trust status: Established and verified

# set  home directory at initial login
authselect enable-feature with-mkhomedir
systemctl enable --now oddjobd</span></code></pre></figure>

<p><img src="/assets/idm4.png" alt="image tooltip here" /></p>

<p><b> Validation of both idm clients with idm and AD user</b></p>

<ul>
  <li>Validate ssh into ubuntu client with AD user “joez@ad.zack.world”</li>
</ul>

<p><img src="/assets/idm6.png" alt="image tooltip here" /></p>

<p>Validate ssh into Centos client with idm user “tina”</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">login as: tina
Keyboard-interactive authentication prompts from server:
| Password:
End of keyboard-interactive prompts from server
Last login: Sun Jul 14 20:48:42 2024 from 11.0.1.1
<span class="o">[</span>tina@idm-client3-centos7 ~]<span class="nv">$ </span><span class="nb">id
</span><span class="nv">uid</span><span class="o">=</span>1240000004<span class="o">(</span>tina<span class="o">)</span> <span class="nv">gid</span><span class="o">=</span>1240000004<span class="o">(</span>tina<span class="o">)</span> <span class="nb">groups</span><span class="o">=</span>1240000004<span class="o">(</span>tina<span class="o">)</span> 
<span class="nv">context</span><span class="o">=</span>unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023</code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we install Redhat IdM server and can enrol client hosts, set up AD trust, ssh and authenticate with both idm and AD users. IdM using Kerberos for authentication, together with user group, policy, HBAC and Sudo roles, provides a flexible and robust authentication framework that supports multiple authentication mechanisms, enabling organizations to authenticate users securely across their Linux and Unix environments.</p>

<p>More info can be found via <a href="https://freeipa.readthedocs.io/en/latest/workshop.html">Freeipa workshop</a>, <a href="https://www.server-world.info/en/note?os=CentOS_Stream_9&amp;p=freeipa&amp;f=8">FreeIPA:FreeIPA trust AD</a>, <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_idm_users_groups_hosts_and_access_control_rules/index">Red Hat product documentation</a>, <a href="https://chamathb.wordpress.com/2019/06/21/setting-up-rhel-idm-with-integrated-dns-on-aws/">Redhat Idm on AWS with DNS forwarder</a>, <a href="https://www.reddit.com/r/redhat/comments/6ixtoe/idmfreeipa_dns_forwarding/">idmfreeipa DNS forwarder configurations on AWS</a>, and <a href="https://redhat.com/en/blog/automating-red-hat-identity-management-installation">Automating Red Hat Identity Management installation with Ansible</a>.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Linux Identity Management]]></summary></entry><entry><title type="html">Serverless with AWS Fargate</title><link href="http://localhost:4000/jekyll/cat2/2024/07/10/aws-fargate.html" rel="alternate" type="text/html" title="Serverless with AWS Fargate" /><published>2024-07-10T10:15:29+10:00</published><updated>2024-07-10T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/07/10/aws-fargate</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/07/10/aws-fargate.html"><![CDATA[<p><b>Why go serverless</b></p>

<p>Some of the company’s applications recently moved from Rancher to Fargate, which is understandable as the cloud resource and traffic will be very intensive only during a certain period (HSC exam), hence AWS serverless with Fargate can be a better option for such business mode so rest of the year without exam we can save cost significantly.</p>

<p><b>Hosting our blog on Fargate? Why not!</b></p>

<p>In the past, I used to try different methods to host this blog:</p>

<ul>
  <li><a href="https://zackz.site/jekyll/cat2/2023/11/02/about-this-project.html">EC2 with docker</a></li>
  <li><a href="https://zackz.site/jekyll/cat2/2023/11/07/ArgoCD.html">K8s with ArgoCD</a></li>
  <li><a href="https://zackz.site/jekyll/cat2/2024/04/30/serverless.html">S3 with static website</a></li>
  <li><a href="https://zackz.site/jekyll/cat2/2024/05/12/Helm.html">Customize Helm Chart for Zack’ Blog</a></li>
</ul>

<p>Here I will use AWS Fargate, together with AWS ECR, Docker, Terraform and Github Action workflow to move this blog to AWS serverless compute for containers.</p>

<ul>
  <li>Terraform Provisioning</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Provider Configuration  "provider.tf"</span>
provider <span class="s2">"aws"</span> <span class="o">{</span>
 region <span class="o">=</span> <span class="s2">"ap-southeast-2"</span>
<span class="o">}</span>

<span class="c"># Create an ECR Repository "ecr.tf"</span>
resource <span class="s2">"aws_ecr_repository"</span> <span class="s2">"zackblog_repo"</span> <span class="o">{</span>
 name <span class="o">=</span> <span class="s2">"zackblog-repo"</span>
<span class="o">}</span>

<span class="c"># Fargate Task Definition  "task_definition.tf"</span>
resource <span class="s2">"aws_ecs_task_definition"</span> <span class="s2">"zackblog_task"</span> <span class="o">{</span>
 family                   <span class="o">=</span> <span class="s2">"zackblog-task"</span>
 network_mode             <span class="o">=</span> <span class="s2">"awsvpc"</span>
 requires_compatibilities <span class="o">=</span> <span class="o">[</span><span class="s2">"FARGATE"</span><span class="o">]</span>
 cpu                      <span class="o">=</span> <span class="s2">"256"</span>
 memory                   <span class="o">=</span> <span class="s2">"512"</span>

 container_definitions <span class="o">=</span> jsonencode<span class="o">([</span>
 <span class="o">{</span>
 name      <span class="o">=</span> <span class="s2">"zackblog-container"</span>,
 image     <span class="o">=</span> <span class="s2">"</span><span class="k">${</span><span class="nv">aws_ecr_repository</span><span class="p">.zackblog_repo.repository_url</span><span class="k">}</span><span class="s2">:latest"</span>,
 essential <span class="o">=</span> <span class="nb">true</span>,
 portMappings <span class="o">=</span> <span class="o">[</span>
 <span class="o">{</span>
 containerPort <span class="o">=</span> 80,
 hostPort      <span class="o">=</span> 80,
 protocol      <span class="o">=</span> <span class="s2">"tcp"</span>
 <span class="o">}</span>
 <span class="o">]</span>
 <span class="o">}</span>
 <span class="o">])</span>
<span class="o">}</span>

<span class="c"># Create an ECS Cluster "cluster.tf"</span>
resource <span class="s2">"aws_ecs_cluster"</span> <span class="s2">"zackblog_cluster"</span> <span class="o">{</span>
 name <span class="o">=</span> <span class="s2">"zackblog-cluster"</span>
<span class="o">}</span>

<span class="c"># Configure Networking to Use Default VPC - save cost haha</span>
<span class="c"># use the data block to fetch existing resources</span>
data <span class="s2">"aws_vpc"</span> <span class="s2">"default"</span> <span class="o">{</span>
 default <span class="o">=</span> <span class="nb">true</span>
<span class="o">}</span>

data <span class="s2">"aws_subnet"</span> <span class="s2">"default"</span> <span class="o">{</span>
 filter <span class="o">{</span>
 name   <span class="o">=</span> <span class="s2">"vpc-id"</span>
 values <span class="o">=</span> <span class="o">[</span>data.aws_vpc.default.id]
 <span class="o">}</span>
<span class="o">}</span>

resource <span class="s2">"aws_security_group"</span> <span class="s2">"zackblog_sg"</span> <span class="o">{</span>
 name_prefix <span class="o">=</span> <span class="s2">"zackblog-sg"</span>
 vpc_id      <span class="o">=</span> data.aws_vpc.default.id

 ingress <span class="o">{</span>
 from_port   <span class="o">=</span> 80
 to_port     <span class="o">=</span> 80
 protocol    <span class="o">=</span> <span class="s2">"tcp"</span>
 cidr_blocks <span class="o">=</span> <span class="o">[</span><span class="s2">"0.0.0.0/0"</span><span class="o">]</span>
 <span class="o">}</span>

 egress <span class="o">{</span>
 from_port   <span class="o">=</span> 0
 to_port     <span class="o">=</span> 0
 protocol    <span class="o">=</span> <span class="s2">"-1"</span>
 cidr_blocks <span class="o">=</span> <span class="o">[</span><span class="s2">"0.0.0.0/0"</span><span class="o">]</span>
 <span class="o">}</span>
<span class="o">}</span>

<span class="c"># Define the ECS Service "service.tf"</span>
resource <span class="s2">"aws_ecs_service"</span> <span class="s2">"zackblog_service"</span> <span class="o">{</span>
 name            <span class="o">=</span> <span class="s2">"zackblog-service"</span>
 cluster         <span class="o">=</span> aws_ecs_cluster.zackblog_cluster.id
 task_definition <span class="o">=</span> aws_ecs_task_definition.zackblog_task.arn
 desired_count   <span class="o">=</span> 1
 launch_type     <span class="o">=</span> <span class="s2">"FARGATE"</span>

 network_configuration <span class="o">{</span>
 subnets         <span class="o">=</span> <span class="o">[</span><span class="k">for </span>subnet <span class="k">in </span>data.aws_subnet.default : subnet.id]
 security_groups <span class="o">=</span> <span class="o">[</span>aws_security_group.zackblog_sg.id]
 assign_public_ip <span class="o">=</span> <span class="nb">true</span>
 <span class="o">}</span>
<span class="o">}</span>

<span class="c"># Configure Load Balancer and attach to Fargate service "load_balancer.tf"</span>
resource <span class="s2">"aws_lb"</span> <span class="s2">"zackblog_lb"</span> <span class="o">{</span>
 name               <span class="o">=</span> <span class="s2">"zackblog-lb"</span>
 internal           <span class="o">=</span> <span class="nb">false
 </span>load_balancer_type <span class="o">=</span> <span class="s2">"application"</span>
 security_groups    <span class="o">=</span> <span class="o">[</span>aws_security_group.zackblog_sg.id]
 subnets            <span class="o">=</span> <span class="o">[</span><span class="k">for </span>subnet <span class="k">in </span>data.aws_subnet.default : subnet.id]
<span class="o">}</span>

resource <span class="s2">"aws_lb_target_group"</span> <span class="s2">"zackblog_tg"</span> <span class="o">{</span>
 name     <span class="o">=</span> <span class="s2">"zackblog-tg"</span>
 port     <span class="o">=</span> 80
 protocol <span class="o">=</span> <span class="s2">"HTTP"</span>
 vpc_id   <span class="o">=</span> data.aws_vpc.default.id
<span class="o">}</span>

resource <span class="s2">"aws_lb_listener"</span> <span class="s2">"zackblog_listener"</span> <span class="o">{</span>
 load_balancer_arn <span class="o">=</span> aws_lb.zackblog_lb.arn
 port              <span class="o">=</span> 80
 protocol          <span class="o">=</span> <span class="s2">"HTTP"</span>

 default_action <span class="o">{</span>
 <span class="nb">type</span>             <span class="o">=</span> <span class="s2">"forward"</span>
 target_group_arn <span class="o">=</span> aws_lb_target_group.zackblog_tg.arn
 <span class="o">}</span>
<span class="o">}</span>

resource <span class="s2">"aws_lb_target_group_attachment"</span> <span class="s2">"zackblog_tg_attachment"</span> <span class="o">{</span>
 target_group_arn <span class="o">=</span> aws_lb_target_group.zackblog_tg.arn
 target_id        <span class="o">=</span> aws_ecs_service.zackblog_service.id
 port             <span class="o">=</span> 80
<span class="o">}</span></code></pre></figure>

<ul>
  <li>Github Action Workflow fo CICD</li>
</ul>

<p>1.First we need to create Github Secret to contain dockerhub and aws credentials and some other vars:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">AWS_ACCESS_KEY_ID

AWS_SECRET_ACCESS_KEY

AWS_REGION

<span class="c"># xxx.dkr.ecr.ap-southeast-2.amazonaws.com</span>
ECR_REGISTRY  

<span class="c"># zackblog-repo</span>
ECR_REPOSITORY  

<span class="c"># zackblog-cluster</span>
ECS_CLUSTER 

<span class="c"># zackblog-service</span>
ECS_SERVICE </code></pre></figure>

<p>2.Then define the workflow to create /.github/workflows/zackblog-fargate.yaml, in this configure Github runner, it will :</p>

<p>Log in to Amazon ECR</p>

<p>Build and push Docker Image to the ECR repository</p>

<p>Deploy to ECS by updating the ECS service to use the new image by forcing a new deployment</p>

<figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">name</span><span class="pi">:</span> <span class="s">Deploy to AWS Fargate</span>

<span class="na">on</span><span class="pi">:</span>
 <span class="na">push</span><span class="pi">:</span>
 <span class="na">branches</span><span class="pi">:</span>
 <span class="pi">-</span> <span class="s">editing  # not main branch</span>

<span class="na">jobs</span><span class="pi">:</span>
 <span class="na">deploy</span><span class="pi">:</span>
 <span class="na">runs-on</span><span class="pi">:</span> <span class="s">ubuntu-latest</span>

 <span class="na">steps</span><span class="pi">:</span>
 <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Checkout code</span>
 <span class="na">uses</span><span class="pi">:</span> <span class="s">actions/checkout@v3</span>

 <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Set up Docker Buildx</span>
 <span class="na">uses</span><span class="pi">:</span> <span class="s">docker/setup-buildx-action@v2</span>

 <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Log in to Amazon ECR</span>
 <span class="na">env</span><span class="pi">:</span>
 <span class="na">AWS_REGION</span><span class="pi">:</span> <span class="s">${{ secrets.AWS_REGION }}</span>
 <span class="na">run</span><span class="pi">:</span> <span class="pi">|</span>
 <span class="s">aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin ${{ secrets.ECR_REGISTRY }}</span>

 <span class="s">- name: Build and push Docker image</span>
 <span class="s">env:</span>
 <span class="s">IMAGE_TAG: ${{ github.sha }}</span>
 <span class="s">ECR_REGISTRY: ${{ secrets.ECR_REGISTRY }}</span>
 <span class="s">ECR_REPOSITORY: ${{ secrets.ECR_REPOSITORY }}</span>
 <span class="s">run: |</span>
 <span class="s">docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .</span>
 <span class="s">docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG</span>

 <span class="s">- name: Deploy to ECS</span>
 <span class="s">env:</span>
 <span class="s">AWS_REGION: ${{ secrets.AWS_REGION }}</span>
 <span class="s">ECS_CLUSTER: ${{ secrets.ECS_CLUSTER }}</span>
 <span class="s">ECS_SERVICE: ${{ secrets.ECS_SERVICE }}</span>
 <span class="s">ECR_REGISTRY: ${{ secrets.ECR_REGISTRY }}</span>
 <span class="s">ECR_REPOSITORY: ${{ secrets.ECR_REPOSITORY }}</span>
 <span class="s">IMAGE_TAG: ${{ github.sha }}</span>
 <span class="s">run: |</span>
 <span class="s">aws ecs update-service --cluster $ECS_CLUSTER --service $ECS_SERVICE --force-new-deployment --region $AWS_REGION</span></code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we have a seamless incurvature as a code together with CICD pipeline to ensure that the “Zack’s Blog” can be moved to AWS serverless container service Fargate, every time I update the blog by committing changes to “zack-gitops-project” editing branch, a new Docker image will be built, pushed to ECR, and the AWS Fargate service is automatically updated.  </p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Why go serverless]]></summary></entry><entry><title type="html">Automate Package Deployment via AWS System Manager</title><link href="http://localhost:4000/jekyll/cat2/2024/06/23/aws-ssm.html" rel="alternate" type="text/html" title="Automate Package Deployment via AWS System Manager" /><published>2024-06-23T10:15:29+10:00</published><updated>2024-06-23T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/06/23/aws-ssm</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/06/23/aws-ssm.html"><![CDATA[<p><b>The task</b></p>

<p>Recently we got a task from the Company’s security team, to install 2 security agents which will be used to perform centralized security scans for all active AWS EC2 instances. here I will see how to use AWS Systems Manager for software distribution and installation for multiple AWS accounts and infrastructure at scale.</p>

<p>SSM features will be used :</p>

<ul>
  <li><b>Session Manager</b>: ensure the EC2 instance has the SSM Agent installed and running and The instances need an IAM role with at least the <code class="language-plaintext highlighter-rouge">AmazonSSMManagedInstanceCore</code> policy attached  </li>
  <li><b>Run Command</b>: send command and execute security agent software package installation scripts and command to varify post-installation status on remote instances for task automation</li>
</ul>

<p>Prerequisites:</p>
<ul>
  <li>
    <p><b>AWSCLI</b>: programatically manage all the operation bellow.</p>
  </li>
  <li>
    <p><b>SSM Agent</b>: Ensure the SSM Agent is installed and running on all EC2 instances. Most Amazon Machine Images (AMIs) have the SSM Agent pre-installed.</p>
  </li>
  <li>
    <p><b>IAM Role</b>: Attach an IAM role to each instance with the AmazonSSMManagedInstanceCore policy.</p>
  </li>
</ul>

<p>Create and attach IAM role to EC2 instance for SSM to be able to perform action:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>cloudshell-user@ip-10-134-56-72 ~]<span class="nv">$ </span>vim trust-policy.json

<span class="o">{</span>
 <span class="s2">"Version"</span>: <span class="s2">"2012-10-17"</span>,
 <span class="s2">"Statement"</span>: <span class="o">[</span>
 <span class="o">{</span>
 <span class="s2">"Effect"</span>: <span class="s2">"Allow"</span>,
 <span class="s2">"Principal"</span>: <span class="o">{</span>
 <span class="s2">"Service"</span>: <span class="s2">"ec2.amazonaws.com"</span>
 <span class="o">}</span>,
 <span class="s2">"Action"</span>: <span class="s2">"sts:AssumeRole"</span>
 <span class="o">}</span>
 <span class="o">]</span>
<span class="o">}</span>

<span class="o">[</span>cloudshell-user@ip-10-134-56-72 ~]<span class="nv">$ </span>aws iam create-role <span class="nt">--role-name</span> SSMAccessRole <span class="nt">--assume-role-policy-document</span> file://trust-policy.json
<span class="o">{</span>
 <span class="s2">"Role"</span>: <span class="o">{</span>
 <span class="s2">"Path"</span>: <span class="s2">"/"</span>,
 <span class="s2">"RoleName"</span>: <span class="s2">"SSMAccessRole"</span>,
 <span class="s2">"RoleId"</span>: <span class="s2">"AROA4MTWLTSHKK3QL3NOU"</span>,
 <span class="s2">"Arn"</span>: <span class="s2">"arn:aws:iam::85xxxxxx1342:role/SSMAccessRole"</span>,
 <span class="s2">"CreateDate"</span>: <span class="s2">"2024-06-25T00:29:07+00:00"</span>,
 <span class="s2">"AssumeRolePolicyDocument"</span>: <span class="o">{</span>
 <span class="s2">"Version"</span>: <span class="s2">"2012-10-17"</span>,
 <span class="s2">"Statement"</span>: <span class="o">[</span>
 <span class="o">{</span>
 <span class="s2">"Effect"</span>: <span class="s2">"Allow"</span>,
 <span class="s2">"Principal"</span>: <span class="o">{</span>
 <span class="s2">"Service"</span>: <span class="s2">"ec2.amazonaws.com"</span>
 <span class="o">}</span>,
 <span class="s2">"Action"</span>: <span class="s2">"sts:AssumeRole"</span>
 <span class="o">}</span>
 <span class="o">]</span>
 <span class="o">}</span>
 <span class="o">}</span>
<span class="o">}</span>

<span class="o">[</span>cloudshell-user@ip-10-134-56-72 ~]<span class="nv">$ </span>aws iam attach-role-policy <span class="nt">--role-name</span> SSMAccessRole <span class="nt">--policy-arn</span> arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore

<span class="o">[</span>cloudshell-user@ip-10-134-56-72 ~]<span class="nv">$ </span>aws iam create-instance-profile <span class="nt">--instance-profile-name</span> SSMInstanceProfile
<span class="o">{</span>
 <span class="s2">"InstanceProfile"</span>: <span class="o">{</span>
 <span class="s2">"Path"</span>: <span class="s2">"/"</span>,
 <span class="s2">"InstanceProfileName"</span>: <span class="s2">"SSMInstanceProfile"</span>,
 <span class="s2">"InstanceProfileId"</span>: <span class="s2">"AIPA4MTWLTSHF7KJRFLYC"</span>,
 <span class="s2">"Arn"</span>: <span class="s2">"arn:aws:iam::851xxxxxxx42:instance-profile/SSMInstanceProfile"</span>,
 <span class="s2">"CreateDate"</span>: <span class="s2">"2024-06-25T00:31:25+00:00"</span>,
 <span class="s2">"Roles"</span>: <span class="o">[]</span>
 <span class="o">}</span>
<span class="o">}</span>

<span class="o">[</span>cloudshell-user@ip-10-134-56-72 ~]<span class="nv">$ INSTANCE_IDS</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--query</span> <span class="s2">"Reservations[*].Instances[*].InstanceId"</span> <span class="nt">--output</span> text<span class="si">)</span>
<span class="k">for </span>INSTANCE_ID <span class="k">in</span> <span class="nv">$INSTANCE_IDS</span><span class="p">;</span> <span class="k">do</span>
<span class="o">&gt;</span> aws ec2 associate-iam-instance-profile <span class="nt">--instance-id</span> <span class="nv">$INSTANCE_ID</span> <span class="nt">--iam-instance-profile</span> <span class="nv">Name</span><span class="o">=</span>SSMInstanceProfile
<span class="o">&gt;</span> <span class="k">done</span>

<span class="o">{</span>
 <span class="s2">"IamInstanceProfileAssociation"</span>: <span class="o">{</span>
 <span class="s2">"AssociationId"</span>: <span class="s2">"iip-assoc-04e81626bf6bcd9b5"</span>,
 <span class="s2">"InstanceId"</span>: <span class="s2">"i-0762xxxxxxxcf2"</span>,
 <span class="s2">"IamInstanceProfile"</span>: <span class="o">{</span>
 <span class="s2">"Arn"</span>: <span class="s2">"arn:aws:iam::8517xxxxxxx42:instance-profile/SSMInstanceProfile"</span>,
 <span class="s2">"Id"</span>: <span class="s2">"AIPA4MTWLTSHF7KJRFLYC"</span>
 <span class="o">}</span>,
 <span class="s2">"State"</span>: <span class="s2">"associating"</span>
 <span class="o">}</span>
<span class="o">}</span>
<span class="o">{</span>
 <span class="s2">"IamInstanceProfileAssociation"</span>: <span class="o">{</span>
 <span class="s2">"AssociationId"</span>: <span class="s2">"iip-assoc-09233165b3a53ca68"</span>,
 <span class="s2">"InstanceId"</span>: <span class="s2">"i-01a2xxxxxx048c"</span>,
 <span class="s2">"IamInstanceProfile"</span>: <span class="o">{</span>
 <span class="s2">"Arn"</span>: <span class="s2">"arn:aws:iam::851xxxx42:instance-profile/SSMInstanceProfile"</span>,
 <span class="s2">"Id"</span>: <span class="s2">"AIPA4MTWLTSHF7KJRFLYC"</span>
 <span class="o">}</span>,
 <span class="s2">"State"</span>: <span class="s2">"associating"</span>
 <span class="o">}</span>
<span class="o">}</span>
<span class="o">{</span>
 <span class="s2">"IamInstanceProfileAssociation"</span>: <span class="o">{</span>
 <span class="s2">"AssociationId"</span>: <span class="s2">"iip-assoc-09236a8ee456e39cd"</span>,
 <span class="s2">"InstanceId"</span>: <span class="s2">"i-07axxxxxxb823"</span>,
 <span class="s2">"IamInstanceProfile"</span>: <span class="o">{</span>
 <span class="s2">"Arn"</span>: <span class="s2">"arn:aws:iam::85xxxxxxx42:instance-profile/SSMInstanceProfile"</span>,
 <span class="s2">"Id"</span>: <span class="s2">"AIPA4MTWLTSHF7KJRFLYC"</span>
 <span class="o">}</span>,
 <span class="s2">"State"</span>: <span class="s2">"associating"</span>
 <span class="o">}</span>
<span class="o">}</span>

<span class="o">[</span>cloudshell-user@ip-10-134-56-72 ~]<span class="nv">$ </span>aws ssm describe-instance-information</code></pre></figure>

<p>Now verify from session manager to see if instances are there:</p>

<p><img src="/assets/ssm1.png" alt="image tooltip here" /></p>

<ul>
  <li>Tag Instances</li>
</ul>

<p>Tag EC2 instances to identify which instances need the security agent to be installed:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nv">Key</span><span class="o">=</span>InstallSecurityAgent, <span class="nv">Value</span><span class="o">=</span>True.</code></pre></figure>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># List all instance IDs #: </span>
<span class="nv">INSTANCE_IDS</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--query</span> <span class="s2">"Reservations[*].Instances[*].InstanceId"</span> <span class="nt">--output</span> text<span class="si">)</span>

<span class="c"># Tag all instances with Key=InstallSecurityAgent and Value=True</span>
<span class="k">for </span>INSTANCE_ID <span class="k">in</span> <span class="nv">$INSTANCE_IDS</span><span class="p">;</span> <span class="k">do
 </span>aws ec2 create-tags <span class="nt">--resources</span> <span class="nv">$INSTANCE_ID</span> <span class="nt">--tags</span> <span class="nv">Key</span><span class="o">=</span>InstallSecurityAgent,Value<span class="o">=</span>True
<span class="k">done</span></code></pre></figure>

<ul>
  <li>SSM Run Command to Create a custom SSM document that contains the script to install the security agent</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>cloudshell-user@ip-10-134-56-72 ~]<span class="nv">$ </span>vim install_security_agent.json
<span class="o">{</span>
 <span class="s2">"schemaVersion"</span>: <span class="s2">"2.2"</span>,
 <span class="s2">"description"</span>: <span class="s2">"Install Security Agent"</span>,
 <span class="s2">"mainSteps"</span>: <span class="o">[</span>
 <span class="o">{</span>
 <span class="s2">"action"</span>: <span class="s2">"aws:runShellScript"</span>,
 <span class="s2">"name"</span>: <span class="s2">"installSecurityAgent"</span>,
 <span class="s2">"inputs"</span>: <span class="o">{</span>
 <span class="s2">"runCommand"</span>: <span class="o">[</span>
 <span class="s2">"curl -o /tmp/security-agent-installer.sh https://github.com/ZackZhouHB/zack-gitops-project/blob/editing/Python_scripts/security-agent-installer.sh"</span>,
 <span class="s2">"chmod +x /tmp/security-agent-installer.sh"</span>,
 <span class="s2">"/tmp/security-agent-installer.sh"</span>
 <span class="o">]</span>
 <span class="o">}</span>
 <span class="o">}</span>
 <span class="o">]</span>
<span class="o">}</span></code></pre></figure>

<p>Create Document to execute the installation script on all tagged instances.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>cloudshell-user@ip-10-134-56-72 ~]<span class="nv">$ </span>aws ssm create-document <span class="se">\</span>
 <span class="nt">--name</span> <span class="s2">"InstallSecurityAgent"</span> <span class="se">\</span>
 <span class="nt">--document-type</span> <span class="s2">"Command"</span> <span class="se">\</span>
 <span class="nt">--content</span> file://install_security_agent.json

<span class="o">{</span>
 <span class="s2">"DocumentDescription"</span>: <span class="o">{</span>
 <span class="s2">"Hash"</span>: <span class="s2">"9e17a699d2d987134eb05f6b49a7c837161320b0ed42635b07928acc557970b5"</span>,
 <span class="s2">"HashType"</span>: <span class="s2">"Sha256"</span>,
 <span class="s2">"Name"</span>: <span class="s2">"InstallSecurityAgent"</span>,
 <span class="s2">"Owner"</span>: <span class="s2">"851725491342"</span>,
 <span class="s2">"CreatedDate"</span>: <span class="s2">"2024-06-25T01:51:36.666000+00:00"</span>,
 <span class="s2">"Status"</span>: <span class="s2">"Creating"</span>,
 <span class="s2">"DocumentVersion"</span>: <span class="s2">"1"</span>,
 <span class="s2">"Description"</span>: <span class="s2">"Install Security Agent"</span>,
 <span class="s2">"PlatformTypes"</span>: <span class="o">[</span>
 <span class="s2">"Linux"</span>,
 <span class="s2">"MacOS"</span>
 <span class="o">]</span>,
 <span class="s2">"DocumentType"</span>: <span class="s2">"Command"</span>,
 <span class="s2">"SchemaVersion"</span>: <span class="s2">"2.2"</span>,
 <span class="s2">"LatestVersion"</span>: <span class="s2">"1"</span>,
 <span class="s2">"DefaultVersion"</span>: <span class="s2">"1"</span>,
 <span class="s2">"DocumentFormat"</span>: <span class="s2">"JSON"</span>,
 <span class="s2">"Tags"</span>: <span class="o">[]</span>
 <span class="o">}</span>
<span class="o">}</span>

<span class="o">[</span>cloudshell-user@ip-10-132-90-150 ~]<span class="nv">$ </span>aws ssm send-command <span class="se">\</span>
<span class="o">&gt;</span> <span class="nt">--document-name</span> <span class="s2">"InstallSecurityAgent"</span> <span class="se">\</span>
<span class="o">&gt;</span> <span class="nt">--targets</span> <span class="s2">"Key=tag:InstallSecurityAgent,Values=True"</span> <span class="se">\</span>
<span class="o">&gt;</span> <span class="nt">--comment</span> <span class="s2">"Installing security agent on all instances with the specified tag"</span> <span class="se">\</span>
<span class="o">&gt;</span> <span class="nt">--max-concurrency</span> <span class="s2">"50"</span> <span class="se">\</span>
<span class="o">&gt;</span> <span class="nt">--max-errors</span> <span class="s2">"0"</span> <span class="se">\</span>
<span class="o">&gt;</span> <span class="nt">--region</span> ap-southeast-2
<span class="o">{</span>
 <span class="s2">"Command"</span>: <span class="o">{</span>
 <span class="s2">"CommandId"</span>: <span class="s2">"edfc9e9b-5e74-4660-8335-a98eb48251f7"</span>,
 <span class="s2">"DocumentName"</span>: <span class="s2">"InstallSecurityAgent"</span>,
 <span class="s2">"DocumentVersion"</span>: <span class="s2">"</span><span class="nv">$DEFAULT</span><span class="s2">"</span>,
 <span class="s2">"Comment"</span>: <span class="s2">"Installing security agent on all instances with the specified tag"</span>,
 <span class="s2">"ExpiresAfter"</span>: <span class="s2">"2024-06-25T04:03:44.665000+00:00"</span>,
 <span class="s2">"Parameters"</span>: <span class="o">{}</span>,
 <span class="s2">"InstanceIds"</span>: <span class="o">[]</span>,
 <span class="s2">"Targets"</span>: <span class="o">[</span>
 <span class="o">{</span>
 <span class="s2">"Key"</span>: <span class="s2">"tag:InstallSecurityAgent"</span>,
 <span class="s2">"Values"</span>: <span class="o">[</span>
 <span class="s2">"True"</span>
 <span class="o">]</span>
 <span class="o">}</span>
 <span class="o">]</span>,
 <span class="s2">"RequestedDateTime"</span>: <span class="s2">"2024-06-25T02:03:44.665000+00:00"</span>,
 <span class="s2">"Status"</span>: <span class="s2">"Pending"</span>,
 <span class="s2">"StatusDetails"</span>: <span class="s2">"Pending"</span>,
 <span class="s2">"OutputS3Region"</span>: <span class="s2">"ap-southeast-2"</span>,
 <span class="s2">"OutputS3BucketName"</span>: <span class="s2">""</span>,
 <span class="s2">"OutputS3KeyPrefix"</span>: <span class="s2">""</span>,
 <span class="s2">"MaxConcurrency"</span>: <span class="s2">"50"</span>,
 <span class="s2">"MaxErrors"</span>: <span class="s2">"0"</span>,
 <span class="s2">"TargetCount"</span>: 0,
 <span class="s2">"CompletedCount"</span>: 0,
 <span class="s2">"ErrorCount"</span>: 0,
 <span class="s2">"DeliveryTimedOutCount"</span>: 0,
 <span class="s2">"ServiceRole"</span>: <span class="s2">""</span>,
 <span class="s2">"NotificationConfig"</span>: <span class="o">{</span>
 <span class="s2">"NotificationArn"</span>: <span class="s2">""</span>,
 <span class="s2">"NotificationEvents"</span>: <span class="o">[]</span>,
 <span class="s2">"NotificationType"</span>: <span class="s2">""</span>
 <span class="o">}</span>,
 <span class="s2">"CloudWatchOutputConfig"</span>: <span class="o">{</span>
 <span class="s2">"CloudWatchLogGroupName"</span>: <span class="s2">""</span>,
 <span class="s2">"CloudWatchOutputEnabled"</span>: <span class="nb">false</span>
 <span class="o">}</span>,
 <span class="s2">"TimeoutSeconds"</span>: 3600,
 <span class="s2">"AlarmConfiguration"</span>: <span class="o">{</span>
 <span class="s2">"IgnorePollAlarmFailure"</span>: <span class="nb">false</span>,
 <span class="s2">"Alarms"</span>: <span class="o">[]</span>
 <span class="o">}</span>,
 <span class="s2">"TriggeredAlarms"</span>: <span class="o">[]</span>
 <span class="o">}</span>
<span class="o">}</span>
<span class="o">(</span>END<span class="o">)</span></code></pre></figure>

<p>Validate from Run Command console for the installation:
<img src="/assets/ssm2.png" alt="image tooltip here" /></p>

<p>Create an SSM Document to Check the package installation status:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>cloudshell-user@ip-10-134-56-72 ~]<span class="nv">$ </span>vim verify.json

<span class="o">{</span>
 <span class="s2">"schemaVersion"</span>: <span class="s2">"2.2"</span>,
 <span class="s2">"description"</span>: <span class="s2">"Install Security Agent"</span>,
 <span class="s2">"mainSteps"</span>: <span class="o">[</span>
 <span class="o">{</span>
 <span class="s2">"action"</span>: <span class="s2">"aws:runShellScript"</span>,
 <span class="s2">"name"</span>: <span class="s2">"installSecurityAgent"</span>,
 <span class="s2">"inputs"</span>: <span class="o">{</span>
 <span class="s2">"runCommand"</span>: <span class="o">[</span>
 <span class="s2">"apt list --installed | grep nfs-common"</span>,
 <span class="s2">"apt list --installed | grep lrzsz"</span>
 <span class="o">]</span>
 <span class="o">}</span>
 <span class="o">}</span>
 <span class="o">]</span>
<span class="o">}</span></code></pre></figure>

<p>Create this document using the AWS CLI:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>cloudshell-user@ip-10-134-56-72 ~]<span class="nv">$ </span>aws ssm create-document <span class="se">\</span>
 <span class="nt">--name</span> <span class="s2">"VerifyPackageInstallation"</span> <span class="se">\</span>
 <span class="nt">--document-type</span> <span class="s2">"Command"</span> <span class="se">\</span>
 <span class="nt">--content</span> file://verify.json

<span class="o">[</span>cloudshell-user@ip-10-134-56-72 ~]<span class="nv">$ </span>aws ssm send-command <span class="se">\</span>
 <span class="nt">--document-name</span> <span class="s2">"VerifyPackageInstallation"</span> <span class="se">\</span>
 <span class="nt">--targets</span> <span class="s2">"Key=tag:InstallSecurityAgent,Values=True"</span> <span class="se">\</span>
 <span class="nt">--comment</span> <span class="s2">"Check if Packages installed on all instances"</span> <span class="se">\</span>
 <span class="nt">--max-concurrency</span> <span class="s2">"50"</span> <span class="se">\</span>
 <span class="nt">--max-errors</span> <span class="s2">"0"</span> <span class="se">\</span>
 <span class="nt">--region</span> ap-southeast-2</code></pre></figure>

<p>Verify both “nfs-common” and “lrzsz”, we have 3 machines with “nfs-common” installed, and 2 instances with Ubuntu24.04 which did not get “lrzsz” installed.</p>

<p><img src="/assets/ssm3.png" alt="image tooltip here" /></p>

<p><img src="/assets/ssm4.png" alt="image tooltip here" /></p>

<p><b> Conclusion</b></p>

<p>So now we can use AWS CLI and AWS System Manager to automate software deployment and verify the installation status.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[The task]]></summary></entry><entry><title type="html">Handling a RDS MySQL cluster CPU 100%</title><link href="http://localhost:4000/jekyll/cat2/2024/06/17/mysql-cup100.html" rel="alternate" type="text/html" title="Handling a RDS MySQL cluster CPU 100%" /><published>2024-06-17T10:15:29+10:00</published><updated>2024-06-17T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/06/17/mysql-cup100</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/06/17/mysql-cup100.html"><![CDATA[<p><b>The Incident</b></p>

<p>Today I got a performance issue from our analytic team, saying they experienced a Production MySQL cluster running on RDS very slow since yesterday morning.  </p>

<p>I started to look into bellow areas for investigation:</p>

<ul>
  <li>AWS CloudWatch Metrics for RDS</li>
</ul>

<p>AWS CloudWatch provides a wide range of metrics that can help diagnose resource usage for databases. So I started with</p>

<p><em>CloudWatch - Metrics - All metrics - Add query - RDS - Top 10 RDS instances by highest CPU utilization</em></p>

<p>This only queries the recent 3 hours metrics, but it is enough for me to identify the issue: CPU 100%</p>

<p><img src="/assets/mysqlcpu1.png" alt="image tooltip here" /></p>

<p>To further understand the high CUP, I go:</p>

<p><em>CloudWatch - Metrics - All metrics - Browse - RDS - DBClusterIdentifier - CPUUtilization</em></p>

<p>which gives me a long period of monitoring, so I can see it started to 100% CPU since yesterday morning.</p>

<p><img src="/assets/mysqlcpu2.png" alt="image tooltip here" /></p>

<ul>
  <li>AWS console RDS Logs &amp; events</li>
</ul>

<p>Now let’s find out from the RDS logs to see if any errors can indicate who could be the person. So I go</p>

<p><em>RDS - “the DB cluster” - “the DB instance” - “Logs &amp; events” - “error/mysql-error-running.log.2024-06-18.02”</em></p>

<p>I got :</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">2024-06-18T00:04:58.750212Z 2831474 <span class="o">[</span>Note] Aborted connection 2831474 to db: <span class="s1">'xxxxxx'</span> user: <span class="s1">'xxxx'</span> host: <span class="s1">'10.xx.xx.xx'</span> <span class="o">(</span>Unknown error<span class="o">)</span>
2024-06-18T00:12:00.798173Z 2831498 <span class="o">[</span>Note] Aborted connection 2831498 to db: <span class="s1">'xxxx'</span> user: <span class="s1">'xxxx'</span> host: <span class="s1">'10.xx.xx.xx'</span> <span class="o">(</span>Got an error writing communication packets<span class="o">)</span>
<span class="nt">-----------------------</span> END OF LOG <span class="nt">----------------------</span></code></pre></figure>

<p>Up to here I generally have an idea of what is going on and can locate the person “xxxx” who was running something at the time CPU 100%.</p>

<ul>
  <li>MySQL Client Tool to list, identiry and terminate long-running queries</li>
</ul>

<p>It is time to log in to the RDS endpoint to see what is happening and which queries might cause the CPU usage. Here we need login via MySQL “root” to be able to see all other users’ running processes. Then pay attention to the high “Time” and “State” values indicating all the stuck processes, then we kill them and restart the RDS instance</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">mysql <span class="nt">-u</span> root <span class="nt">-p</span> <span class="nt">-h</span> rds_endpoint
SHOW PROCESSLIST<span class="p">;</span>
KILL &lt;process_id&gt;<span class="p">;</span></code></pre></figure>

<p><img src="/assets/mysqlcpu4.png" alt="image tooltip here" /></p>

<p>Then I go <em>AWS RDS console - Actions - Reboot</em> the RDS instance.</p>

<ul>
  <li>Now the CPU usage started to drop and back to normal after terminating the stuck processes and DB instance reboot.</li>
</ul>

<p><img src="/assets/mysqlcpu3.png" alt="image tooltip here" /></p>

<p>Done.</p>

<p><b> Conclusion</b></p>

<p>Even the issue had been fixed, I was still thinking how to better monitor RDS resource usage. I think we need:</p>

<ul>
  <li>
    <p>A “CloudWatch Alarm” to set “CPUUtilization” metric threshold to 80%, then specify the period (e.g., 5 minutes) and the number of periods (e.g., 2 out of 3) that the metric must breach the threshold to trigger the alarm.</p>
  </li>
  <li>
    <p>Create “SNS topic” with team Email for the alarm to send a notification</p>
  </li>
  <li>
    <p>Enable “RDS Performance Insights”, this can monitor the load on the database, identify the source of bottlenecks, and understand how the DB is performing, especially during troubleshooting.</p>
  </li>
  <li>
    <p>Enable “Enhanced Monitoring” and select the monitoring interval (e.g., 1 minute), which provides real-time metrics for the operating system that the DB instance runs on, this helps for immediate investigation on OS level</p>
  </li>
  <li>
    <p>Enable “Slow Query Log” for regularly analysing slow query logs and performance insights to optimize RDS database queries, identify queries that take a long time to execute, use tools like EXPLAIN to understand query performance, add appropriate indexes, and then ultimately rewrite queries for better performance.</p>
  </li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[The Incident]]></summary></entry><entry><title type="html">Python Microservice: K8S deployment</title><link href="http://localhost:4000/jekyll/cat2/2024/05/22/py-flask5.html" rel="alternate" type="text/html" title="Python Microservice: K8S deployment" /><published>2024-05-22T00:17:29+10:00</published><updated>2024-05-22T00:17:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/22/py-flask5</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/22/py-flask5.html"><![CDATA[<p><b> About K8S deployment</b></p>

<p>Now It is time to change from docker-compose to deploy into Kubernetes.  </p>

<p>As this is not new to me to deploy microservice into K8S, also I already have a running Kubernetes cluster in hand, so here I will just create docker images for the 3 services: API gateway, user and order, then push them into the docker hub repository, then create Kubernetes manifest for deployment and service.  </p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Folder structure</span>
/07-with-k8s
<span class="nb">.</span>
├── api_gateway.py
├── depolyment.yaml
├── Dockerfile_apigateway
├── Dockerfile_order
├── Dockerfile_user
├── order_service.py
└── user_service.py

<span class="c"># Build, tag and push the docker images</span>
docker login

docker build <span class="nt">-t</span> zackz001/python-user:latest <span class="nt">-f</span> Dockerfile_user <span class="nb">.</span>
docker build <span class="nt">-t</span> zackz001/python-order:latest <span class="nt">-f</span> Dockerfile_order <span class="nb">.</span>
docker build <span class="nt">-t</span> zackz001/python-apigateway:latest <span class="nt">-f</span> Dockerfile_apigateway <span class="nb">.</span>
docker push zackz001/python-apigateway:latest
docker push zackz001/python-user:latest
docker push zackz001/python-order:latest

docker image <span class="nb">ls

</span>REPOSITORY                                      TAG       IMAGE ID       CREATED        SIZE
zackz001/python-apigateway                      latest    bc3db11f4be8   1 hours ago    138MB
zackz001/python-user                            latest    c93973bece33   1 hours ago    136MB
zackz001/python-order                           latest    e35d5de9254b   1 hours ago    136MB
prom/prometheus                                 latest    1bd2b9635267   8 days ago     271MB
grafana/grafana                                 latest    c42c21cd0ebc   3 weeks ago    453MB
consul                                          1.15.4    686495461132   4 months ago   155MB
docker.elastic.co/elasticsearch/elasticsearch   7.13.2    11a830014f7c   3 years ago    1.02GB
docker.elastic.co/logstash/logstash             7.13.2    8dc1af4dd662   3 years ago    965MB
docker.elastic.co/kibana/kibana                 7.13.2    6c4869a27be1   3 years ago    1.35GB

<span class="c"># k8s deployment Manifests</span>

apiVersion: apps/v1
kind: Deployment
metadata:
 name: user-service
spec:
 replicas: 1
 selector:
 matchLabels:
 app: user-service
 template:
 metadata:
 labels:
 app: user-service
 spec:
 containers:
 - name: user-service
 image: zackz001/python-user:latest
 ports:
 - containerPort: 5001

<span class="nt">---</span>
apiVersion: v1
kind: Service
metadata:
 name: user-service
spec:
 selector:
 app: user-service
 ports:
 - protocol: TCP
 port: 5001
 targetPort: 5001

<span class="nt">---</span>
apiVersion: apps/v1
kind: Deployment
metadata:
 name: order-service
spec:
 replicas: 1
 selector:
 matchLabels:
 app: order-service
 template:
 metadata:
 labels:
 app: order-service
 spec:
 containers:
 - name: order-service
 image: zackz001/python-order:latest
 ports:
 - containerPort: 5002

<span class="nt">---</span>
apiVersion: v1
kind: Service
metadata:
 name: order-service
spec:
 selector:
 app: order-service
 ports:
 - protocol: TCP
 port: 5002
 targetPort: 5002

<span class="nt">---</span>
apiVersion: apps/v1
kind: Deployment
metadata:
 name: api-gateway
spec:
 replicas: 1
 selector:
 matchLabels:
 app: api-gateway
 template:
 metadata:
 labels:
 app: api-gateway
 spec:
 containers:
 - name: api-gateway
 image: zackz001/python-apigateway:latest
 ports:
 - containerPort: 5000

<span class="nt">---</span>
apiVersion: v1
kind: Service
metadata:
 name: api-gateway
spec:
 <span class="nb">type</span>: NodePort
 selector:
 app: api-gateway
 ports:
 - protocol: TCP
 port: 5000
 targetPort: 5000</code></pre></figure>

<p>Now Run kubectl apply -f to bring all deployments and services up and running. Should see all services in the Rancher console.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl create ns python

kubectl apply <span class="nt">-f</span> depolyment.yaml <span class="nt">-n</span> python

kubectl get all <span class="nt">-n</span> python

NAME                                 READY   STATUS    RESTARTS      AGE
pod/api-gateway-d664cf8c4-7l7q8      1/1     Running   2 <span class="o">(</span>50m ago<span class="o">)</span>   1h
pod/order-service-856577f666-gk5gt   1/1     Running   2 <span class="o">(</span>50m ago<span class="o">)</span>   1h
pod/user-service-5d8766d9cb-4rqnz    1/1     Running   2 <span class="o">(</span>50m ago<span class="o">)</span>   1h

NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>          AGE
service/api-gateway     NodePort    10.43.38.27     &lt;none&gt; 5000:32060/TCP   1h
service/order-service   ClusterIP   10.43.27.255    &lt;none&gt; 5002/TCP         1h
service/user-service    ClusterIP   10.43.160.232   &lt;none&gt; 5001/TCP         1h

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/api-gateway     1/1     1            1           1h
deployment.apps/order-service   1/1     1            1           1h
deployment.apps/user-service    1/1     1            1           1h

NAME                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/api-gateway-d664cf8c4      1         1         1       1h
replicaset.apps/order-service-856577f666   1         1         1       1h
replicaset.apps/user-service-5d8766d9cb    1         1         1       1h</code></pre></figure>

<p><b> Verify API gateway, user and order services</b></p>

<p>Access API gateway via http://NodeIP:NodePort/users and http://NodeIP:NodePort/orders</p>

<p><img src="/assets/flask11.png" alt="image tooltip here" /></p>

<p><img src="/assets/flask12.png" alt="image tooltip here" /></p>

<p><b> Conclusion</b></p>

<p>Now we complete all Python Flask sessions.</p>

<p>I have done this End-to-End Python Microservice application solution development, which enhanced my DevOps practices of Python programming, microservices architecture design and deployment with docker-compose, API Gateway implementation, service registery with Consul, logging and monitoring, and finally Kubernetes deployment.  </p>

<ul>
  <li>
    <p>simple Python Flask app</p>
  </li>
  <li>
    <p>Microservice applications with user and order</p>
  </li>
  <li>
    <p>Create API gateway Flask app</p>
  </li>
  <li>
    <p>Service registery with Consul</p>
  </li>
  <li>
    <p>Logging with ELK</p>
  </li>
  <li>
    <p>Monitoring with Prometheus and Grafana</p>
  </li>
  <li>
    <p>K8S deployment</p>
  </li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About K8S deployment]]></summary></entry><entry><title type="html">Python Microservice: Monitoring with Prometheus and Grafana</title><link href="http://localhost:4000/jekyll/cat2/2024/05/21/py-flask4.html" rel="alternate" type="text/html" title="Python Microservice: Monitoring with Prometheus and Grafana" /><published>2024-05-21T00:17:29+10:00</published><updated>2024-05-21T00:17:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/21/py-flask4</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/21/py-flask4.html"><![CDATA[<p><b> About Monitoring with Prometheus and Grafana</b></p>

<p>Both Prometheus and Grafana are compatible with microservice applications, integrating Prometheus with Flask is straightforward to provide performance and monitoring metrics.</p>

<p>Here I will update the docker-compose to add Prometheus and Grafana as services, then add Prometheus metrics in both order and user application code by importing <code class="language-plaintext highlighter-rouge">PrometheusMetrics</code> from the <code class="language-plaintext highlighter-rouge">prometheus_flask_exporter</code> module, which is used to expose Prometheus metrics for the Flask application. then initialize Prometheus metrics with <code class="language-plaintext highlighter-rouge">metrics = PrometheusMetrics(app)</code>. Use <code class="language-plaintext highlighter-rouge">@metrics.counter('get_orders_count', 'Count of calls to the get_orders endpoint')</code> to define a Prometheus counter metric that increments each time the get_orders endpoint is called.</p>

<p>Import <code class="language-plaintext highlighter-rouge">os</code> to enable environment variable for Logstash host, <code class="language-plaintext highlighter-rouge">logstash_host = os.getenv('LOGSTASH_HOST', 'localhost')</code> to fetche the Logstash host from environment variables, defaulting to localhost.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Folder structure</span>
/06-with-monitoringstack
├── api_gateway
│   ├── api_gateway.py
│   └── Dockerfile
├── docker-compose.yml
├── logstash.conf
├── order_service
│   ├── Dockerfile
│   ├── order_service.py
├── prometheus.yaml
└── user_service
    ├── Dockerfile
    └── user_service.py

<span class="c"># add Prometheus and Grafana in docker-compose.yaml</span>
version: <span class="s1">'3'</span>
services:
  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - <span class="s2">"9090:9090"</span>
  grafana:
    image: grafana/grafana
    ports:
      - <span class="s2">"3000:3000"</span>

<span class="c"># create prometheus.yml for Prometheus configration</span>
<span class="c"># vim prometheus.yml</span>
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: <span class="s1">'flask'</span>
    static_configs:
      - targets: <span class="o">[</span><span class="s1">'user-service:5001'</span>, <span class="s1">'order-service:5002'</span><span class="o">]</span>

<span class="c"># add Monitoring logic into python code </span>

<span class="c"># order_service.py</span>
import logging
import requests
from flask import Flask, jsonify
from pygelf import GelfUdpHandler
from prometheus_flask_exporter import PrometheusMetrics
import os

app <span class="o">=</span> Flask<span class="o">(</span>__name__<span class="o">)</span>
metrics <span class="o">=</span> PrometheusMetrics<span class="o">(</span>app<span class="o">)</span>

@app.route<span class="o">(</span><span class="s1">'/orders'</span><span class="o">)</span>
def get_orders<span class="o">()</span>:
    orders <span class="o">=</span> <span class="o">[</span>
        <span class="o">{</span><span class="s1">'id'</span>: 1, <span class="s1">'item'</span>: <span class="s1">'Laptop'</span>, <span class="s1">'price'</span>: 1200<span class="o">}</span>,
        <span class="o">{</span><span class="s1">'id'</span>: 2, <span class="s1">'item'</span>: <span class="s1">'Phone'</span>, <span class="s1">'price'</span>: 800<span class="o">}</span>
    <span class="o">]</span>
    app.logger.info<span class="o">(</span><span class="s2">"Fetched order data"</span><span class="o">)</span>
    <span class="k">return </span>jsonify<span class="o">(</span>orders<span class="o">)</span>

def register_service<span class="o">()</span>:
    payload <span class="o">=</span> <span class="o">{</span>
        <span class="s2">"ID"</span>: <span class="s2">"order-service"</span>,
        <span class="s2">"Name"</span>: <span class="s2">"order-service"</span>,
        <span class="s2">"Address"</span>: <span class="s2">"order-service"</span>,
        <span class="s2">"Port"</span>: 5002
    <span class="o">}</span>
    response <span class="o">=</span> requests.put<span class="o">(</span><span class="s1">'http://consul:8500/v1/agent/service/register'</span>, <span class="nv">json</span><span class="o">=</span>payload<span class="o">)</span>
    <span class="k">if </span>response.status_code <span class="o">==</span> 200:
        app.logger.info<span class="o">(</span><span class="s2">"Order service registered successfully"</span><span class="o">)</span>
    <span class="k">else</span>:
        app.logger.error<span class="o">(</span><span class="s2">"Failed to register order service"</span><span class="o">)</span>

<span class="k">if </span>__name__ <span class="o">==</span> <span class="s1">'__main__'</span>:
    <span class="c"># Configure logging</span>
    logstash_host <span class="o">=</span> os.getenv<span class="o">(</span><span class="s1">'LOGSTASH_HOST'</span>, <span class="s1">'localhost'</span><span class="o">)</span>
    handler <span class="o">=</span> GelfUdpHandler<span class="o">(</span><span class="nv">host</span><span class="o">=</span>logstash_host, <span class="nv">port</span><span class="o">=</span>12201<span class="o">)</span>
    app.logger.addHandler<span class="o">(</span>handler<span class="o">)</span>
    app.logger.setLevel<span class="o">(</span>logging.INFO<span class="o">)</span>
    
    register_service<span class="o">()</span>
    app.run<span class="o">(</span><span class="nv">host</span><span class="o">=</span><span class="s1">'0.0.0.0'</span>, <span class="nv">port</span><span class="o">=</span>5002<span class="o">)</span>


<span class="c"># user_service.py</span>
import logging
import requests
from flask import Flask, jsonify
from pygelf import GelfUdpHandler
from prometheus_flask_exporter import PrometheusMetrics
import os

app <span class="o">=</span> Flask<span class="o">(</span>__name__<span class="o">)</span>
metrics <span class="o">=</span> PrometheusMetrics<span class="o">(</span>app<span class="o">)</span>

@app.route<span class="o">(</span><span class="s1">'/users'</span><span class="o">)</span>
def get_users<span class="o">()</span>:
    <span class="nb">users</span> <span class="o">=</span> <span class="o">[</span>
        <span class="o">{</span><span class="s1">'id'</span>: 1, <span class="s1">'name'</span>: <span class="s1">'Alice'</span><span class="o">}</span>,
        <span class="o">{</span><span class="s1">'id'</span>: 2, <span class="s1">'name'</span>: <span class="s1">'Bob'</span><span class="o">}</span>
    <span class="o">]</span>
    app.logger.info<span class="o">(</span><span class="s2">"Fetched user data"</span><span class="o">)</span>
    <span class="k">return </span>jsonify<span class="o">(</span><span class="nb">users</span><span class="o">)</span>

def register_service<span class="o">()</span>:
    payload <span class="o">=</span> <span class="o">{</span>
        <span class="s2">"ID"</span>: <span class="s2">"user-service"</span>,
        <span class="s2">"Name"</span>: <span class="s2">"user-service"</span>,
        <span class="s2">"Address"</span>: <span class="s2">"user-service"</span>,
        <span class="s2">"Port"</span>: 5001
    <span class="o">}</span>
    response <span class="o">=</span> requests.put<span class="o">(</span><span class="s1">'http://consul:8500/v1/agent/service/register'</span>, <span class="nv">json</span><span class="o">=</span>payload<span class="o">)</span>
    <span class="k">if </span>response.status_code <span class="o">==</span> 200:
        app.logger.info<span class="o">(</span><span class="s2">"User service registered successfully"</span><span class="o">)</span>
    <span class="k">else</span>:
        app.logger.error<span class="o">(</span><span class="s2">"Failed to register user service"</span><span class="o">)</span>

<span class="k">if </span>__name__ <span class="o">==</span> <span class="s1">'__main__'</span>:
    <span class="c"># Configure logging</span>
    logstash_host <span class="o">=</span> os.getenv<span class="o">(</span><span class="s1">'LOGSTASH_HOST'</span>, <span class="s1">'localhost'</span><span class="o">)</span>
    handler <span class="o">=</span> GelfUdpHandler<span class="o">(</span><span class="nv">host</span><span class="o">=</span>logstash_host, <span class="nv">port</span><span class="o">=</span>12201<span class="o">)</span>
    app.logger.addHandler<span class="o">(</span>handler<span class="o">)</span>
    app.logger.setLevel<span class="o">(</span>logging.INFO<span class="o">)</span>
    
    register_service<span class="o">()</span>
    app.run<span class="o">(</span><span class="nv">host</span><span class="o">=</span><span class="s1">'0.0.0.0'</span>, <span class="nv">port</span><span class="o">=</span>5001<span class="o">)</span></code></pre></figure>

<p>Now Run docker-compose to bring all containers up and running. Should see all services with Prometheus and Grafana.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">docker-compose up <span class="nt">--build</span>

Creating 06-with-monitoringstack_api-gateway_1   ... <span class="k">done
</span>Creating 06-with-monitoringstack_order-service_1 ... <span class="k">done
</span>Creating 06-with-monitoringstack_user-service_1  ... <span class="k">done
</span>Creating 06-with-monitoringstack_kibana_1        ... <span class="k">done
</span>Creating 06-with-monitoringstack_grafana_1       ... <span class="k">done
</span>Creating 06-with-monitoringstack_logstash_1      ... <span class="k">done
</span>Creating 06-with-monitoringstack_prometheus_1    ... <span class="k">done
</span>Creating 06-with-monitoringstack_elasticsearch_1 ... <span class="k">done
</span>Creating 06-with-monitoringstack_consul_1        ... <span class="k">done</span></code></pre></figure>

<p><b> Verify Prometheus and Grafana</b></p>

<p>Access Grafana via http://localhost:3000, log in with admin/admin.</p>

<p>Add Prometheus as a Data Source in Grafana by setting the URL to http://localhost:9090 and save.</p>

<p>Create Dashboards in Grafana to use the Prometheus data source to visualize metrics from user-service and order-service.</p>

<p><img src="/assets/flask10.png" alt="image tooltip here" /></p>

<p><img src="/assets/flask9.png" alt="image tooltip here" /></p>

<p><b> Conclusion</b></p>

<p>Now we can enable monitoring with Prometheus and Grafana.</p>

<p>In the next post, I will see how to depoly our Python Flask microservice into Kubernetes.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Monitoring with Prometheus and Grafana]]></summary></entry><entry><title type="html">Python Microservice: Logging with ELK</title><link href="http://localhost:4000/jekyll/cat2/2024/05/20/py-flask3.html" rel="alternate" type="text/html" title="Python Microservice: Logging with ELK" /><published>2024-05-20T00:16:29+10:00</published><updated>2024-05-20T00:16:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/05/20/py-flask3</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/05/20/py-flask3.html"><![CDATA[<p><b> About logging with ELK (Elasticsearch, Logstash, Kibana) Stack</b></p>

<p>ELK (Elasticsearch, Logstash, Kibana) is a popular log management solution. We will use the ELK Stack to collect and analyze logs.</p>

<p>Here I need to extend the current configuration by adding services in docker-compose file for <code class="language-plaintext highlighter-rouge">Elasticsearch</code>, <code class="language-plaintext highlighter-rouge">Logstash</code>, and <code class="language-plaintext highlighter-rouge">Kibana</code>, and configure the microservices to send logs to Logstash.</p>

<p>Also, I will need to configure the logging in both <code class="language-plaintext highlighter-rouge">user</code> and <code class="language-plaintext highlighter-rouge">order</code> Python application code to send logs to Logstash. By importing the built-in <code class="language-plaintext highlighter-rouge">logging</code> module and  <code class="language-plaintext highlighter-rouge">GelfUdpHandler</code> from the <code class="language-plaintext highlighter-rouge">pygelf</code> module, to provide a flexible framework for emitting log messages from Python programs to send log messages in the <code class="language-plaintext highlighter-rouge">GELF</code> (Graylog Extended Log Format) to a remote Graylog server, which is typically part of the ELK stack.</p>

<p>By adding log messages using <code class="language-plaintext highlighter-rouge">app.logger.info</code> and <code class="language-plaintext highlighter-rouge">app.logger.error</code>, together with the defined the logging level, I can set the logging level to <code class="language-plaintext highlighter-rouge">INFO</code>, which means all log messages at this level or higher will be emitted.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># folder structure </span>
05-with-ELK/
├── api_gateway/
│   └── Dockerfile
├── order_service/
│   └── Dockerfile
├── user_service/
│   └── Dockerfile
├── logstash.conf
└── docker-compose.yml

<span class="c"># create Logstash Configuration</span>
<span class="c"># vim logstash.conf</span>
input <span class="o">{</span>
 gelf <span class="o">{</span>
 port <span class="o">=&gt;</span> 12201
 <span class="o">}</span>
<span class="o">}</span>
output <span class="o">{</span>
 elasticsearch <span class="o">{</span>
 hosts <span class="o">=&gt;</span> <span class="o">[</span><span class="s2">"elasticsearch:9200"</span><span class="o">]</span>
 index <span class="o">=&gt;</span> <span class="s2">"%{[@metadata][beat]}-%{+YYYY.MM.dd}"</span>
 <span class="o">}</span>
<span class="o">}</span>
<span class="c"># vim user_service.py</span>

import logging
import requests
from flask import Flask, jsonify
from pygelf import GelfUdpHandler

app <span class="o">=</span> Flask<span class="o">(</span>__name__<span class="o">)</span>

@app.route<span class="o">(</span><span class="s1">'/users'</span><span class="o">)</span>
def get_users<span class="o">()</span>:
 <span class="nb">users</span> <span class="o">=</span> <span class="o">[</span>
 <span class="o">{</span><span class="s1">'id'</span>: 1, <span class="s1">'name'</span>: <span class="s1">'Alice'</span><span class="o">}</span>,
 <span class="o">{</span><span class="s1">'id'</span>: 2, <span class="s1">'name'</span>: <span class="s1">'Bob'</span><span class="o">}</span>
 <span class="o">]</span>
 app.logger.info<span class="o">(</span><span class="s2">"Fetched user data"</span><span class="o">)</span>
 <span class="k">return </span>jsonify<span class="o">(</span><span class="nb">users</span><span class="o">)</span>

def register_service<span class="o">()</span>:
 payload <span class="o">=</span> <span class="o">{</span>
 <span class="s2">"ID"</span>: <span class="s2">"user-service"</span>,
 <span class="s2">"Name"</span>: <span class="s2">"user-service"</span>,
 <span class="s2">"Address"</span>: <span class="s2">"user-service"</span>,
 <span class="s2">"Port"</span>: 5001
 <span class="o">}</span>
 response <span class="o">=</span> requests.put<span class="o">(</span><span class="s1">'http://consul:8500/v1/agent/service/register'</span>, <span class="nv">json</span><span class="o">=</span>payload<span class="o">)</span>
 <span class="k">if </span>response.status_code <span class="o">==</span> 200:
 app.logger.info<span class="o">(</span><span class="s2">"User service registered successfully"</span><span class="o">)</span>
 <span class="k">else</span>:
 app.logger.error<span class="o">(</span><span class="s2">"Failed to register user service"</span><span class="o">)</span>

<span class="k">if </span>__name__ <span class="o">==</span> <span class="s1">'__main__'</span>:
 <span class="c"># Configure logging</span>
 handler <span class="o">=</span> GelfUdpHandler<span class="o">(</span><span class="nv">host</span><span class="o">=</span><span class="s1">'logstash'</span>, <span class="nv">port</span><span class="o">=</span>12201<span class="o">)</span>
 app.logger.addHandler<span class="o">(</span>handler<span class="o">)</span>
 app.logger.setLevel<span class="o">(</span>logging.INFO<span class="o">)</span>
    
 register_service<span class="o">()</span>
 app.run<span class="o">(</span><span class="nv">host</span><span class="o">=</span><span class="s1">'0.0.0.0'</span>, <span class="nv">port</span><span class="o">=</span>5001<span class="o">)</span>

<span class="c"># vim order_service.py</span>

import logging
import requests
from flask import Flask, jsonify
from pygelf import GelfUdpHandler

app <span class="o">=</span> Flask<span class="o">(</span>__name__<span class="o">)</span>

@app.route<span class="o">(</span><span class="s1">'/orders'</span><span class="o">)</span>
def get_orders<span class="o">()</span>:
 orders <span class="o">=</span> <span class="o">[</span>
 <span class="o">{</span><span class="s1">'id'</span>: 1, <span class="s1">'item'</span>: <span class="s1">'Laptop'</span>, <span class="s1">'price'</span>: 1200<span class="o">}</span>,
 <span class="o">{</span><span class="s1">'id'</span>: 2, <span class="s1">'item'</span>: <span class="s1">'Phone'</span>, <span class="s1">'price'</span>: 800<span class="o">}</span>
 <span class="o">]</span>
 app.logger.info<span class="o">(</span><span class="s2">"Fetched order data"</span><span class="o">)</span>
 <span class="k">return </span>jsonify<span class="o">(</span>orders<span class="o">)</span>

def register_service<span class="o">()</span>:
 payload <span class="o">=</span> <span class="o">{</span>
 <span class="s2">"ID"</span>: <span class="s2">"order-service"</span>,
 <span class="s2">"Name"</span>: <span class="s2">"order-service"</span>,
 <span class="s2">"Address"</span>: <span class="s2">"order-service"</span>,
 <span class="s2">"Port"</span>: 5002
 <span class="o">}</span>
 response <span class="o">=</span> requests.put<span class="o">(</span><span class="s1">'http://consul:8500/v1/agent/service/register'</span>, <span class="nv">json</span><span class="o">=</span>payload<span class="o">)</span>
 <span class="k">if </span>response.status_code <span class="o">==</span> 200:
 app.logger.info<span class="o">(</span><span class="s2">"Order service registered successfully"</span><span class="o">)</span>
 <span class="k">else</span>:
 app.logger.error<span class="o">(</span><span class="s2">"Failed to register order service"</span><span class="o">)</span>

<span class="k">if </span>__name__ <span class="o">==</span> <span class="s1">'__main__'</span>:
 <span class="c"># Configure logging</span>
 handler <span class="o">=</span> GelfUdpHandler<span class="o">(</span><span class="nv">host</span><span class="o">=</span><span class="s1">'logstash'</span>, <span class="nv">port</span><span class="o">=</span>12201<span class="o">)</span>
 app.logger.addHandler<span class="o">(</span>handler<span class="o">)</span>
 app.logger.setLevel<span class="o">(</span>logging.INFO<span class="o">)</span>
    
 register_service<span class="o">()</span>
 app.run<span class="o">(</span><span class="nv">host</span><span class="o">=</span><span class="s1">'0.0.0.0'</span>, <span class="nv">port</span><span class="o">=</span>5002<span class="o">)</span>


<span class="c"># create user_service/requirements.txt for each service (user, order)</span>
flask
requests
pygelf

<span class="c"># modify each Dockerfile: Dockerfile-user</span>

<span class="c"># Use an official Python runtime as a parent image</span>
FROM python:3.9-slim

<span class="c"># Set the working directory in the container</span>
WORKDIR /app

<span class="c"># Copy the current directory contents into the container at /app</span>
COPY <span class="nb">.</span> /app

<span class="c"># Install any needed packages specified in requirements.txt</span>
RUN pip <span class="nb">install</span> <span class="nt">--no-cache-dir</span> <span class="nt">-r</span> requirements.txt

<span class="c"># Make port 5001 available to the world outside this container</span>
EXPOSE 5001

<span class="c"># Define environment variable</span>
ENV <span class="nv">FLASK_APP</span><span class="o">=</span>user_service.py

<span class="c"># Run user_service.py when the container launches</span>
CMD <span class="o">[</span><span class="s2">"flask"</span>, <span class="s2">"run"</span>, <span class="s2">"--host=0.0.0.0"</span>, <span class="s2">"--port=5001"</span><span class="o">]</span>


<span class="c"># vim Dockerfile-order</span>

<span class="c"># Use an official Python runtime as a parent image</span>
FROM python:3.9-slim

<span class="c"># Set the working directory in the container</span>
WORKDIR /app

<span class="c"># Copy the current directory contents into the container at /app</span>
COPY <span class="nb">.</span> /app

<span class="c"># Install any needed packages specified in requirements.txt</span>
RUN pip <span class="nb">install</span> <span class="nt">--no-cache-dir</span> <span class="nt">-r</span> requirements.txt

<span class="c"># Make port 5002 available to the world outside this container</span>
EXPOSE 5002

<span class="c"># Define environment variable</span>
ENV <span class="nv">FLASK_APP</span><span class="o">=</span>order_service.py

<span class="c"># Run order_service.py when the container launches</span>
CMD <span class="o">[</span><span class="s2">"flask"</span>, <span class="s2">"run"</span>, <span class="s2">"--host=0.0.0.0"</span>, <span class="s2">"--port=5002"</span><span class="o">]</span>

<span class="c"># modify docker-compose.yaml</span>

version: <span class="s1">'3'</span>
services:
 consul:
 image: consul:1.15.4
 ports:
 - <span class="s2">"8500:8500"</span>

 elasticsearch:
 image: docker.elastic.co/elasticsearch/elasticsearch:7.13.2
 environment:
 - discovery.type<span class="o">=</span>single-node
 ports:
 - <span class="s2">"9200:9200"</span>
 - <span class="s2">"9300:9300"</span>
 volumes:
 - esdata:/usr/share/elasticsearch/data

 logstash:
 image: docker.elastic.co/logstash/logstash:7.13.2
 volumes:
 - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
 ports:
 - <span class="s2">"12201:12201/udp"</span>
 - <span class="s2">"5044:5044"</span>

 kibana:
 image: docker.elastic.co/kibana/kibana:7.13.2
 ports:
 - <span class="s2">"5601:5601"</span>
 depends_on:
 - elasticsearch

 user-service:
 build:
 context: ./user_service
 depends_on:
 - consul
 - logstash
 environment:
 - <span class="nv">CONSUL_HTTP_ADDR</span><span class="o">=</span>consul:8500
 ports:
 - <span class="s2">"5001:5001"</span>
 logging:
 driver: gelf
 options:
 gelf-address: udp://logstash:12201

 order-service:
 build:
 context: ./order_service
 depends_on:
 - consul
 - logstash
 environment:
 - <span class="nv">CONSUL_HTTP_ADDR</span><span class="o">=</span>consul:8500
 ports:
 - <span class="s2">"5002:5002"</span>
 logging:
 driver: gelf
 options:
 gelf-address: udp://logstash:12201

 api-gateway:
 build:
 context: ./api_gateway
 depends_on:
 - consul
 - user-service
 - order-service
 - logstash
 environment:
 - <span class="nv">CONSUL_HTTP_ADDR</span><span class="o">=</span>consul:8500
 ports:
 - <span class="s2">"5000:5000"</span>
 logging:
 driver: gelf
 options:
 gelf-address: udp://logstash:12201

volumes:
 esdata:</code></pre></figure>

<p>Now Run docker-compose to bring all containers up and running. Should see all services with ES, Logstash and Kibana populating logs on the screen.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">docker-compose up <span class="nt">--build</span>

Creating 05-with-elk_logstash_1      ... <span class="k">done
</span>Creating 05-with-elk_consul_1        ... <span class="k">done
</span>Creating 05-with-elk_elasticsearch_1 ... <span class="k">done
</span>Creating 05-with-elk_kibana_1        ... <span class="k">done
</span>Creating 05-with-elk_user-service_1  ... <span class="k">done
</span>Creating 05-with-elk_order-service_1 ... <span class="k">done
</span>Creating 05-with-elk_api-gateway_1   ... <span class="k">done


</span>logstash_1       | <span class="o">[</span>2024-05-18T14:55:13,140][INFO <span class="o">][</span>logstash.inputs.udp      ][main][a30d8db137f99f1de18acbd53c081374cd720430a4dd0e752ff4a99c3005f9d0] Starting UDP listener <span class="o">{</span>:address<span class="o">=&gt;</span><span class="s2">"0.0.0.0:12201"</span><span class="o">}</span>
logstash_1       | <span class="o">[</span>2024-05-18T14:55:13,187][INFO <span class="o">][</span>logstash.inputs.udp      ][main][a30d8db137f99f1de18acbd53c081374cd720430a4dd0e752ff4a99c3005f9d0] UDP listener started <span class="o">{</span>:address<span class="o">=&gt;</span><span class="s2">"0.0.0.0:12201"</span>, :receive_buffer_bytes<span class="o">=&gt;</span><span class="s2">"106496"</span>, :queue_size<span class="o">=&gt;</span><span class="s2">"2000"</span><span class="o">}</span>
consul_1         | 2024-05-18T14:55:46.686Z <span class="o">[</span>DEBUG] agent: Skipping remote check since it is managed automatically: <span class="nv">check</span><span class="o">=</span>serfHealth
consul_1         | 2024-05-18T14:55:46.688Z <span class="o">[</span>DEBUG] agent: Node info <span class="k">in </span><span class="nb">sync
</span>logstash_1       | https://www.elastic.co/guide/en/logstash/current/monitoring-with-metricbeat.html
elasticsearch_1  | <span class="o">{</span><span class="s2">"type"</span>: <span class="s2">"deprecation.elasticsearch"</span>, <span class="s2">"timestamp"</span>: <span class="s2">"2024-05-18T14:55:09,016Z"</span>, <span class="s2">"level"</span>: <span class="s2">"DEPRECATION"</span>, <span class="s2">"component"</span>: <span class="s2">"o.e.d.r.RestController"</span>, <span class="s2">"cluster.name"</span>: <span class="s2">"docker-cluster"</span>, <span class="s2">"node.name"</span>: <span class="s2">"430bff78a529"</span>, <span class="s2">"message"</span>: <span class="s2">"Legacy index templates are deprecated in favor of composable templates."</span>, <span class="s2">"cluster.uuid"</span>: <span class="s2">"B9QKhgEGTA6Ot5auY9skQQ"</span>, <span class="s2">"node.id"</span>: <span class="s2">"QzKPL7DYSB2_CeWJpUaxXg"</span>  <span class="o">}</span>
kibana_1         | <span class="o">{</span><span class="s2">"type"</span>:<span class="s2">"log"</span>,<span class="s2">"@timestamp"</span>:<span class="s2">"2024-05-18T14:55:09+00:00"</span>,<span class="s2">"tags"</span>:[<span class="s2">"info"</span>,<span class="s2">"plugins"</span>,<span class="s2">"monitoring"</span>,<span class="s2">"monitoring"</span>,<span class="s2">"kibana-monitoring"</span><span class="o">]</span>,<span class="s2">"pid"</span>:952,<span class="s2">"message"</span>:<span class="s2">"Starting monitoring stats collection"</span><span class="o">}</span></code></pre></figure>

<p><img src="/assets/flask6.png" alt="image tooltip here" /></p>

<p><b> Verify ElasticSearch and Kibana</b></p>

<ul>
  <li>Validate ElasticSearch status via localhost:9200</li>
</ul>

<p><img src="/assets/flask7.png" alt="image tooltip here" /></p>

<ul>
  <li>Visit localhost:5601 to access Kibana dashboard, add Index Pattern “logs-*” to see data populated in the Discover tab</li>
</ul>

<p><img src="/assets/flask8.png" alt="image tooltip here" />
<b> Conclusion</b></p>

<p>Now we can enable logging with ELK stack, and use Logstash, ElasticSearch and Kibana.</p>

<p>In the next post, I will see how to enable monitoring with Prometheus and Grafana stack.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About logging with ELK (Elasticsearch, Logstash, Kibana) Stack]]></summary></entry></feed>