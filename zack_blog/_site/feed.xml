<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-04-29T09:38:55+10:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Zack’s Blog</title><subtitle>## AWS   ## Jenkins  ## Microservices ## Automation ## K8S   ## CICD     ## Gitops  [京ICP备2024056683号-1]</subtitle><entry><title type="html">Ubuntu 18.04 to 22.04 in-place upgrade</title><link href="http://localhost:4000/jekyll/cat2/2024/04/28/ubt-upg.html" rel="alternate" type="text/html" title="Ubuntu 18.04 to 22.04 in-place upgrade" /><published>2024-04-28T10:15:29+10:00</published><updated>2024-04-28T10:15:29+10:00</updated><id>http://localhost:4000/jekyll/cat2/2024/04/28/ubt-upg</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/04/28/ubt-upg.html"><![CDATA[<p><b> Ubuntu releases EOL roadmap </b></p>

<p>Every single Ubuntu LTS comes with 5 years of standard support. During those five years, bug fixes and security patches will be provided. Ubuntu 18.04 ‘Bionic Beaver’ is reaching End of Standard Support this May. so today we are going to run in-place upgrade for ubuntu 18.04 LTS to 22.04 LTS.</p>

<p><img src="/assets/ubt-upg1.png" alt="image tooltip here" /></p>

<p><b> Pre-upgrade checklist </b></p>

<ul>
  <li>validate current OS version and running service (nginx)</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># current OS version</span>
root@ubuntu-test:~# <span class="nb">cat</span> /etc/os-release 
<span class="nv">NAME</span><span class="o">=</span><span class="s2">"Ubuntu"</span>
<span class="nv">VERSION</span><span class="o">=</span><span class="s2">"18.04.6 LTS (Bionic Beaver)"</span>
<span class="nv">ID</span><span class="o">=</span>ubuntu
<span class="nv">ID_LIKE</span><span class="o">=</span>debian
<span class="nv">PRETTY_NAME</span><span class="o">=</span><span class="s2">"Ubuntu 18.04.6 LTS"</span>
<span class="nv">VERSION_ID</span><span class="o">=</span><span class="s2">"18.04"</span>
<span class="nv">HOME_URL</span><span class="o">=</span><span class="s2">"https://www.ubuntu.com/"</span>
<span class="nv">SUPPORT_URL</span><span class="o">=</span><span class="s2">"https://help.ubuntu.com/"</span>
<span class="nv">BUG_REPORT_URL</span><span class="o">=</span><span class="s2">"https://bugs.launchpad.net/ubuntu/"</span>
<span class="nv">PRIVACY_POLICY_URL</span><span class="o">=</span><span class="s2">"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"</span>
<span class="nv">VERSION_CODENAME</span><span class="o">=</span>bionic
<span class="nv">UBUNTU_CODENAME</span><span class="o">=</span>bionic

<span class="c"># nginx service status</span>

root@ubuntu-test:~# root@ubuntu-test:~# <span class="nb">echo</span> <span class="s2">"ubuntu-inplace-upgrade zack-testing-nginx-service!!"</span>  <span class="o">&gt;&gt;</span> /var/www/html/index.html
root@ubuntu-test:~# systemctl restart nginx
root@ubuntu-test:~# curl localhost
ubuntu-inplace-upgrade zack-testing-nginx-service!!</code></pre></figure>

<ul>
  <li>Fully update the system</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># update system</span>
root@ubuntu-test:~# <span class="nb">sudo </span>apt update
Hit:1 http://au.archive.ubuntu.com/ubuntu bionic InRelease
Hit:2 http://au.archive.ubuntu.com/ubuntu bionic-updates InRelease
Hit:3 http://au.archive.ubuntu.com/ubuntu bionic-backports InRelease
Hit:4 http://au.archive.ubuntu.com/ubuntu bionic-security InRelease
Reading package lists... Done                      
Building dependency tree       
Reading state information... Done
All packages are up to date.
root@ubuntu-test:~# <span class="nb">sudo </span>apt upgrade <span class="nt">-y</span>
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Calculating upgrade... Done
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.

<span class="c"># reboot system before upgrade</span>
root@ubuntu-test:~# <span class="nb">sudo </span><span class="k">do</span><span class="nt">-release-upgrade</span>
Checking <span class="k">for </span>a new Ubuntu release
You have not rebooted after updating a package which requires a reboot. Please reboot before upgrading.
root@ubuntu-test:~# reboot
Connection closing...Socket close.</code></pre></figure>

<ul>
  <li>take full system backup</li>
</ul>

<p>here I took a VM snapshot before upgrade</p>

<p><b> 18.04 to 22.04 upgrade</b></p>

<p>There is no direct upgrade path from 18.04 LTS to Ubuntu 22.04 LTS, so we go Ubuntu 20.04 LTS first and then to Ubuntu 22.04 LTS.</p>

<ul>
  <li>first upgrade to 20.04</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># run upgrade</span>

root@ubuntu-test:~# <span class="nb">sudo </span><span class="k">do</span><span class="nt">-release-upgrade</span>

This session appears to be running under ssh. It is not recommended 
to perform a upgrade over ssh currently because <span class="k">in case</span> of failure it 
is harder to recover. 

If you <span class="k">continue</span>, an additional ssh daemon will be started at port 
<span class="s1">'1022'</span><span class="nb">.</span> 
Do you want to <span class="k">continue</span>? 

Continue <span class="o">[</span>yN] y

Starting additional sshd 

Calculating the changes
  MarkInstall libfwupdplugin1:amd64 &lt; none -&gt; 1.5.11-0ubuntu1~20.04.2 @un uN Ib <span class="o">&gt;</span> <span class="nv">FU</span><span class="o">=</span>1
  Installing libxmlb1 as Depends of libfwupdplugin1
    MarkInstall libxmlb1:amd64 &lt; none -&gt; 0.1.15-2ubuntu1~20.04.1 @un uN <span class="o">&gt;</span> <span class="nv">FU</span><span class="o">=</span>0

Do you want to start the upgrade? 

Continue <span class="o">[</span>yN]  Details <span class="o">[</span>d]y</code></pre></figure>

<ul>
  <li>allow service restart during upgrade</li>
</ul>

<p><img src="/assets/ubt-upg2.png" alt="image tooltip here" /></p>

<ul>
  <li>reboot after upgrade</li>
</ul>

<p>The installation and removing of packages may take some time, then reboot is required after ungrade completion</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">Purging configuration files <span class="k">for </span>ebtables <span class="o">(</span>2.0.11-3build1<span class="o">)</span> ...
Purging configuration files <span class="k">for </span>python3.6-minimal <span class="o">(</span>3.6.9-1~18.04ubuntu1.12<span class="o">)</span> ...
Purging configuration files <span class="k">for </span>mlocate <span class="o">(</span>0.26-3ubuntu3<span class="o">)</span> ...
Processing triggers <span class="k">for </span>dbus <span class="o">(</span>1.12.16-2ubuntu2.3<span class="o">)</span> ...
Processing triggers <span class="k">for </span>systemd <span class="o">(</span>245.4-4ubuntu3.23<span class="o">)</span> ...

System upgrade is complete.

Restart required 

To finish the upgrade, a restart is required. 
If you <span class="k">select</span> <span class="s1">'y'</span> the system will be restarted. 

Continue <span class="o">[</span>yN] y</code></pre></figure>

<ul>
  <li>validate OS and service</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># validate nginx service</span>
root@ubuntu-test:~# curl localhost
ubuntu-inplace-upgrade  zack-testing-nginx-service!!
<span class="c"># validate OS version</span>
root@ubuntu-test:~# <span class="nb">cat</span> /etc/os-release 
<span class="nv">NAME</span><span class="o">=</span><span class="s2">"Ubuntu"</span>
<span class="nv">VERSION</span><span class="o">=</span><span class="s2">"20.04.6 LTS (Focal Fossa)"</span>
<span class="nv">ID</span><span class="o">=</span>ubuntu
<span class="nv">ID_LIKE</span><span class="o">=</span>debian
<span class="nv">PRETTY_NAME</span><span class="o">=</span><span class="s2">"Ubuntu 20.04.6 LTS"</span>
<span class="nv">VERSION_ID</span><span class="o">=</span><span class="s2">"20.04"</span>
<span class="nv">HOME_URL</span><span class="o">=</span><span class="s2">"https://www.ubuntu.com/"</span>
<span class="nv">SUPPORT_URL</span><span class="o">=</span><span class="s2">"https://help.ubuntu.com/"</span>
<span class="nv">BUG_REPORT_URL</span><span class="o">=</span><span class="s2">"https://bugs.launchpad.net/ubuntu/"</span>
<span class="nv">PRIVACY_POLICY_URL</span><span class="o">=</span><span class="s2">"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"</span>
<span class="nv">VERSION_CODENAME</span><span class="o">=</span>focal
<span class="nv">UBUNTU_CODENAME</span><span class="o">=</span>focal</code></pre></figure>

<ul>
  <li>then upgrade to 22.04</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">root@ubuntu-test:~# <span class="nb">sudo </span>apt update
Hit:1 http://au.archive.ubuntu.com/ubuntu focal InRelease
Hit:2 http://au.archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:3 http://au.archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:4 http://au.archive.ubuntu.com/ubuntu focal-security InRelease
Reading package lists... Done
Building dependency tree       
Reading state information... Done
All packages are up to date.

root@ubuntu-test:~# <span class="nb">sudo </span>apt upgrade
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Calculating upgrade... Done
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.

root@ubuntu-test:~# <span class="nb">sudo </span><span class="k">do</span><span class="nt">-release-upgrade</span>

<span class="c"># validate after upgrade </span>
root@ubuntu-test:~# curl localhost
ubuntu-inplace-upgrade  zack-testing-nginx-service!!

root@ubuntu-test:~# lsb_release <span class="nt">-a</span>
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 22.04.4 LTS
Release:	22.04
Codename:	jammy</code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we complete the in-place ubuntu OS release upgrade from 18.04 to 22.04. The whole upgrade took about 1 hour to finish, with several comfirmation required during upgrade process. The service nginx was running after each upgrade.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Ubuntu releases EOL roadmap]]></summary></entry><entry><title type="html">RedHat Identity management (IdM)</title><link href="http://localhost:4000/jekyll/cat2/2024/04/04/Redhat-idm.html" rel="alternate" type="text/html" title="RedHat Identity management (IdM)" /><published>2024-04-04T11:15:29+11:00</published><updated>2024-04-04T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/04/04/Redhat-idm</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/04/04/Redhat-idm.html"><![CDATA[<p><b> About Linux Identity management </b></p>

<p>In Linux, user and permission management is crucial for maintaining system security and controlling access to files, directories, and resources.</p>

<p>Redhat provides such enterprise-level solution called redhat Identity management (IdM), also with its opensource free version called freeIPA, to offer centralized authentication, authorization, and identity management services such as Single Sign-On (SSO), Role-Based Access Control (RBAC), Identity Federation, Integration with Microsoft AD and AAD, also can cross-cloud access with AWS SSO as external identity provider.</p>

<p>when a company facing challenge to manage its Linux environments across local and public cloud, RedHat Identity management can be the solution to achieve:</p>

<ul>
  <li>
    <p>With Local AD and Azure AD (AAD) Integration</p>
  </li>
  <li>
    <p>With AWS SSO Integration as externel identity provider</p>
  </li>
  <li>
    <p>LDAP – an LDAP directory (389 Directory Server) is embedded</p>
  </li>
  <li>
    <p>Kerberos – KDC (MIT Kerberos) for Kerberos key management and single signon</p>
  </li>
  <li>
    <p>DNS – BIND (bind-dyndb-ldap) for domain name services</p>
  </li>
  <li>
    <p>PKI – Red Hat Certificate System (Dogtag certificate system) provides public key infrastructure for certificate management</p>
  </li>
  <li>
    <p>NTP – Network Time Protocol service for time sync</p>
  </li>
  <li>
    <p>A web-based management front-end running on Apache</p>
  </li>
</ul>

<p><b> A Typical AD User Authentication Flow across AWS and IdM would be: </b></p>

<ul>
  <li>
    <p>When a user attempts to access AWS resources, they first authenticate through Azure AD using their Azure AD credentials.</p>
  </li>
  <li>
    <p>Azure AD authenticates the user and issues a security token.</p>
  </li>
  <li>
    <p>The user accesses AWS SSO, which is configured to use Red Hat IDM as an identity provider.</p>
  </li>
  <li>
    <p>AWS SSO redirects the user to the Red Hat IDM authentication page.</p>
  </li>
  <li>
    <p>The user enters their Red Hat IDM credentials and authenticates against the IdM server.</p>
  </li>
  <li>
    <p>Red Hat IDM validates the user’s credentials and issues a SAML assertion to AWS SSO.</p>
  </li>
  <li>
    <p>AWS SSO validates the SAML assertion and grants the user access to the requested AWS resources.</p>
  </li>
</ul>

<p><b> FreeIPA: the opensource version of RedHat IdM </b></p>

<p>Here I am going to install and configure a local lab IdM portal using the opensource version of RedHat IdM called “freeIPA”, with 3 linux boxes to validate the user permission and client hosts (both CentOS and Ubuntu) enrollment, requirement and design as bellow:</p>

<ul>
  <li>
    <p>freeIPA Server: freeipa-server.zackz.oonline 11.0.1.150 (CentOS 7.9)</p>
  </li>
  <li>
    <p>freeIPA Client1: freeipa-client1.zackz.oonline 11.0.1.151 (CentOS 7.9)</p>
  </li>
  <li>
    <p>freeIPA Client2: freeipa-client2.zackz.oonline 11.0.1.72 (Ubuntu 22.04)</p>
  </li>
</ul>

<p><b> Local testlab FreeIPA server installation </b></p>

<p>On freeIPA Server freeipa-server.zackz.oonline 11.0.1.150 (CentOS 7.9)</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># set hostname with domain</span>
hostnamectl set-hostname freeipa-server.zackz.oonline

<span class="c"># add 3 hosts to /etc/hosts</span>
<span class="nb">echo </span>11.0.1.150 freeipa-server.zackz.oonline  ipa <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.151 freeipa-client1.zackz.oonline ipa <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.72  freeipa-client2.zackz.oonline ipa <span class="o">&gt;&gt;</span> /etc/hosts

<span class="c"># install ipa-server</span>
yum <span class="nb">install </span>ipa-server bind-dyndb-ldap ipa-server-dns

<span class="c"># Configure ipa-server and DNS, here set ipa console and domain admin passwd</span>
ipa-server-install <span class="nt">--setup-dns</span>

<span class="c"># configure firewall rules and services</span>
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-service</span><span class="o">={</span>http,https,ldap,ldaps,kerberos,dns,kpasswd,ntp<span class="o">}</span>
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-service</span> freeipa-ldap
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-service</span> freeipa-ldaps
firewall-cmd <span class="nt">--reload</span>

<span class="c"># check ipastatus</span>
<span class="o">[</span>root@freeipa ~]# ipactl status
Directory Service: RUNNING
krb5kdc Service: RUNNING
kadmin Service: RUNNING
httpd Service: RUNNING
ipa-custodia Service: RUNNING
ntpd Service: RUNNING
pki-tomcatd Service: RUNNING
ipa-otpd Service: RUNNING
ipa: INFO: The ipactl <span class="nb">command </span>was successful


<span class="c"># Obtain a Kerberos ticket for the Kerberos admin user and Verify the ticket</span>
kinit admin
klist

Ticket cache: KEYRING:persistent:0:0
Default principal: admin@ZACKZ.OONLINE

Valid starting     Expires            Service principal
04/04/24 22:17:29  05/04/24 22:02:43  HTTP/freeipa-server.zackz.oonline@ZACKZ.OONLINE

<span class="c"># check content of /etc/resolv.conf</span>
<span class="nb">cat</span> /etc/resolv.conf
search zackz.oonline
nameserver 127.0.0.1

<span class="c"># Configure FreeIPA for User Authentication</span>

yum <span class="nb">install</span> <span class="nt">-y</span> vsftpd
systemctl <span class="nb">enable </span>vsftpd <span class="o">&amp;&amp;</span> systemctl start vsftpd
firewall-cmd <span class="nt">--permanent</span> <span class="nt">--add-service</span><span class="o">=</span>ftp
firewall-cmd <span class="nt">--reload</span>


<span class="c"># copy CA certificate of the IPA server to the FTP site</span>
<span class="nb">cp</span> /root/cacert.p12 /var/ftp/pub

<span class="c"># Configure default login shell to Bash and Create Users</span>
ipa config-mod <span class="nt">--defaultshell</span><span class="o">=</span>/bin/bash
ipa user-add alice <span class="nt">--first</span><span class="o">=</span>alice <span class="nt">--last</span><span class="o">=</span>abernathy <span class="nt">--password</span>
ipa user-add vince <span class="nt">--first</span><span class="o">=</span>vincent <span class="nt">--last</span><span class="o">=</span>valentine <span class="nt">--password</span>

<span class="c"># add client hosts </span>
ipa host-add <span class="nt">--ip-address</span> 11.0.1.151 freeipa-client1.zackz.oonline
ipa host-add <span class="nt">--ip-address</span> 11.0.1.72 freeipa-client2.zackz.oonline

<span class="c"># create NFS service entry in the IdM domain</span>
ipa service-add nfs/freeipa-client1.zackz.oonline
ipa service-add nfs/freeipa-client2.zackz.oonline

<span class="c"># add entry to the keytab file /etc/krb5.keytab</span>
kadmin.local
Authenticating as principal admin/admin@RHCE.LOCAL with password.
kadmin.local:  ktadd nfs/freeipa-client1.zackz.oonline
kadmin.local:  ktadd nfs/freeipa-client2.zackz.oonline
kadmin.local:  quit

<span class="c"># verify keytab file</span>
<span class="o">[</span>root@freeipa-server pub]# klist <span class="nt">-k</span>
Keytab name: FILE:/etc/krb5.keytab
KVNO Principal
<span class="nt">----</span> <span class="nt">--------------------------------------------------------------------------</span>
   2 host/freeipa-server.zackz.oonline@ZACKZ.OONLINE
   2 host/freeipa-server.zackz.oonline@ZACKZ.OONLINE
   2 host/freeipa-server.zackz.oonline@ZACKZ.OONLINE
   2 host/freeipa-server.zackz.oonline@ZACKZ.OONLINE
   2 host/freeipa-server.zackz.oonline@ZACKZ.OONLINE
   2 host/freeipa-server.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client2.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client2.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client2.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client2.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client2.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client2.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client1.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client1.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client1.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client1.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client1.zackz.oonline@ZACKZ.OONLINE
   1 nfs/freeipa-client1.zackz.oonline@ZACKZ.OONLINE


<span class="c"># Generate keys to copy over to NFS systems, chmod to Make the keytab file accessible to FTP clients</span>
ipa-getkeytab <span class="nt">-s</span> freeipa-server.zackz.oonline <span class="nt">-p</span> nfs/freeipa-client2.zackz.oonline <span class="nt">-k</span> /var/ftp/pub/freeipa-client2.keytab
ipa-getkeytab <span class="nt">-s</span> freeipa-server.zackz.oonline <span class="nt">-p</span> nfs/freeipa-client1.zackz.oonline <span class="nt">-k</span> /var/ftp/pub/freeipa-client1.keytab
<span class="nb">chmod </span>644 /var/ftp/pub/<span class="k">*</span>.keytab

<span class="c"># Configure DNS</span>
ipa dnszone-mod <span class="nt">--allow-transfer</span><span class="o">=</span>11.0.1.0/24 zackz.oonline
ipa dnsrecord-add zackz.oonline vhost1 <span class="nt">--ttl</span><span class="o">=</span>3600 <span class="nt">--a-ip-address</span><span class="o">=</span>11.0.1.151
ipa dnsrecord-add zackz.oonline dynamic1 <span class="nt">--ttl</span><span class="o">=</span>3600 <span class="nt">--a-ip-address</span><span class="o">=</span>11.0.1.151
ipa dnsrecord-add zackz.oonline @ <span class="nt">--mx-rec</span><span class="o">=</span><span class="s2">"0 freeipa-server.zackz.oonline."</span></code></pre></figure>

<p><b> Console login </b></p>

<p>login freeIPA console with serverIP or DNS name</p>

<p><img src="/assets/linuxidm1.png" alt="image tooltip here" /></p>

<p>list the 2 users “alice” and “vincent” previously created</p>

<p><img src="/assets/linuxidm2.png" alt="image tooltip here" /></p>

<p><b> Add and enroll client hosts into IdM </b></p>

<p>To add client hosts into freeIPA can be done via command or console, but to enroll the host can only be done via each client host by install “freeipa-client” and condigure domain.</p>

<ul>
  <li>For CentOS client (11.0.1.151 freeipa-client1.zackz.oonline) enrollment:</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># set client hostname with domain</span>
hostnamectl set-hostname freeipa-client1.zackz.oonline

<span class="c"># add 3 hosts to /etc/hosts</span>
<span class="nb">echo </span>11.0.1.150 freeipa-server.zackz.oonline  ipa <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.151 freeipa-client1.zackz.oonline ipa <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.72  freeipa-client2.zackz.oonline ipa <span class="o">&gt;&gt;</span> /etc/hosts


<span class="c"># install freeipa-client and manually configure domain</span>
<span class="o">[</span>root@freeipa-client1 ~]# ipa-client-install <span class="nt">--mkhomedir</span>
WARNING: ntpd <span class="nb">time</span>&amp;date synchronization service will not be configured as
conflicting service <span class="o">(</span>chronyd<span class="o">)</span> is enabled
Use <span class="nt">--force-ntpd</span> option to disable it and force configuration of ntpd

DNS discovery failed to determine your DNS domain
Provide the domain name of your IPA server <span class="o">(</span>ex: example.com<span class="o">)</span>: zackz.oonline
Provide your IPA server name <span class="o">(</span>ex: ipa.example.com<span class="o">)</span>: freeipa-server.zackz.oonline
The failure to use DNS to find your IPA server indicates that your resolv.conf file is not properly configured.
Autodiscovery of servers <span class="k">for </span>failover cannot work with this configuration.
If you proceed with the installation, services will be configured to always access the discovered server <span class="k">for </span>all operations and will not fail over to other servers <span class="k">in case</span> of failure.
Proceed with fixed values and no DNS discovery? <span class="o">[</span>no]: <span class="nb">yes
</span>Client <span class="nb">hostname</span>: freeipa-client1.zackz.oonline
Realm: ZACKZ.OONLINE
DNS Domain: zackz.oonline
IPA Server: freeipa-server.zackz.oonline
BaseDN: <span class="nv">dc</span><span class="o">=</span>zackz,dc<span class="o">=</span>oonline

Continue to configure the system with these values? <span class="o">[</span>no]: <span class="nb">yes
</span>Skipping synchronizing <span class="nb">time </span>with NTP server.
User authorized to enroll computers: admin
Password <span class="k">for </span>admin@ZACKZ.OONLINE: 
Successfully retrieved CA cert
    Subject:     <span class="nv">CN</span><span class="o">=</span>Certificate Authority,O<span class="o">=</span>ZACKZ.OONLINE
    Issuer:      <span class="nv">CN</span><span class="o">=</span>Certificate Authority,O<span class="o">=</span>ZACKZ.OONLINE
    Valid From:  2024-04-04 10:45:20
    Valid Until: 2044-04-04 11:45:20

Enrolled <span class="k">in </span>IPA realm ZACKZ.OONLINE
Created /etc/ipa/default.conf
New SSSD config will be created
Configured sudoers <span class="k">in</span> /etc/nsswitch.conf
Configured /etc/sssd/sssd.conf
trying https://freeipa-server.zackz.oonline/ipa/json
<span class="o">[</span>try 1]: Forwarding <span class="s1">'schema'</span> to json server <span class="s1">'https://freeipa-server.zackz.oonline/ipa/json'</span>
trying https://freeipa-server.zackz.oonline/ipa/session/json
<span class="o">[</span>try 1]: Forwarding <span class="s1">'ping'</span> to json server <span class="s1">'https://freeipa-server.zackz.oonline/ipa/session/json'</span>
<span class="o">[</span>try 1]: Forwarding <span class="s1">'ca_is_enabled'</span> to json server <span class="s1">'https://freeipa-server.zackz.oonline/ipa/session/json'</span>
Systemwide CA database updated.
Hostname <span class="o">(</span>freeipa-client1.zackz.oonline<span class="p">)</span> does not have A/AAAA record.
Failed to update DNS records.
Missing reverse record<span class="o">(</span>s<span class="o">)</span> <span class="k">for </span>address<span class="o">(</span>es<span class="o">)</span>: 11.0.1.151.
Adding SSH public key from /etc/ssh/ssh_host_rsa_key.pub
Adding SSH public key from /etc/ssh/ssh_host_ecdsa_key.pub
Adding SSH public key from /etc/ssh/ssh_host_ed25519_key.pub
<span class="o">[</span>try 1]: Forwarding <span class="s1">'host_mod'</span> to json server <span class="s1">'https://freeipa-server.zackz.oonline/ipa/session/json'</span>
Could not update DNS SSHFP records.
SSSD enabled
Configured /etc/openldap/ldap.conf
Unable to find <span class="s1">'admin'</span> user with <span class="s1">'getent passwd admin@zackz.oonline'</span><span class="o">!</span>
Unable to reliably detect configuration. Check NSS setup manually.
Configured /etc/ssh/ssh_config
Configured /etc/ssh/sshd_config
Configuring zackz.oonline as NIS domain.
Configured /etc/krb5.conf <span class="k">for </span>IPA realm ZACKZ.OONLINE
Client configuration complete.
The ipa-client-install <span class="nb">command </span>was successful</code></pre></figure>

<ul>
  <li>for Ubuntu client host (11.0.1.72 freeipa-client2.zackz.oonline) enrollment:</li>
</ul>

<p>First back to freeIPA server, add DNS record for Ubuntu</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># add DNS record for Ubuntu client on freeIPA server</span>
ipa dnsrecord-add hwdomain.io ubuntu-node.hwdomain.io <span class="nt">--a-rec</span> 192.168.10.50</code></pre></figure>

<p>Then login Ubuntu host to install freeipa-client and enroll:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># set hostname </span>
hostnamectl set-hostname freeipa-client2.zackz.oonline

<span class="c"># add 3 hosts to /etc/hosts</span>
<span class="nb">echo </span>11.0.1.150 freeipa-server.zackz.oonline  ipa <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.151 freeipa-client1.zackz.oonline ipa <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nb">echo </span>11.0.1.72  freeipa-client2.zackz.oonline ipa <span class="o">&gt;&gt;</span> /etc/hosts

<span class="c"># install ipa client</span>
apt update <span class="o">&amp;&amp;</span> apt <span class="nb">install </span>freeipa-client oddjob-mkhomedir

<span class="c"># add ubuntu client to freeIPA server</span>
ipa-client-install <span class="nt">--hostname</span><span class="o">=</span><span class="sb">`</span><span class="nb">hostname</span> <span class="nt">-f</span><span class="sb">`</span> <span class="nt">--mkhomedir</span> /
<span class="nt">--server</span><span class="o">=</span>freeipa-server.zackz.oonline /
<span class="nt">--domain</span> zackz.online /
<span class="nt">--realm</span> ZACKZ.OONLINE

<span class="c"># change PAM profile to enable "Create a home directory on login" </span>
pam-auth-update</code></pre></figure>

<p>now back to console, the 2 client hosts had been added and enrolled into freeIPA server
<img src="/assets/linuxidm3.png" alt="image tooltip here" /></p>

<ul>
  <li>verify to use user “alice” and its passwd to ssh login to ubuntu client from freeIPA server</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@freeipa-server ~]# ssh alice@freeipa-client2.zackz.oonline
Password: 
Welcome to Ubuntu 22.04.4 LTS <span class="o">(</span>GNU/Linux 5.15.0-101-generic x86_64<span class="o">)</span>

 <span class="k">*</span> Documentation:  https://help.ubuntu.com
 <span class="k">*</span> Management:     https://landscape.canonical.com
 <span class="k">*</span> Support:        https://ubuntu.com/pro

  System information as of Thu Apr  4 01:02:00 PM UTC 2024

  System load:  0.0               Processes:              234
  Usage of /:   8.0% of 93.93GB   Users logged <span class="k">in</span>:        1
  Memory usage: 11%               IPv4 address <span class="k">for </span>ens33: 11.0.1.72
  Swap usage:   0%

 <span class="k">*</span> Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar <span class="k">for </span>easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

Expanded Security Maintenance <span class="k">for </span>Applications is not enabled.

18 updates can be applied immediately.
To see these additional updates run: apt list <span class="nt">--upgradable</span>

Enable ESM Apps to receive additional future security updates.
See https://ubuntu.com/esm or run: <span class="nb">sudo </span>pro status


Last login: Thu Apr  4 11:47:37 2024 from 11.0.1.150
alice@freeipa-client2:~<span class="nv">$ </span><span class="nb">id
</span><span class="nv">uid</span><span class="o">=</span>1239800001<span class="o">(</span>alice<span class="o">)</span> <span class="nv">gid</span><span class="o">=</span>1239800001<span class="o">(</span>alice<span class="o">)</span> <span class="nb">groups</span><span class="o">=</span>1239800001<span class="o">(</span>alice<span class="o">)</span>
alice@freeipa-client2:~<span class="nv">$ </span></code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we install Redhat IdM server and be able to enroll client hosts, looking at the user details, IdM using Kerberos for authentication, together with user group, policy, HBAC and Sudo roles, provides a flexible and robust authentication framework that supports multiple authentication mechanisms, enabling organizations to authenticate users securely across their Linux and Unix environments.</p>

<p><img src="/assets/linuxidm5.png" alt="image tooltip here" /></p>

<p>More info can be found via <a href="https://freeipa.readthedocs.io/en/latest/workshop.html">Freeipa workshop</a>, <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_idm_users_groups_hosts_and_access_control_rules/index">Red Hat product documentation</a>, <a href="https://chamathb.wordpress.com/2019/06/21/setting-up-rhel-idm-with-integrated-dns-on-aws/">Redhat Idm on AWS with DNS forwarder</a>, <a href="https://www.reddit.com/r/redhat/comments/6ixtoe/idmfreeipa_dns_forwarding/">idmfreeipa DNS forwarder configurations on AWS</a>, and <a href="https://redhat.com/en/blog/automating-red-hat-identity-management-installation">Automating Red Hat Identity Management installation with Ansible</a>.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Linux Identity management]]></summary></entry><entry><title type="html">K8S with External Cloud Controller Manager</title><link href="http://localhost:4000/jekyll/cat2/2024/02/26/External-cloudprovider.html" rel="alternate" type="text/html" title="K8S with External Cloud Controller Manager" /><published>2024-02-26T11:15:29+11:00</published><updated>2024-02-26T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/02/26/External-cloudprovider</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/02/26/External-cloudprovider.html"><![CDATA[<p><b> Scenario and Challange </b></p>

<p>In last post I was able to automate and create a self-owned K8S cluster on AWS EC2 instances by using Ansible and Terraform.</p>

<p>In this Scenario when deploying k8s pods and services in this K8S cluster, it will not create AWS loadbalancer even mentioned type = LoadBalancer, but this can be eaily achieved when in a AWS managed EKS cluster.</p>

<p>By searching online, Kubernetes actually provides such solution called <b>“cloud-provider-aws”</b>, which provides interface between a self-owned AWS Kubernetes cluster and AWS APIs. This allows EC2 instances running Kubernetes node to be able to provision AWS NLB or ELB resources during service deployment by mentioning “LoadBalancer”..</p>

<p><b></b></p>

<p>To enable the Kubernetes External Cloud Controller Manager, a <b>AWS Cloud Controller manager</b> need to be deployed into cluster, by doing so, it will create and a AWS loadbalancers (NLB), then self-owned K8S cluster can expose services externally by creating ELB.</p>

<p><b> Steps</b></p>

<p>There are a few steps need to be done, docs can be followed by bellow link:</p>

<p>https://github.com/kubernetes/cloud-provider-aws/blob/master/docs/getting_started.md</p>

<ul>
  <li>Change EC2 hostname from ip to FQDN</li>
</ul>

<p>To be able to communicate with AWS API, the k8s node should be changed to FQDN rather than default EC2 IP address</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">curl http://169.254.169.254/latest/meta-data/local-hostname
hostnamectl set-hostname</code></pre></figure>

<ul>
  <li>Create and assign IAM roles to k8s nodes</li>
</ul>

<p>IAM role needed for EC2 running K8S nodes to have proper permission to interact with AWS APIs and create and maintain AWS service</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Roles for master and worker nodes can be found bellow link</span>
https://github.com/kubernetes/cloud-provider-aws/blob/master/docs/prerequisites.md </code></pre></figure>

<ul>
  <li>Tag ec2 instances as owned</li>
</ul>

<p>The K8S nodes need to be tagged as owned</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">tag kubernetes.io/cluster/your_cluster_id<span class="o">=</span>owned</code></pre></figure>

<ul>
  <li>AWS Cloud Controller manager need to be deployed into K8S, this will create a NetworkLoadbalancer in AWS</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl apply <span class="nt">-k</span> <span class="s1">'github.com/kubernetes/cloud-provider-aws/examples/existing-cluster/base/?ref=master</span></code></pre></figure>

<ul>
  <li>Add the –cloud-provider=external to the kube-controller-manager config, kube apiserver config and kubelet’s config</li>
</ul>

<p><b> Conclusion</b></p>

<p>now when create a k8s deployment and service, it will automatically create AWS loadbalancer to route traffic from external into K8S internal pods</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Scenario and Challange]]></summary></entry><entry><title type="html">Automate K8S with Terraform and Ansible</title><link href="http://localhost:4000/jekyll/cat2/2024/02/24/EC2-K8S.html" rel="alternate" type="text/html" title="Automate K8S with Terraform and Ansible" /><published>2024-02-24T11:15:29+11:00</published><updated>2024-02-24T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/02/24/EC2-K8S</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/02/24/EC2-K8S.html"><![CDATA[<p><b> K8S on EC2  vs EKS </b></p>

<p>While I was in an interview last week, the company heavily using Ansible to provision and configure their k8s cluster on AWS EC2, aim to have the most control and flexibility in case they need to move to another cloud provider.</p>

<p>Here I will use both Terraform and Ansible to automate K8S cluster creation and configuration on a few EC2 instances.</p>

<p><b> The tools and workflow</b></p>

<ul>
  <li>Major tools will include Ansible, terraform, together with a shell script.</li>
</ul>

<ol>
  <li>
    <p>Use terraform to create S3 as backend state file, create security groups and 4 ec2 instances ( 1 bastion, 1 master, 2 workers)</p>
  </li>
  <li>
    <p>As Instances are created in AWS environment dynamically, Ansible ec2-dynamic-inventory approach is best way to manage them.</p>
  </li>
  <li>
    <p>Using terraform “connection”, to ssh to the bastion host after creation, use provisoners “file” to upload shell script to bootstrap bastion host as a ansbile master, configure ec2-dynamic-inventory to fetch the other 3 instances for K8S.</p>
  </li>
  <li>
    <p>Continue with terraform “file” and “remote-exec” to upload playbooks and with “inline” to execute playbooks to init k8s master node and join the worker node.</p>
  </li>
  <li>
    <p>Finally another playbook to configure bastion to run kubectl</p>
  </li>
</ol>

<p><b> Create SGs and EC2s, with Bastion bootstrap via terraform </b></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># main.tf</span>

resource <span class="s2">"aws_instance"</span> <span class="s2">"bolg"</span> <span class="o">{</span>
  ami             <span class="o">=</span> var.ami_id <span class="c"># Replace with your AMI ID</span>
  instance_type   <span class="o">=</span> var.instance_type_free
  security_groups <span class="o">=</span> <span class="o">[</span>aws_security_group.blog_sg.name]
  <span class="c">#  vpc_security_group_ids = ["sg-089842a753c9309bb"]</span>
  key_name <span class="o">=</span> var.key_pair
  user_data <span class="o">=</span> <span class="o">&lt;&lt;-</span><span class="no">EOF</span><span class="sh">
    #!/bin/bash
    sudo apt update
    sudo apt install software-properties-common
    sudo add-apt-repository --yes --update ppa:ansible/ansible
    sudo apt install ansible -y
</span><span class="no">  EOF
</span>  tags <span class="o">=</span> <span class="o">{</span>
    Name <span class="o">=</span> <span class="s2">"blog"</span>
  <span class="o">}</span>
  lifecycle <span class="o">{</span>
    prevent_destroy <span class="o">=</span> <span class="nb">true</span>
  <span class="o">}</span>
<span class="o">}</span>
resource <span class="s2">"null_resource"</span> <span class="s2">"upload_playbook"</span> <span class="o">{</span>
  triggers <span class="o">=</span> <span class="o">{</span>
    <span class="c"># Add a dummy trigger to force a refresh</span>
    timestamp <span class="o">=</span> <span class="s2">"</span><span class="k">${</span><span class="nv">timestamp</span><span class="p">()</span><span class="k">}</span><span class="s2">"</span>
  <span class="o">}</span>
  depends_on <span class="o">=</span> <span class="o">[</span>null_resource.wait_for_bastion]
  connection <span class="o">{</span>
    <span class="nb">type</span>        <span class="o">=</span> <span class="s2">"ssh"</span>
    user        <span class="o">=</span> <span class="s2">"ubuntu"</span>
    private_key <span class="o">=</span> file<span class="o">(</span><span class="s2">"terraform-new-key1.pem"</span><span class="o">)</span>
    host        <span class="o">=</span> aws_instance.testbastion.public_ip
  <span class="o">}</span>
  provisioner <span class="s2">"file"</span> <span class="o">{</span>
    <span class="nb">source</span>      <span class="o">=</span> <span class="s2">"pb1.yaml"</span>
    destination <span class="o">=</span> <span class="s2">"/home/ubuntu/pb1.yaml"</span>
  <span class="o">}</span>
  provisioner <span class="s2">"file"</span> <span class="o">{</span>
    <span class="nb">source</span>      <span class="o">=</span> <span class="s2">"aws_ec2.yaml"</span>
    destination <span class="o">=</span> <span class="s2">"/home/ubuntu/aws_ec2.yaml"</span>
  <span class="o">}</span>
  provisioner <span class="s2">"file"</span> <span class="o">{</span>
    <span class="nb">source</span>      <span class="o">=</span> <span class="s2">"ansible.sh"</span>
    destination <span class="o">=</span> <span class="s2">"/home/ubuntu/ansible.sh"</span>
  <span class="o">}</span>
  provisioner <span class="s2">"remote-exec"</span> <span class="o">{</span>
    inline <span class="o">=</span> <span class="o">[</span>
      <span class="s2">"cd /home/ubuntu/"</span>,
      <span class="s2">"sudo chmod 600 terraform-new-key1.pem"</span>,
      <span class="s2">"sudo chmod +x pb1.yaml"</span>,
      <span class="s2">"sudo ansible-playbook pb1.yaml"</span>,
      <span class="s2">"sudo chmod +x ansible.sh"</span>,
      <span class="s2">"sudo ./ansible.sh"</span>,
      <span class="s2">"sudo ansible-inventory -i aws_ec2.yaml --list"</span>,
      <span class="s2">"sudo ansible-inventory --graph"</span>
    <span class="o">]</span>
  <span class="o">}</span>
<span class="o">}</span>
 </code></pre></figure>

<p><b> SSH to bastion host and run playbooks to init and join k8s nodes </b></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># k8s-deploy.yaml</span>

<span class="c"># Playbook hosts can be defined by EC2 tags</span>
<span class="nt">---</span>
- name: EC2-K8S cluster setup
  hosts: all
  gather_facts: <span class="nb">false
  </span>tasks:
    - name: <span class="nb">test </span>EC2 dynamic hosts connections
      include_playbook: 2-test-connection.yaml

    - name: Pre-task <span class="k">for </span>all k8s hosts
      include_playbook: 3-k8s-nodes-preparation.yaml

    - name: init Master node
      include_playbook: 4-master-init.yaml

    - name: Join worker nodes
      include_playbook: 5-work-join.yaml

    - name: configure bastion to run kubectl
      include_playbook: 8-local-kubeconf-admin.yaml</code></pre></figure>

<p><b> Conclusion</b></p>

<p>Now we are able to spin up a 3-nodes K8S cluster on EC2 instances in about 20 minutes.</p>

<p>For lab purpose, running K8S on EC2 instances with default VPC can be cheaper and flexible option compare to EKS, as AWS managed EKS require additional VPC and unable to change node instance type unless delete the existing node group.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[K8S on EC2 vs EKS]]></summary></entry><entry><title type="html">Ansible Advance - Roles</title><link href="http://localhost:4000/jekyll/cat2/2024/02/21/Ansible-role.html" rel="alternate" type="text/html" title="Ansible Advance - Roles" /><published>2024-02-21T11:15:29+11:00</published><updated>2024-02-21T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/02/21/Ansible-role</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/02/21/Ansible-role.html"><![CDATA[<p><b>About Ansible Roles </b></p>

<p>Here we play with Ansible Roles, as Ansible is powerful configuration automation tool by running playpook, in addition organizing multiple Ansible contents and playbooks into roles provides a structural and more manageable way to achieve complex tasks for multiple targets or groups.</p>

<p>===================================================================</p>

<p>Ansible Roles can be created by:</p>

<ul>
  <li>Method1: Create Ansible roles by ansible-galaxy</li>
</ul>

<p>Quickly creating a well-defined role directory structure skeleton, we can leverage the command ansible-galaxy init <role_name></role_name></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@localhost ansible]# ansible-galaxy init zack-role
 Role zack-role was created successfully
<span class="o">[</span>root@localhost ansible]# tree zack-role
zack-role
├── defaults
│   └── main.yml
├── files
├── handlers
│   └── main.yml
├── meta
│   └── main.yml
├── README.md
├── tasks
│   └── main.yml
├── templates
├── tests
│   ├── inventory
│   └── test.yml
└── vars
    └── main.yml</code></pre></figure>

<p><b>defaults</b>: Contain default variables for the role.</p>

<p><b>files</b>: Contain static files that you want to copy to the target hosts.</p>

<p><b>handlers</b>: Contain handlers, which are tasks triggered by other tasks.</p>

<p><b>meta</b>: Contain metadata for the role, such as dependencies.</p>

<p><b>tasks</b>: Contains a main.yaml file, which includes tasks specific to the “server1” role.</p>

<p><b>templates</b>: Contain Jinja2 templates.</p>

<p><b>vars</b>: Contain variables specific to the “server1” role</p>

<p>===================================================================</p>

<ul>
  <li>Method2: Manually create roles (server1 &amp; server2) by create on-demand folder structure, Setting up groups in ansible inventory file：</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">vim /etc/ansible/hosts
<span class="c"># add group server1 as web server, server2 as backend</span>
<span class="o">[</span>defaults]
11.0.1.132
11.0.1.131

<span class="o">[</span>server1] <span class="c"># web</span>
11.0.1.131

<span class="o">[</span>server2]  <span class="c"># backend</span>
11.0.1.132

<span class="c"># create own roles structure server1 (web) and server2 (backend)</span>
<span class="c"># create zack.html under role server1/files</span>

<span class="o">[</span>root@localhost ansible]# tree roles
roles
├── base
│   └── tasks
│       └── main.yaml
├── server1
│   ├── defaults
│   ├── files
│   │   └── zack.html
│   ├── handlers
│   ├── meta
│   ├── tasks
│   │   └── main.yaml
│   ├── templates
│   └── vars
└── server2
    ├── defaults
    ├── files
    ├── handlers
    ├── meta
    ├── tasks
    │   └── main.yaml
    ├── templates
    └── vars</code></pre></figure>

<ul>
  <li>define tasks for role server1 and role server2</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">cat</span> /etc/ansible/roles/server1/tasks/main.yaml
<span class="c"># web server tasks include: install httpd, enable service, open 80 port, replace index.html, restart httpd service</span>
- name: Install HTTPD package
  yum:
    name: httpd
    state: present

- name: Enable HTTPD service
  service:
    name: httpd
    enabled: <span class="nb">yes
    </span>state: started

- name: Open port 80 <span class="k">in </span>firewall
  firewalld:
    service: http
    permanent: <span class="nb">yes
    </span>state: enabled

- name: replace index.html
  copy:
    src: zack.html
    dest: /var/www/html/index.html
    owner: root
    group: root
    mode: 0644
  register: httpd_updated

- name: restart httpd service
  service:
    name: httpd
    state: restarted
  when: httpd_updated.changed


<span class="nb">cat</span> /etc/ansible/roles/server2/tasks/main.yaml

<span class="c"># backend server2 tasks include install epel repo, install iftop and lrzsz</span>

- name: Install epel-release
  yum:
    name:
      - epel-release
    state: latest
- name: Check Yum Repository List
  yum:
    list: repo

- name: Install usage packages
  yum:
    name:
      - iftop
      - lrzsz
    state: latest</code></pre></figure>

<ul>
  <li>define role main playbook zack-role.yaml</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># update repo for all hosts, for server1 and server 2, execute each role server1 and server2</span>

<span class="nt">---</span>
- become: <span class="nb">true
  </span>hosts: all
  pre_tasks:
  - name: update repository index
    tags: always
    yum:
      update_cache: <span class="nb">yes
    </span>changed_when: <span class="nb">false</span>

- import_playbook: /etc/ansible/add-more-user.yaml

- hosts: server1
  become: <span class="nb">true
  </span>roles:
    - server1


- hosts: server2
  become: <span class="nb">true
  </span>roles:
    - server2</code></pre></figure>

<ul>
  <li>run playbook for zack-role.yaml</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@localhost ansible]# ansible-playbook zack-role.yaml

PLAY <span class="o">[</span>all] <span class="k">**************************************************************************************************************************</span>

TASK <span class="o">[</span>Gathering Facts] <span class="k">**************************************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.132]
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>update repository index] <span class="k">******************************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]
ok: <span class="o">[</span>11.0.1.132]


PLAY <span class="o">[</span>server1] <span class="k">**********************************************************************************************************************</span>

TASK <span class="o">[</span>Gathering Facts] <span class="k">**************************************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>server1 : Install HTTPD package] <span class="k">**********************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>server1 : Enable HTTPD service] <span class="k">***********************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>server1 : Open port 80 <span class="k">in </span>firewall] <span class="k">*******************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>server1 : replace index.html] <span class="k">*************************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.131]

TASK <span class="o">[</span>server1 : restart httpd service] <span class="k">**********************************************************************************************</span>
skipping: <span class="o">[</span>11.0.1.131]

PLAY <span class="o">[</span>server2] <span class="k">**********************************************************************************************************************</span>

TASK <span class="o">[</span>Gathering Facts] <span class="k">**************************************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.132]

TASK <span class="o">[</span>server2 : Install epel-release] <span class="k">***********************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.132]

TASK <span class="o">[</span>server2 : Check Yum Repository List] <span class="k">******************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.132]

TASK <span class="o">[</span>server2 : Install usage packages] <span class="k">*********************************************************************************************</span>
ok: <span class="o">[</span>11.0.1.132]

PLAY RECAP <span class="k">**************************************************************************************************************************</span>
11.0.1.131                 : <span class="nv">ok</span><span class="o">=</span>11   <span class="nv">changed</span><span class="o">=</span>0    <span class="nv">unreachable</span><span class="o">=</span>0    <span class="nv">failed</span><span class="o">=</span>0    <span class="nv">skipped</span><span class="o">=</span>1    <span class="nv">rescued</span><span class="o">=</span>0    <span class="nv">ignored</span><span class="o">=</span>0   
11.0.1.132                 : <span class="nv">ok</span><span class="o">=</span>10   <span class="nv">changed</span><span class="o">=</span>0    <span class="nv">unreachable</span><span class="o">=</span>0    <span class="nv">failed</span><span class="o">=</span>0    <span class="nv">skipped</span><span class="o">=</span>0    <span class="nv">rescued</span><span class="o">=</span>0    <span class="nv">ignored</span><span class="o">=</span>0   </code></pre></figure>

<ul>
  <li>Bingo! validate httpd on server1, and iftop &amp; lrzsz installed on server2 by leveraging with ansible roles</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="o">[</span>root@localhost ansible]# curl 11.0.1.131
This is zack <span class="nb">test </span>Ansible role web <span class="o">!!!!!!!!!!!</span>   
<span class="o">!!!!!!!!!</span></code></pre></figure>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Ansible Roles]]></summary></entry><entry><title type="html">Enable Free SSL certificate for blog</title><link href="http://localhost:4000/jekyll/cat2/2024/02/07/ssl-cert.html" rel="alternate" type="text/html" title="Enable Free SSL certificate for blog" /><published>2024-02-07T11:15:29+11:00</published><updated>2024-02-07T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/02/07/ssl-cert</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/02/07/ssl-cert.html"><![CDATA[<p><b>HTTPS with SSL cert </b></p>

<ul>
  <li>
    <p>Generate free SSL certificate from https://zerossl.com/</p>
  </li>
  <li>
    <p>Validate the Certificate with Private Key via https://www.sslshopper.com/certificate-key-matcher.html</p>
  </li>
  <li>
    <p>Upload ‘certificate.crt’ and ‘private.key’ to web server /etc/nginx/ssl/</p>
  </li>
  <li>
    <p>Setting up NGINX HTTPS Server by include the ssl parameter to the listen directive in the server block under ‘http’ in ‘nginx.conf’:</p>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">http <span class="o">{</span>

    server <span class="o">{</span>
        listen 443 ssl<span class="p">;</span>
        server_name zackdevops.online<span class="p">;</span>

        ssl_certificate /etc/nginx/ssl/certificate.crt<span class="p">;</span>
        ssl_certificate_key /etc/nginx/ssl/private.key<span class="p">;</span>

    <span class="o">}</span>
...
<span class="o">}</span></code></pre></figure>

<ul>
  <li>fix 2 errors</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">2024/02/07 10:29:33 <span class="o">[</span>emerg] 73175#73175: <span class="s2">"server"</span> directive is not allowed here <span class="k">in</span> /etc/nginx/nginx.conf:11

2024/02/07 10:30:25 <span class="o">[</span>error] 73207#73207: <span class="k">*</span>1 directory index of <span class="s2">"/usr/share/nginx/html/"</span> is forbidden,client: 163.53.144.82, server: zackdevops.online, request: <span class="s2">"GET / HTTP/1.1"</span>, host: <span class="s2">"zackdevops.online"</span>
2024/02/07 10:30:36 <span class="o">[</span>error] 73207#73207: <span class="k">*</span>1 directory index of <span class="s2">"/usr/share/nginx/html/"</span> is forbidden, client: 163.53.144.82, server: zackdevops.online, request: <span class="s2">"GET / HTTP/1.1"</span>, host: <span class="s2">"zackdevops.online"</span></code></pre></figure>

<p>Bingo! https://zackdevops.online connection is secure!</p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[HTTPS with SSL cert]]></summary></entry><entry><title type="html">Ansible for AWS Dynamic Inventory</title><link href="http://localhost:4000/jekyll/cat2/2024/01/22/ansible-aws.html" rel="alternate" type="text/html" title="Ansible for AWS Dynamic Inventory" /><published>2024-01-22T11:15:29+11:00</published><updated>2024-01-22T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/01/22/ansible-aws</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/01/22/ansible-aws.html"><![CDATA[<p><b>About Ansible AWS Dynamic Inventory </b></p>

<p>When using Ansible with AWS, maintaining the inventory file will be a hectic task as AWS has frequently changed IPs, autoscaling instances, and much more.</p>

<p>Here we will install and apply ansible plugin for AWS dynamic inventory which makes an API call to AWS to get the instance information in the run time. It givesthe ec2 instance details dynamically to manage the AWS infrastructure</p>

<p>It supports most of the public and private cloud platforms not limited to just AWS.</p>

<ul>
  <li>The Dynamic Inventory Topology:</li>
</ul>

<p><img src="/assets/ansible-inventory.png" alt="image tooltip here" /></p>

<ul>
  <li>Setup Ansible AWS Dynamic Inventory</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Ensure python3 &amp; pip3 installed in Ansible server</span>
python3 <span class="nt">--version</span>
<span class="nb">sudo </span>apt-get <span class="nb">install </span>python3 <span class="nt">-y</span>
<span class="nb">sudo </span>apt-get <span class="nb">install </span>python3-pip <span class="nt">-y</span>

<span class="c"># Install the boto3 library for ansible boot core to make API calls to AWS to retrieve ec2 instance details</span>
<span class="nb">sudo </span>pip3 <span class="nb">install </span>boto3

fix error
ERROR! The ec2 dynamic inventory plugin requires boto3 and botocore.


<span class="c"># Create an inventory directory under /opt and cd into the directory</span>
<span class="nb">sudo mkdir</span> <span class="nt">-p</span> /opt/ansible/inventory
<span class="nb">cd</span> /opt/ansible/inventory
<span class="nb">sudo </span>vi aws_ec2.yaml

<span class="nt">---</span>
plugin: aws_ec2

aws_access_key: &lt;xxx-AWS-ACCESS-KEY-HERE&gt;
aws_secret_key: &lt;xx-AWS-SECRET-KEY-HERE&gt;

regions:
  - us-west-2

keyed_groups:  <span class="c"># key filter for listing AWS ec2 groups</span>
  - key: tags
    prefix: tag
  - prefix: instance_type
    key: instance_type
  - key: placement.region
    prefix: aws_region

<span class="c"># edit ansible config file to enable AWS plugin and set inventory as above yaml</span>
<span class="nb">sudo </span>vi /etc/ansible/ansible.cfg

<span class="o">[</span>inventory]
enable_plugins <span class="o">=</span> aws_ec2

inventory      <span class="o">=</span> /opt/ansible/inventory/aws_ec2.yaml</code></pre></figure>

<p><img src="/assets/ec2-inventory.png" alt="image tooltip here" /></p>

<ul>
  <li>Test if Ansible is able to ping all the machines returned by the dynamic inventory</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">ansible-inventory <span class="nt">-i</span> /opt/ansible/inventory/aws_ec2.yaml <span class="nt">--list</span>
ansible all <span class="nt">-m</span> ping</code></pre></figure>

<ul>
  <li>Execute Ansible Commands With ec2 Dynamic Inventory</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">ansible-inventory <span class="nt">--graph</span></code></pre></figure>

<p>List all instances grouped under tags, zones, and regions with dynamic group names like</p>

<p>aws_region_ap_southeast_2</p>

<p>instance_type_t2_micro</p>

<p>tag_Name</p>

<p><img src="/assets/list-aws-ec2.png" alt="image tooltip here" /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[About Ansible AWS Dynamic Inventory]]></summary></entry><entry><title type="html">Istio for Traffic Routing</title><link href="http://localhost:4000/jekyll/cat2/2024/01/15/istio.html" rel="alternate" type="text/html" title="Istio for Traffic Routing" /><published>2024-01-15T11:15:29+11:00</published><updated>2024-01-15T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2024/01/15/istio</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2024/01/15/istio.html"><![CDATA[<p><b>Helm install istio </b></p>

<p>Here we use helm to install istio (istio-base, istiod, istia gateway), then deploy a sample online book store microservice “bookinfo”, practise istio tasks include Traffic Management, Observability, Security.</p>

<p>Bookinfo Topology:</p>

<p><img src="/assets/bookinfo.png" alt="image tooltip here" /></p>

<ul>
  <li>Helm install istio (istiod, istio-ingress)</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl create namespace istio-system
helm pull istio/base
helm <span class="nb">install </span>istio-base <span class="nb">.</span> <span class="nt">-n</span> istio-system <span class="nt">--set</span> <span class="nv">defaultRevision</span><span class="o">=</span>default

helm pull istio/istiod
helm <span class="nb">install </span>istiod <span class="nb">.</span> <span class="nt">-n</span> istio-system 

kubectl create namespace istio-ingress
helm pull istio/gateway
helm <span class="nb">install </span>istio-ingress <span class="nb">.</span> <span class="nt">-n</span> istio-ingress

helm <span class="nb">ls</span> <span class="nt">-n</span> istio-system
NAME      	NAMESPACE   	REVISION	UPDATED                                	STATUS  	CHART        	APP VERSION
istio-base	istio-system	1       	2023-12-17 08:14:06.943276388 +0800 CST	deployed	base-1.20.1  	1.20.1     
istiod    	istio-system	1       	2023-12-17 08:15:40.370551503 +0800 CST	deployed	istiod-1.20.1	1.20.1 

helm <span class="nb">ls</span> <span class="nt">-n</span> istio-ingress
NAME         	NAMESPACE    	REVISION	UPDATED                                	STATUS  	CHART         	APP VERSION
istio-ingress	istio-ingress	1       	2023-12-17 08:25:07.111999373 +0800 CST	deployed	gateway-1.20.1	1.20.1</code></pre></figure>

<ul>
  <li>Deploy bookinfo microservice and istio ingressgateway and virtualservice</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl label namespace istio-system istio-injection<span class="o">=</span>enabled

kubectl apply <span class="nt">-f</span> https://github.com/istio/istio/blob/master/samples/bookinfo/platform/kube/bookinfo.yaml <span class="nt">-oyaml</span> <span class="o">&gt;</span> bookinfo.yaml
kubectl apply <span class="nt">-f</span> bookinfo.yaml

kubectl get po
NAME                                                     READY   STATUS    RESTARTS       AGE
details-v1-698d88b-wmfcb                                 2/2     Running   0              21m
ratings-v1-6484c4d9bb-cb6gx                              2/2     Running   0              21m
reviews-v1-5b5d6494f4-jrsvc                              2/2     Running   0              21m
reviews-v2-5b667bcbf8-jgfzj                              2/2     Running   0              21m
reviews-v3-5b9bd44f4-tmmfz                               2/2     Running   0              21m

kubectl apply <span class="nt">-f</span> https://github.com/istio/istio/blob/master/samples/bookinfo/networking/bookinfo-gateway.yaml <span class="nt">-oyaml</span> <span class="o">&gt;</span> bookinfo-gateway.yaml
kubectl apply <span class="nt">-f</span> bookinfo-gateway.yaml</code></pre></figure>

<ul>
  <li>Deploy Kiali, jaeger, grafana, prometheus</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">wget https://raw.githubusercontent.com/istio/istio/release-1.20/samples/addons/prometheus.yaml
wget https://raw.githubusercontent.com/istio/istio/release-1.20/samples/addons/jaeger.yaml
wget https://raw.githubusercontent.com/istio/istio/release-1.20/samples/addons/grafana.yaml
kubectl create <span class="nt">-f</span> prometheus.yaml <span class="nt">-f</span> jaeger.yaml <span class="nt">-f</span> grafana.yaml</code></pre></figure>

<ul>
  <li>
    <p>visit http://book.istio:31000/productpage, with review (v1, v2, v3)
<img src="/assets/kiali.png" alt="image tooltip here" /></p>
  </li>
  <li>
    <p>define destination rules and virtual service for reviews</p>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">wget https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/networking/destination-rule-all.yaml
wget https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/networking/virtual-service-reviews-90-10.yaml

kubectl create <span class="nt">-f</span> destination-rule-all.yaml <span class="nt">-f</span> virtual-service-reviews-90-10.yaml   <span class="c"># route v1 10% and v3 90%</span>
kubectl scale deployment reviews-v2 <span class="nt">-n</span> istio-system <span class="nt">--replicas</span><span class="o">=</span>0 <span class="c"># scale down v2 to 0</span>

kubectl get dr <span class="nt">-A</span>
NAMESPACE      NAME          HOST          AGE
istio-system   details       details       6m56s
istio-system   productpage   productpage   6m56s
istio-system   ratings       ratings       6m56s
istio-system   reviews       reviews       6m56s
kubectl get vs <span class="nt">-A</span>
NAMESPACE      NAME       GATEWAYS               HOSTS            AGE
istio-system   bookinfo   <span class="o">[</span><span class="s2">"bookinfo-gateway"</span><span class="o">]</span>   <span class="o">[</span><span class="s2">"book.istio"</span><span class="o">]</span>   19h
istio-system   reviews                           <span class="o">[</span><span class="s2">"reviews"</span><span class="o">]</span>      5m26s

Spec:
  Hosts:
    reviews
  Http:
    Route:
      Destination:
        Host:    reviews
        Subset:  v1
      Weight:    10
      Destination:
        Host:    reviews
        Subset:  v3
      Weight:    90</code></pre></figure>

<p>refresh bookinfo webpage, test Traffic route weight as bellow</p>

<p>90% traffic for reviews v3  vs  10% traffic for review v1</p>

<p><img src="/assets/1090.png" alt="image tooltip here" /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Helm install istio]]></summary></entry><entry><title type="html">Redis cluster with Helm</title><link href="http://localhost:4000/jekyll/cat2/2023/12/09/redis.html" rel="alternate" type="text/html" title="Redis cluster with Helm" /><published>2023-12-09T11:15:29+11:00</published><updated>2023-12-09T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2023/12/09/redis</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2023/12/09/redis.html"><![CDATA[<p><b>Helm install Redis cluster</b></p>

<p>Here we use helm to install Redis cluster, then validate statefuleset storage and cluster avalibility</p>

<p>Typical Redis cluster (3 master + 3 slave for slots) Topology:</p>

<p><img src="/assets/redis-slot.png" alt="image tooltip here" /></p>

<ul>
  <li>Helm install bitnami/redis-cluster</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">helm repo add bitnami https://charts.bitnami.com/bitnami
helm pull bitnami/redis-cluster
kubectl create ns redis
helm <span class="nb">install </span>zz-redis bitnami/redis-cluster <span class="nt">-n</span> redis</code></pre></figure>

<ul>
  <li>redis-cluster status</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl get po | <span class="nb">grep </span>zz-redis
zz-redis-redis-cluster-0                                 1/1     Running   3 <span class="o">(</span>33m ago<span class="o">)</span>     36m
zz-redis-redis-cluster-1                                 1/1     Running   1 <span class="o">(</span>32m ago<span class="o">)</span>     36m
zz-redis-redis-cluster-2                                 1/1     Running   1 <span class="o">(</span>33m ago<span class="o">)</span>     36m
zz-redis-redis-cluster-3                                 1/1     Running   1 <span class="o">(</span>33m ago<span class="o">)</span>     36m
zz-redis-redis-cluster-4                                 1/1     Running   1 <span class="o">(</span>33m ago<span class="o">)</span>     36m
zz-redis-redis-cluster-5                                 1/1     Running   1 <span class="o">(</span>33m ago<span class="o">)</span>     36m

kubectl get svc
zz-redis-redis-cluster                    ClusterIP   10.96.68.34     &lt;none&gt;        6379/TCP                        37m
zz-redis-redis-cluster-headless           ClusterIP   None            &lt;none&gt;        6379/TCP,16379/TCP              37m

kubectl get pvc
NAME                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
redis-data-zz-redis-redis-cluster-0   Bound    pvc-81f69c15-2f59-4704-9fec-c3ab217ebca5   1Gi        RWO            rook-ceph-block   37m
redis-data-zz-redis-redis-cluster-1   Bound    pvc-871dbb68-a78b-48a8-8feb-8726eb8a795e   1Gi        RWO            rook-ceph-block   37m
redis-data-zz-redis-redis-cluster-2   Bound    pvc-afd8c82e-c314-426a-a3cd-3c8d10b42bb1   1Gi        RWO            rook-ceph-block   37m
redis-data-zz-redis-redis-cluster-3   Bound    pvc-e8fd5d47-dd60-4358-8cf5-a17d6574bbe2   1Gi        RWO            rook-ceph-block   37m
redis-data-zz-redis-redis-cluster-4   Bound    pvc-04d4f148-b7c1-407e-b9a8-b2fa911405f0   1Gi        RWO            rook-ceph-block   37m
redis-data-zz-redis-redis-cluster-5   Bound    pvc-99e8b2cc-c5d5-4636-a19b-042922eca3cc   1Gi        RWO            rook-ceph-block   37m</code></pre></figure>

<ul>
  <li>Validate cluster by set key</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl <span class="nb">exec</span> <span class="nt">-it</span> zz-redis-redis-cluster-0 <span class="nt">--</span> sh
redis-cli
info replication
cluster info
cluster nodes
054ff137e9530c0e4d8afd1d00162d01952580de 172.16.122.179:6379@16379 slave 2ebcfdf7e74aa8755250384f7efa466f4d18e9d4 0 1702703474000 3 connected
7cfeb1d88dc3838e600a32c1e233b1c5f05006f1 172.16.58.197:6379@16379 master - 0 1702703473000 2 connected 5461-10922
1e56bad62197062fad465bbcc6b625bea8364db2 172.16.58.224:6379@16379 slave 7cfeb1d88dc3838e600a32c1e233b1c5f05006f1 0 1702703474954 2 connected
7e539b2209581c8375f7fb0aa9eedf5b98754b05 172.16.85.252:6379@16379 slave 889515187dbbb525fd73dc840d5bcad78305645d 0 1702703473947 1 connected
889515187dbbb525fd73dc840d5bcad78305645d 172.16.195.10:6379@16379 myself,master - 0 1702703469000 1 connected 0-5460
2ebcfdf7e74aa8755250384f7efa466f4d18e9d4 172.16.85.229:6379@16379 master - 0 1702703473000 3 connected 10923-16383


127.0.0.1:6379&gt; <span class="nb">set </span>dad zack
OK
127.0.0.1:6379&gt; get dad
<span class="s2">"zack"</span>

kubectl <span class="nb">exec</span> <span class="nt">-it</span> zz-redis-redis-cluster-4 <span class="nt">--</span> sh
redis-cli
127.0.0.1:6379&gt; KEYS <span class="k">*</span>
1<span class="o">)</span> <span class="s2">"dad"</span></code></pre></figure>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Helm install Redis cluster]]></summary></entry><entry><title type="html">Rook-Ceph for dynamic Persistent Volume</title><link href="http://localhost:4000/jekyll/cat2/2023/11/09/ceph.html" rel="alternate" type="text/html" title="Rook-Ceph for dynamic Persistent Volume" /><published>2023-11-09T11:15:29+11:00</published><updated>2023-11-09T11:15:29+11:00</updated><id>http://localhost:4000/jekyll/cat2/2023/11/09/ceph</id><content type="html" xml:base="http://localhost:4000/jekyll/cat2/2023/11/09/ceph.html"><![CDATA[<p><b>Helm install Rook-Ceph for persistent storage</b></p>

<ul>
  <li>
    <p>Configure local VM block storage to add 50Gb sdb to all k8s master and work nodes</p>
  </li>
  <li>
    <p>install rook-ceph cluster</p>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">git clone <span class="nt">--single-branch</span> <span class="nt">--branch</span> master https://github.com/rook/rook.git
<span class="nb">cd </span>rook/deploy/examples
kubectl create <span class="nt">-f</span> crds.yaml <span class="nt">-f</span> common.yaml <span class="nt">-f</span> operator.yaml
kubectl create <span class="nt">-f</span> cluster.yaml</code></pre></figure>

<ul>
  <li>Ceph toolbox to check cluster status</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl create <span class="nt">-f</span> toolbox.yaml
kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> deploy/rook-ceph-tools <span class="nt">--</span> bash
ceph status
ceph osd status
ceph <span class="nb">df
</span>rados <span class="nb">df</span></code></pre></figure>

<ul>
  <li>Ceph Dashboard svc for https login</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl create <span class="nt">-f</span> dashboard-external-https.yaml
kubectl <span class="nt">-n</span> rook-ceph get secret rook-ceph-dashboard-password <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{['data']['password']}"</span> | <span class="nb">base64</span> <span class="nt">--decode</span> <span class="o">&amp;&amp;</span> <span class="nb">echo</span></code></pre></figure>

<ul>
  <li>Create storage pool and storageclass</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl create <span class="nt">-f</span> pool.yaml
<span class="nb">cd </span>csi/rbd
kubectl create <span class="nt">-f</span> storageclass.yaml</code></pre></figure>

<ul>
  <li>set “rook-ceph-block” as default sc</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl patch storageclass rook-ceph-block <span class="nt">-p</span> <span class="s1">'{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'</span>

kubectl get sc
NAME                        PROVISIONER                  RECLAIMPOLICY  VOLUMEBINDINGMODE  ALLOWVOLUMEEXPANSION   AGE
rook-ceph-block <span class="o">(</span>default<span class="o">)</span>   rook-ceph.rbd.csi.ceph.com   Delete          Immediate          <span class="nb">true                  </span>8d</code></pre></figure>

<p>check pool and OSDs in ceph webui</p>

<p><img src="/assets/ceph.png" alt="image tooltip here" /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="Cat2" /><summary type="html"><![CDATA[Helm install Rook-Ceph for persistent storage]]></summary></entry></feed>